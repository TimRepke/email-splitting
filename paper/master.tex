% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
%\PassOptionsToPackage{table,x11names}{xcolor}
\documentclass{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage[table]{xcolor}
\usepackage{listings}
\lstdefinestyle{mystyle}{
	%backgroundcolor=\color{backcolour},   
	%commentstyle=\color{codegreen},
	%keywordstyle=\color{magenta},
	%numberstyle=\tiny\color{codegray},
	%stringstyle=\color{codepurple},
	basicstyle=\scriptsize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	%numbers=left,                    
	%numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2,
	%frame=single
}

\lstset{style=mystyle}

% ----------------------
% fancyref
\usepackage[plain]{fancyref}

\newcommand*{\fancyrefalglabelprefix}{alg}
\newcommand*{\fancyreflstlabelprefix}{lst}
\newcommand*{\fancyrefapplabelprefix}{app}

\fancyrefaddcaptions{english}{%
	\providecommand*{\frefalgname}{algorithm}%
	\providecommand*{\Frefalgname}{Algorithm}%
	%
	\providecommand*{\freflstname}{listing}%
	\providecommand*{\Freflstname}{Listing}%
	%
	\providecommand*{\frefappname}{appendix}%
	\providecommand*{\Frefappname}{Appendix}%
}%

\frefformat{plain}{\fancyrefalglabelprefix}{%
	\frefalgname\fancyrefdefaultspacing#1%
}%
\Frefformat{plain}{\fancyrefalglabelprefix}{%
	\Frefalgname\fancyrefdefaultspacing#1%
}
\frefformat{vario}{\fancyrefalglabelprefix}{%
	\frefalgname\fancyrefdefaultspacing#1#3%
}%
\Frefformat{vario}{\fancyrefalglabelprefix}{%
	\Frefalgname\fancyrefdefaultspacing#1#3%
}%

\frefformat{plain}{\fancyreflstlabelprefix}{%
	\freflstname\fancyrefdefaultspacing#1%
}%
\Frefformat{plain}{\fancyreflstlabelprefix}{%
	\Freflstname\fancyrefdefaultspacing#1%
}
\frefformat{vario}{\fancyreflstlabelprefix}{%
	\freflstname\fancyrefdefaultspacing#1#3%
}%
\Frefformat{vario}{\fancyreflstlabelprefix}{%
	\Freflstname\fancyrefdefaultspacing#1#3%
}%

\frefformat{plain}{\fancyrefapplabelprefix}{%
	\frefappname\fancyrefdefaultspacing#1%
}%
\Frefformat{plain}{\fancyrefapplabelprefix}{%
	\Frefappname\fancyrefdefaultspacing#1%
}
\frefformat{vario}{\fancyrefapplabelprefix}{%
	\frefappname\fancyrefdefaultspacing#1#3%
}%
\Frefformat{vario}{\fancyrefapplabelprefix}{%
	\Frefappname\fancyrefdefaultspacing#1#3%
}%


% line breaks in table cells:

\usepackage{makecell}
\renewcommand\cellalign{lt}


\captionsetup{compatibility=false}
%

\newcommand{\dummyfig}[3]{
	\centering
	\fbox{
		\begin{minipage}[c][#1\textheight][c]{#2\textwidth}
			\centering{#3}
		\end{minipage}
	}
}
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads

\mainmatter              % start of the contributions
%
\title{Bringing Back Structure to Free Text Email Conversations with Recurrent Neural Networks}
%
\titlerunning{Disentangling Email Conversations}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Tim Repke \and Ralf Krestel}
%
\authorrunning{Tim Repke et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Tim Repke, Ralf Krestel}
%
\institute{Hasso Plattner Institute, Potsdam, Germany\\
\email{(tim.repke|ralf.krestel)@hpi.de}}

\maketitle              % typeset the title of the contribution

\begin{abstract}
The abstract should summarize the contents of the paper
using at least 70 and at most 150 words. It will be set in 9-point
font size and be inset 1.0 cm from the right and left margins.
There will be two blank lines before and after the Abstract. \dots
%\keywords{computational geometry, graph theory, Hamilton cycles}
\end{abstract}
%
\section{Introduction}
Emails are and important part of day to day business communication, hence their analysis inspired research from a variety of disciplines.
Many of those solely use information contained in the well structured email protocol headers, such as in Social Network Analysis, User Profiling, or Behaviour Analysis.

However, a lot more information remains hidden in the free text body of an email, which contains additional meta-data about a discussion in the form of quoted messages that are forwarded or replied to.
In the early days of email communication, users followed clear rules such as prefixing quoted text with angle brackets ($>$).

Nowadays though, due to the diversity of email programs, formatting standards, and the freedom to edit quoted text, identifying the different parts of a message body is a surprisingly challenging task.
Email programs like Outlook, Thunderbird, or even online services such as Gmail usually group emails into conversations and attempt to hide quoted parts.
To do so, they rely on having the preceding emails that are matched by subject and sender, but fail when the subject or quoted text was edited.

We propose a neural network based approach and enable downstream tasks to utilise otherwise hidden or noisy information and overcome problems of error-prone rule-based approaches.
Further we show improvements in flexibility and performance over earlier work on similar tasks.





%früher: emails strukturiert, eingerückt, etc; heute: sehr frei/divers, geht nicht mehr mit regeln; Ansatz: deep learning

%ALTERNATIV:

%emails used for SNA, classification, behaviour, profiling, and more
%most based on full mails, but conversations contain quoted messaged including additional metadata; distracts downstream tasks

%our goal: clean up emails properly, don't assume full corpus, therefore keep all information

\paragraph{Problem statement}
In this paper we propose approaches to extract the inherent structure of free text emails containing a conversation thread composed of consecutive quoted or forwarded messages.
Components of an email are referred to as \textit{zones} similar to definition used by Lampert et al~\cite{zones}.
We assume that a conversation thread is represented as a sequence of \textit{header and body blocks}, which is consistent with emails in the Enron corpus.
A pair of corresponding header and body is called \textit{conversational part} or \textit{message}.

Hereby client headers are blocks of meta-data automatically inserted by an email program, usually containing information on the sender, recipient, date, and subject of the quoted email.
Usually the header indicates, whether the subsequent message body was forwarded or replied to by the text above.
Bodies are the actual written messages, which on reply or forward are quoted below the newer message.

Message bodies are further separated into a \textit{greeting} (such as a formal or informal address of the recipient at the beginning of the message), \textit{authored text} (the actual message), \textit{signoff} (closing words of the message), and a \textit{signature} (containing contact information, advertising, or legal disclaimers).

Without loss of generality, we consider exactly one zone type per line as \Fref{fig:examplemail} exemplary shows.
In case of conflicts, the predominant or detailed type is used.


%describe task: split mails in different parts of the thread; parse meta information in intermediate headers
%- use term: zone
%- email as chain of header/body
%- within body: greetings, signoff, signature, text
%- header: contains metadata like from, to, when subject


\begin{figure}
% potential example listings:
% presto-k/discussion_threads/23.
% skilling-j/all_documents/1529.
% skilling-j/discussion_threads/1092.
\centering
\begin{tabular}{|c|}
	\hline 
	\scriptsize{
	\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lr}
		\makecell{
			\textit{From:} Alice\\ 
			\textit{To:} Bob, Brian\\
			\textit{Subject:} RE: Telephone Call with Jerry Murdock
		} &
		\hspace*{\fill} { \textit{Sent:} Mon, 14 May 2001 07:15 AM}
	\end{tabular*}
}
	\\ 
	\hline 
	\scriptsize{
	\begin{tabular*}{\textwidth}{l|l} 
		Body           & Thank you for your help. \\
		Body           & \\
		Body/Signature & ISC Hotline\\\hline
		Header         & 03/15/2001 10:32 AM \\
		Header         & \\
		Header         & Sent by: Randi Howard \\
		Header         & To: Jeff Skilling/Corp/Enron@ENRON\\
		Header         & cc:\\
		Header         & \\
		Header         & Subject: Re: My "P" Number\\\hline
		Body           & \\
		Body/Greeting  & Mr. Skilling: \\
		Body           & \\
		Body           & Your P number is P00500599.  For your convenience, you can also go to\\
		Body           & http://isc.enron.com/ under Site Highlights and reset your password or \\
		Body           & find your "P" number.\\
		Body           & \\
		Body/Signoff   & Thanks,\\
		Body/Signoff   & \\
		Body/Signoff   & Randi Howard\\
		Body/Signature & ISC HOTLINE\\
		Body           & \\\hline
		Header         & From:  Jeff Skilling                           03/15/2001 10:01 AM \\
		Header         & \\
		Header         & To: ISC Hotline/Corp/Enron@Enron\\
		Header         & cc:\\
		Header         & \\
		Header         & Subject: My "P" Number\\\hline
		Body           & \\
		Body           & Could you please forward my "P" number.  I am unable to get into the XMS \\
		Body           & system and need this ASAP.\\
		Body           & \\
		Body/Signoff   & Thanks for your help.\\
	\end{tabular*}
}
	\\ 
	\hline 
\end{tabular} 
\caption{Example email with zones; consecutive empty lines reduced to one}
\label{fig:examplemail}
\end{figure}





\section{Related Work}
% structure outline
% 1. show some tasks where authors found, that cleaning bodies is needed
% 2. highlight previous tasks/approaches (aligning, extract first message, multi zone)
% 2.1 jamison et al, yeh et al
% 2.2 carvalho using cperceptron (collins)
% 2.3 lampert et al zoning + request for action (highlight improvements through zoning first)
%     similarly rauscher et al (knowledge zoning), estival et al (author profiling)

Email corpora provide fascinating insights into human communication behaviour and therefore inspire research in many different areas.
Datasets such as the Enron or Avocado corpus~\cite{avocado,enron} provide real world information about business communication and contain not only professional emails, but also personal emails as well as spam.
For research on private emails, Ben Shneiderman published parts of his personal email archive~\cite{shneiderman}.
Another corpus used in related research is the Twenty Newsgroups dataset of emails sampled from newsgroups in the early 90s, however it only contains a few conversation threads~\cite{20news}.

A recent survey provides a detailed overview of the multitude email classification tasks alone~\cite{classification}.
Similarly interesting is the analysis of communication networks based on meta-data such as senders, recipients, and time.

Models based on the written content of emails may get confused by automatically inserted text blocks or quoted messages.
Thus, working with real world data requires the need to normalise data prior to the problem at hand.
Rauscher et al. developed an approach to detect zones inside work-related emails where relevant business knowledge may be found~\cite{rauscher2015context}.

In their work towards detecting emails containing requests for action, Lampert et al. compare the performance of their model on full emails (which may contain quoted text) against the same emails after automatically removing quoted sections and managed to achieve a relative error reduction by 40\%~\cite{rfa}.
Similar observations were made more recently predicting reply behaviour within the Avocado dataset~\cite{replying}.

We identified two approaches in the literature on separating email zones.
Assuming a complete email corpus, with the email archives of all users, a message in the outbox of one user is found in the inbox of another.
Likewise, quoted messages exist within the corpus as an original message from preceding communication.
By finding overlapping text passages across the corpus, Jamison et al. managed to resolve email threads of the Enron corpus almost perfectly~\cite{headerless}.
It has to be noted, that the claimed accuracy of almost 100\% was only tested on 20 email threads.

In order to reassemble email threads, Yeh et al. considered a similar approach with a more elaborate evaluation reaching an accuracy of 98\% separating email conversations into parts~\cite{similarity}.
To do so, they rely on additional meta information in emails sent through Microsoft Outlook (thread index) and rules that match specific client headers.
Thus, such an approach will not work on arbitrary emails, nor can it handle different localisation or edits by the user.

In this paper, we don't assume a complete corpus to be able to extract all information from only a single email archive or even a single email.
Carvalho and Cohn proposed Jangada, a system to remove quoted text and signature blocks from emails in the 20 newsgroup dataset~\cite{signature,20news}.
They first classify emails to find those, that contain quoted text or signatures and then classify each line individually using Conditional Random Fields and Collin's Perceptrons~\cite{crf, cperceptron}.
Reported accuracies range from 97\% to above 99\%.

Those result could not be reproduced on our dataset and modified task.
Similar findings are confirmed by Estival et al., who measured an accuracy of around 64\% using Jangada on Hotmail emails~\cite{profiling}.
Using CRFs, they managed to extract five different zones (author text, signature, advertisement, quoted text and reply lines) with an average accuracy of up to 88\%.

Most similar to our problem statement is the Zebra system by Lampert et al., which was developed for their previously mentioned work on requests for action~\cite{zones,rfa}.
The definition of zones is very similar to ours.

Contrary to our objectives, Zebra only tries to identify the zones within the very last message within an email thread and reject the rest as quoted text, whereas we aim to detect the zones across the entire email.
They describe graphic, orthographic, and lexical features to represent each line and classify line by line using an SVM reaching an average accuracy of 93\% on the two-zone task and 87\% on a nine-zone task.
It was found, that adding contextual features didn't improve the performance.
Comparing the performance by zone type, most problems are caused by signature lines (F-score around 60\%), signoffs (70\%) and attachments (69\%).

In our work, we aim to improve upon those results, while providing a system that is able to detect zones along the entire conversation thread contained in an email and not only the first part.
This way, even very small or incomplete datasets can be utilised for downstream tasks like social network analysis, speech acts and other research areas using email data.


%\cite{rfa} (detecting request for action) works better with zoning (see \cite{zones}), removes noise and focuses on text itself (excluding signature etc)

%\cite{zones} "Zebra" detects nine different zones, only last email in thread is considered, rest is quoted, classifies lines individually with features using SVM, three zones with 0.91 accuracy, 9 zones 0.87 accuracy; context features didn't add improve performance

%\cite{profiling} don't describe much, uses CRF inspired by \cite{signature}, five categories (author text, signature, advertisement, quoted text, reply lines), for eval using three zones with accuracy 0.88, F1 0.9, compared with \cite{signature} 0.64 and 0.75 on their data

%\cite{signature} "Jangada" detects signature and reply lines, only considers the last $n$ lines, using CRF with 0.97 accuracy, CPerceptron with 0.989 accuracy; extraction task only performed on mails known to contain signatures! used 20 newsgroup dataset \cite{20news}

%\cite{headerless} look for overlaps in enron dataset to split mails into parts of the conversation; closest to our task; we don't consider full dataset though (looking at each mail individually); their goal is to then reassemble the conversation threads, tested on 20 threads (leading to 465 mails), claim accuracy approaching 100\%

%\cite{similarity} similar to \cite{headerless}

%[citation needed] previous social network analysis wrong, because not considering full picture

%\cite{workhard} overview of recent technologies applied to enron and avocado (personal business classification + SNA)

%\cite{replying} also recent, on avocado, reply behaviour, only "main header"

%\cite{enron} enron corpus

%\cite{avocado} avocado corpus

%[citation needed] shneiderman corpus

%\cite{20news} 20 newsgroups

\section{Segmentation of Emails}
\label{sec:model}
Systems for email segmentation that are discussed earlier are based on hand written rules to match common structures directly or use them as features for machine learning models.
Such an approach will fail when client headers are localised, formats are changed or quoted messages are edited by users or get corrupted.
In most cases it may seem obvious to the human eye how to segment an email into client headers and quoted text even though different or corrupted formats are used.

However, even a sophisticated text parsing program will fail since client headers follow no standardised format.
Usually lines start with attribute keywords such as "From:" or "Subject:", however their value may span multiple lines and use varying delimiters.
This even makes it hard to detect the boundaries between header and body blocks, since one can not rely on the presence of keywords or well formed, deterministic schemas.

Therefore, our proposed model is based on multiple stages of neural network architectures which have proven their robustness in other applications.
In this section we describe how email text is represented and how classifiers can be used as a reliable and robust preprocessor for a simple program extract its inherent structure.

\subsection{Representation of Email Data}
In the initial stage of our system, the email text data is encoded into a low dimensional space.
This representation is used as input to later stages.
Here, the smallest fragment to be considered for email zoning are the lines in the email text.
The end of a line is delimited by a newline character (\textbackslash n).
Note however, that this may not necessarily be the same as floated lines displayed by an email program.
Analysis of the annotated data shows, that this granularity is sufficient for all header, body and signature zones as was assumed by other research on similar tasks.

Each line is encoded as a sequence of one-hot vectors representing respective characters.
We distinguish one hundred different case-sensitive alpha-numeric characters and basic ASCII symbols plus an out-of-scope placeholder.
This scope is sufficient for all email corpora we looked at, where only a negligible portion of characters exceeds this set.
For applications with Cyrillic, Arabic or other alphabets, this can easily be adapted.
Most lines in our test sets are between 40 and 50 characters long.

Inspired by research on character-aware language models~\cite{char_nn}, we experiment with recurrent and convolutional neural networks.
The first model consists of a layer with varying number of gated recurrent units (GRU), where the last unit's output later serves as a fixed size embedding of the line.
The second model uses a two convolutional layers, which scan along the sequence of characters in a line and are intertwined by max-pooling and global-averaging layers finally leading into a densely connected layer, where the number of neurons corresponds to the embedding size.

In both models, the line representations are learnt in a supervised fashion.
During training, a densely connected layer with softmax activation is appended so that a classifier can be trained to distinguish between lines of corresponding zone types.
We found, that limiting the length of lines improves the accuracy of the embedding.
Optimal parameters of the topology such as the number of layers and embedding size are determined experimentally.


% parameters line num layers, embedding size etc experimentally determined
% limit line len
% learn char or line embedding

\subsection{Classification of Email Lines}
The previously described model for line representations is already constructed as a classification of lines into zones.
That way however, the context in which a line appears is missing, resulting in less ideal performance on ambiguous or deceptive cases.
Thus, we add a second stage to our model for sequence to sequence classification, where the input is a sequence of line embeddings of an email.
Best performance was achieved with a bidirectional GRU layer, where the hidden states of each direction are concatenated, followed by a Conditional Random Field (CRF) as model output.
Furthermore, as input we concatenate two embeddings, which are pre-trained on a two- and five-zone classification.
This way, we managed to improve the performance by implicitly helping the model understand the inherent ontology, where emails consist of header and body blocks and the body can be further segmented.

In sequence to sequence classification, recurrent neural networks only consider the previous hidden state but neglect the actually predicted label sequence.
By using a bidirectional layer, we observed a small improvement over a unidirectional recurrent layer.
Like in language models~\cite{lstm_crf,lstm_cnn_crf}, the addition of a CRF to the output shows further performance gains.


% continue training for classification into header/body
% how to split thread based on that

\subsection{Extraction of Email Meta-Data}
continue training for further classification of body and header parts individually

\section{Experimental Setup}
In this section we present an overview of the email datasets we used and discuss the sampling of emails for an unbiased evaluation.
Further, we describe competing approaches that are used as baselines for comparison of our results.
We also analyse model parameters and its robustness to changes in email text.

\subsection{Dataset}
We evaluate our proposed approach to email zoning on the Enron Corpus~\cite{enron} and emails gathered from public mail archives of the Apache Software Foundation (ASF)\footnote{\url{http://mail-archives.apache.org/mod\_mbox/}}. 
Estival et al. and Lampert et al. discussed shortcomings in working with Usenet-style emails, so we refrain from using the twenty newsgroup dataset as was done for the Jangada system ~\cite{profiling,zones,20news}.
We found that more recent email threads from the ASF archives, especially those on mailinglists for users of different software projects, offer diverse formatting patterns.

Each dataset is divided into three subsets for training, testing, and final evaluation.
Emails are sampled at random from their respective original dataset and put into one of those subsets.
To ensure representative results that are not biased by author or domain, sampling per subset is restricted to distinct mailboxes (Enron) or mailing lists (ASF).
The ASF dataset was compiled by randomly selecting emails from the \textit{flink-user}, \textit{spark-user}, and \textit{lucene-solr-user} mailing list archives.

Prior to sampling from the Enron corpus, duplicates are removed based on sender, recipient(s), and time provided by structured email meta data.
This reduces the corpus almost by 50\%, which confirms the assumption of a full dataset, where an email in one's sent folder is also found in someone else's archive.

\begin{table}
	\caption{Annotated Datasets in Numbers}
	\label{tab:dataset}
	\centering
	\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} ccccccc}
		\toprule
		& \multicolumn{3}{c}{Enron} &  \multicolumn{3}{c}{ASF} \\
		\cmidrule{2-4}
		\cmidrule{5-7}
		                          & Train    & Test   & Eval   & Train   & Test   & Eval  \\
		\midrule
		Emails                    & 500      & 200    & 100    &  350   &  100   &   50   \\
		Individual messages       & 1048     & 474    & 233    &  934   &  226   &   108  \\
		Average length of threads & 3.5      & 3.6    & 3.5    &  3.5   &  3.7   &   3.1  \\
		Number of Signatures      & 103      & 58     & 26     &  76    &  13    &   5    \\
		\bottomrule
	\end{tabular*}
\end{table}

%##### > ../../DATA/ENRON/ANNOTATED < #####
%=========== Train:
%num mails 500
%num mails - no thread 279
%num mails - with thread 221
%num actual messages 1048
%avg threadlen 2.096
%avg threadlen - nonzero 3.47963800905
%num headers per mail 0: 0.56, 1: 0.16, 2: 0.14, 3: 0.05, 4: 0.04, 5: 0.02, 6: 0.01, 7: 0.01, 8: 0.01, 18: 0.00
%num sigs 103
%avg sig len 6.73786407767
%avg email len (lines) 46.934
%avg message len (lines) 23.4055299539
%=========== Test:
%num mails 200
%num mails - no thread 94
%num mails - with thread 106
%num actual messages 474
%avg threadlen 2.37
%avg threadlen - nonzero 3.58490566038
%num headers per mail 0: 0.47, 1: 0.14, 2: 0.23, 3: 0.07, 4: 0.04, 5: 0.03, 6: 0.02, 7: 0.01, 8: 0.01, 9: 0.01, 15: 0.01
%num sigs 58
%avg sig len 6.81034482759
%avg email len (lines) 42.875
%avg message len (lines) 20.5965909091
%=========== Eval:
%num mails 101
%num mails - no thread 48
%num mails - with thread 53
%num actual messages 233
%avg threadlen 2.30693069307
%avg threadlen - nonzero 3.49056603774
%num headers per mail 0: 0.48, 1: 0.18, 2: 0.22, 3: 0.05, 4: 0.04, 5: 0.01, 6: 0.01, 8: 0.01, 20: 0.01
%num sigs 26
%avg sig len 5.53846153846
%avg email len (lines) 42.0594059406
%avg message len (lines) 20.3579545455
%
%##### > ../../DATA/ASF/ANNOTATED < #####
%=========== Train:
%num mails 358
%num mails - no thread 132
%num mails - with thread 226
%num actual messages 934
%avg threadlen 2.60893854749
%avg threadlen - nonzero 3.54867256637
%num headers per mail 0: 0.37, 1: 0.26, 2: 0.15, 3: 0.08, 4: 0.06, 5: 0.02, 6: 0.03, 7: 0.01, 8: 0.01, 9: 0.00, 10: 0.00, 11: 0.00, 13: 0.00
%num sigs 76
%avg sig len 6.28947368421
%avg email len (lines) 68.7988826816
%avg message len (lines) 26.1347438753
%=========== Test:
%num mails 91
%num mails - no thread 41
%num mails - with thread 50
%num actual messages 226
%avg threadlen 2.48351648352
%avg threadlen - nonzero 3.7
%num headers per mail 0: 0.45, 1: 0.22, 2: 0.11, 3: 0.07, 4: 0.05, 5: 0.03, 6: 0.02, 7: 0.02, 8: 0.02
%num sigs 13
%avg sig len 6.15384615385
%avg email len (lines) 66.8351648352
%avg message len (lines) 24.1651785714
%=========== Eval:
%num mails 45
%num mails - no thread 16
%num mails - with thread 29
%num actual messages 108
%avg threadlen 2.4
%avg threadlen - nonzero 3.1724137931
%num headers per mail 0: 0.36, 1: 0.33, 2: 0.13, 3: 0.09, 4: 0.02, 6: 0.02, 7: 0.04
%num sigs 5
%avg sig len 5.8
%avg email len (lines) 64.8666666667
%avg message len (lines) 26.4166666667

\Fref{tab:dataset} shows an overview of the selected dataset and the expected number of messages to be extracted.
Prior heuristic analysis of the Enron corpus estimated 60\% of emails to contain conversation threads~\cite{enron}, which is close to our annotated data.
On average an email has two parts with 20 lines per message.
Only a few messages contain a signature, which on average are six lines long.

Our selected subsets of the Enron corpus are annotated with spans of characters nested by level of detail of their type.
Hereby the most coarse span types are headers and bodies.
Span types within a header are Subject, Person Name, Email Address, Date, and Time.
Within a body the following span types are annotated: Greeting and Signoff, Person Name within those, and Signatures. Signatures are further broken down into Name, Organisation, Addresses, Phone Numbers, Attachments, and Disclaimers.

Spans containing names (or email addresses) are linked as alias if they refer to the same person.
Other supplementary information such as organisations a person works for or contact details are also linked accordingly.
Although this level of detail exceeds the requirements for the objective of this paper, it provides valuable data for future work and research on social networks, speech acts, or other tasks surrounding email\footnote{Different versions of the dataset are published our web page:\\ \url{https://url-remov.ed/for/review}}.

%We hope to inspire future work in the area [knowledge graph]

% describe how data is selected from enron corpus (800: 500 train, 200 test, 100 eval)
% annotated very detailed, even with some entity linking and signature parts, considered level of detail: header, body, signature, intro/outro; one annotator
% some numbers comparing raw data and contained information in \Fref{tab:dataset}
% based on subject and sender/recipient around 60% threads\cite{enron}
% we removed duplicates based users and time, about 50%
% https://hpi.de//naumann/projects/web-science/business-communication-analysis/email-structure.html
% https://github.com/TimRepke/email-splitting


\subsection{Competing Approaches}
We compare our proposed model for extracting zones from emails against several other approaches.
Most notably, Jangada and Zebra~\cite{zones,signature} are reimplemented with slight modifications to fit the more refined problem statement.
Both systems originally are intended to distinguish lines within an email, which are not part of the latest message of that thread.
Clearly that deviates from our goal to extract \textit{all} individual parts and detect zones with additional detail within those.
Since the systems are supposed to detect zones within the first part of the email, their features and underlying models should in principle also work on our task.

The source code for Jangada is freely available on the author's web page\footnote{\url{http://www.cs.cmu.edu/~vitor/software/jangada/}}.
We used the source code as a basis for an implementation in Python, however continue to use the the same implementation of the Collins Perceptron for sequence labelling, which is part of the MinorThird Library~\cite{minorthird,cperceptron}.
For the extraction of signatures, Jangada originally only considers the last ten lines of an email.
In our implementation, the perceptron performs a multi-class classification along all lines of the email corresponding to zone types defined earlier.

The Zebra project web page\footnote{\url{http://zebra.thoughtlets.org/zoning.php}} does only provide annotated data, but not the system's source code. 
Gossen et al. implemented\footnote{\url{https://github.com/gerhardgossen/soZebra}} it for their work on classification of action items in emails~\cite{sozebra}.
We used that as a guideline for our adapted Python implementation.

Both models originally weren't designed to find the inherent structure of an entire email, but rather only the latest message in a conversation along with zones within it.
It seems fair to assume, that the approaches for zoning the last message in an email should easily transfer to following messages as well.
We use a selection of features from both models is taken as input for a recurrent neural network with two GRU layers~\cite{gru}, which will be referred to as \textit{FeatureRNN}.

Finally, we compare those to our proposed model as described in \Fref{sec:model}.


% baseline (regex+rules)
% reimplement zebra\footnote{\url{http://zebra.thoughtlets.org/zoning.php}} https://github.com/gerhardgossen/soZebra
% jangada\footnote{\url{http://www.cs.cmu.edu/~vitor/software/jangada/}}
% describe differences, what needs to be changed to make it fair
% RNN with features
% show which task is more or less comparable between all

\subsection{Learning Model Parameters}

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\dummyfig{0.17}{0.9}{xaxis=perturbation, yaxis=accuracy, one line/boxplot} 
		\caption{Obfuscating test samples}
	\end{subfigure}%
	~ 
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\dummyfig{0.17}{0.9}{xaxis=perturbation, yaxis=accuracy, three lines for different perturbation during eval} 
		\caption{Training with perturbation}
	\end{subfigure}
	\caption{Tolerance of the trained model against perturbation of the input}
\end{figure}

line length

embedding size

size of model/dimensions

size of training dataset (how much annotated data needed)

cross corpus (?)

\section{Results}
evaluate different complexities: full task (get all information), split head+body, additionally split body parts (sig, intro, outro)

results in \Fref{tab:results-comp} and \Fref{tab:results}

\begin{table}
	\centering
	\caption{Results of Two-Zone Task}
	\label{tab:results-comp}
	\begin{tabular}{|c|cc|}
		\hline
		Approach & Dataset& result\\ \hline
		We 1& a & 0.98\\
		We 2& a & 0.98\\
		We 3& a & 0.98\\
		Zebra\cite{zones} & d & 0.78\\
		Jangada\cite{signature} & d & 0.8\\
		\hline	
	\end{tabular}
\end{table}

\begin{table}
	\centering
	\caption{Experimental Results}
	\label{tab:results}
	\begin{tabular}{|c|cc|}
		\hline
		task & prec/rec& f1\\ \hline
		2zone& a & 0.98\\
		5zone& a & 0.98\\
		full& a & 0.98\\
		\hline	
	\end{tabular}
\end{table}

\Fref{fig:mpd} shows influence of extracting information vs using raw data

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\dummyfig{0.17}{0.9}{xaxis=timebuckets, yaxis=freq} 
		\caption{raw data}
	\end{subfigure}%
	~ 
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\dummyfig{0.17}{0.9}{xaxis=timebuckets, yaxis=freq} 
		\caption{extracted data}
	\end{subfigure}
	\caption{Mail Frequency}
	\label{fig:mpd}
\end{figure}


maybe: email classification with+without splitting (i.e. try to classify folders)

\section{Conclusion and Future Work}
nicht nur für mails, auch forenthreads und andere semi-strukturierte daten

% novel clean dataset for email research on enron
% flexible model to clean up large amounts of email data


\bibliographystyle{splncs03}
\bibliography{biblio} 
\end{document}
