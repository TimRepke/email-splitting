% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
%\PassOptionsToPackage{table,x11names}{xcolor}
\documentclass{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{pgfplots}
\usepackage{filecontents}
\usepackage{footnote}
\makesavenoteenv{tabular}
\makesavenoteenv{table}
%\usepackage{pstricks}
\usepackage{booktabs}
%\usepackage[table]{xcolor}
\usepackage{listings}
\lstdefinestyle{mystyle}{
	%backgroundcolor=\color{backcolour},   
	%commentstyle=\color{codegreen},
	%keywordstyle=\color{magenta},
	%numberstyle=\tiny\color{codegray},
	%stringstyle=\color{codepurple},
	basicstyle=\scriptsize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	%numbers=left,                    
	%numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2,
	%frame=single
}

\lstset{style=mystyle}

% ----------------------
% fancyref
\usepackage[plain]{fancyref}

\newcommand*{\fancyrefalglabelprefix}{alg}
\newcommand*{\fancyreflstlabelprefix}{lst}
\newcommand*{\fancyrefapplabelprefix}{app}

\fancyrefaddcaptions{english}{%
	\providecommand*{\frefalgname}{algorithm}%
	\providecommand*{\Frefalgname}{Algorithm}%
	%
	\providecommand*{\freflstname}{listing}%
	\providecommand*{\Freflstname}{Listing}%
	%
	\providecommand*{\frefappname}{appendix}%
	\providecommand*{\Frefappname}{Appendix}%
}%

\frefformat{plain}{\fancyrefalglabelprefix}{%
	\frefalgname\fancyrefdefaultspacing#1%
}%
\Frefformat{plain}{\fancyrefalglabelprefix}{%
	\Frefalgname\fancyrefdefaultspacing#1%
}
\frefformat{vario}{\fancyrefalglabelprefix}{%
	\frefalgname\fancyrefdefaultspacing#1#3%
}%
\Frefformat{vario}{\fancyrefalglabelprefix}{%
	\Frefalgname\fancyrefdefaultspacing#1#3%
}%

\frefformat{plain}{\fancyreflstlabelprefix}{%
	\freflstname\fancyrefdefaultspacing#1%
}%
\Frefformat{plain}{\fancyreflstlabelprefix}{%
	\Freflstname\fancyrefdefaultspacing#1%
}
\frefformat{vario}{\fancyreflstlabelprefix}{%
	\freflstname\fancyrefdefaultspacing#1#3%
}%
\Frefformat{vario}{\fancyreflstlabelprefix}{%
	\Freflstname\fancyrefdefaultspacing#1#3%
}%

\frefformat{plain}{\fancyrefapplabelprefix}{%
	\frefappname\fancyrefdefaultspacing#1%
}%
\Frefformat{plain}{\fancyrefapplabelprefix}{%
	\Frefappname\fancyrefdefaultspacing#1%
}
\frefformat{vario}{\fancyrefapplabelprefix}{%
	\frefappname\fancyrefdefaultspacing#1#3%
}%
\Frefformat{vario}{\fancyrefapplabelprefix}{%
	\Frefappname\fancyrefdefaultspacing#1#3%
}%


% line breaks in table cells:

\usepackage{makecell}
\renewcommand\cellalign{lt}


\captionsetup{compatibility=false}
%

\newcommand{\dummyfig}[3]{
	\centering
	\fbox{
		\begin{minipage}[c][#1\textheight][c]{#2\textwidth}
			\centering{#3}
		\end{minipage}
	}
}
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads

\mainmatter              % start of the contributions
%
\title{Bringing Back Structure to Free Text Email Conversations with Recurrent Neural Networks}
%
\titlerunning{Disentangling Email Conversations}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
%\author{Tim Repke \and Ralf Krestel}
\author{First Author \and Second Author}
%
%\authorrunning{Tim Repke et al.} % abbreviated author list (for running head)
\authorrunning{First Author et al.}
%
%%%% list of authors for the TOC (use if author list has to be modified)
%\tocauthor{Tim Repke, Ralf Krestel}
\tocauthor{First Author, Second Author}
%
%\institute{Hasso Plattner Institute, Potsdam, Germany\\
%\email{(tim.repke|ralf.krestel)@hpi.de}}

\institute{A Research Institute, City, Country\\
	\email{(second.author|first.author)@domain.tld}}

\maketitle              % typeset the title of the contribution

\begin{abstract}
The abstract should summarize the contents of the paper
using at least 70 and at most 150 words. It will be set in 9-point
font size and be inset 1.0 cm from the right and left margins.
There will be two blank lines before and after the Abstract. \dots
%\keywords{computational geometry, graph theory, Hamilton cycles}
\end{abstract}
%
\section{Introduction}
Emails are an important part of day to day business communication, hence their analysis inspired research from a variety of disciplines.
Many of those solely use information contained in the well structured email protocol headers, such as in Social Network Analysis, User Profiling, or Behaviour Analysis.

However, a lot more information remains hidden in the free text body of an email, which contains additional meta-data about a discussion in the form of quoted messages that are forwarded or replied to.
In the early days of email communication, users followed clear rules, e.g. prefixing quoted text with angle brackets ($>$).

Nowadays, due to the diversity of email programs, formatting standards, and the freedom to edit quoted text, identifying the different parts of a message body is a surprisingly challenging task.
Email programs like Outlook, Thunderbird, or even online services such as Gmail, usually group emails into conversations and attempt to hide quoted parts.
To this end, they try to match preceding emails by subject and sender.
This procedure fails in case the subject or quoted text was edited.

We propose a neural network based approach to utilise otherwise hidden or noisy information and overcome problems of error-prone rule-based approaches.
This enables downstream tasks to work with much clearer data and additional information.
Further we show improvements in flexibility and performance over earlier work on similar tasks.


%früher: emails strukturiert, eingerückt, etc; heute: sehr frei/divers, geht nicht mehr mit regeln; Ansatz: deep learning

%ALTERNATIV:

%emails used for SNA, classification, behaviour, profiling, and more
%most based on full mails, but conversations contain quoted messaged including additional metadata; distracts downstream tasks

%our goal: clean up emails properly, don't assume full corpus, therefore keep all information

\paragraph{Problem statement}
Our goal is to extract the inherent structure of free text emails containing a conversation thread composed of consecutive quoted or forwarded messages.
Components of an email are referred to as \textit{zones} similar to the definition used by Lampert et al~\cite{zones}.
We assume that a conversation thread is represented as a sequence of \textit{client header} and \textit{body blocks}.\footnote{This is consistent with emails in the Enron corpus}
A pair of corresponding header and body is called \textit{conversational part} or \textit{message}.

In this context, client headers are blocks of meta-data automatically inserted by an email program, usually containing information on the sender, recipient, date, and subject of the quoted email.
Generally the header indicates, whether the subsequent message body was forwarded or replied to by the text above.
Bodies are the actual written messages, which on reply or forward are quoted below the newer message.

Message bodies can often be further separated into a \textit{greeting} (such as a formal or informal address of the recipient at the beginning of the message), \textit{authored text} (the actual message), \textit{signoff} (closing words of the message), and a \textit{signature} (containing contact information, advertising, or legal disclaimers).

We assume that each single line can be assigned to exactly one zone.
Without loss of generality, we consider exactly one zone type per line as \Fref{fig:examplemail} exemplary shows.
In case of conflicts, the predominant or detailed type is used.


%describe task: split mails in different parts of the thread; parse meta information in intermediate headers
%- use term: zone
%- email as chain of header/body
%- within body: greetings, signoff, signature, text
%- header: contains metadata like from, to, when subject


\begin{figure}
% potential example listings:
% presto-k/discussion_threads/23.
% skilling-j/all_documents/1529.
% skilling-j/discussion_threads/1092.
\centering
\begin{tabular}{|c|}
	\hline 
	\scriptsize{
	\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lr}
		\makecell{
			\textit{From:} Alice\\ 
			\textit{To:} Bob, Brian\\
			\textit{Subject:} RE: Telephone Call with Jerry Murdock
		} &
		\hspace*{\fill} { \textit{Sent:} Mon, 14 May 2001 07:15 AM}
	\end{tabular*}
}
	\\ 
	\hline 
	\scriptsize{
	\begin{tabular*}{\textwidth}{l|l} 
		Body           & Thank you for your help. \\
		Body           & \\
		Body/Signature & ISC Hotline\\\hline
		Header         & 03/15/2001 10:32 AM \\
		Header         & \\
		Header         & Sent by: Randi Howard \\
		Header         & To: Jeff Skilling/Corp/Enron@ENRON\\
		Header         & cc:\\
		Header         & \\
		Header         & Subject: Re: My "P" Number\\\hline
		Body           & \\
		Body/Greeting  & Mr. Skilling: \\
		Body           & \\
		Body           & Your P number is P00500599.  For your convenience, you can also go to\\
		Body           & http://isc.enron.com/ under Site Highlights and reset your password or \\
		Body           & find your "P" number.\\
		Body           & \\
		Body/Signoff   & Thanks,\\
		Body/Signoff   & \\
		Body/Signoff   & Randi Howard\\
		Body/Signature & ISC HOTLINE\\
		Body           & \\\hline
		Header         & From:  Jeff Skilling                           03/15/2001 10:01 AM \\
		Header         & \\
		Header         & To: ISC Hotline/Corp/Enron@Enron\\
		Header         & cc:\\
		Header         & \\
		Header         & Subject: My "P" Number\\\hline
		Body           & \\
		Body           & Could you please forward my "P" number.  I am unable to get into the XMS \\
		Body           & system and need this ASAP.\\
		Body           & \\
		Body/Signoff   & Thanks for your help.\\
	\end{tabular*}
}
	\\ 
	\hline 
\end{tabular} 
\caption{Example email with zones; consecutive blank lines reduced to one}
\label{fig:examplemail}
\end{figure}





\section{Related Work}
% structure outline
% 1. show some tasks where authors found, that cleaning bodies is needed
% 2. highlight previous tasks/approaches (aligning, extract first message, multi zone)
% 2.1 jamison et al, yeh et al
% 2.2 carvalho using cperceptron (collins)
% 2.3 lampert et al zoning + request for action (highlight improvements through zoning first)
%     similarly rauscher et al (knowledge zoning), estival et al (author profiling)
\paragraph{Email Corpora}
Email corpora provide fascinating insights into human communication behaviour and therefore inspire research in many different areas.
Datasets such as the Enron~\cite{enron} or Avocado corpus~\cite{avocado} provide real world information about business communication and contain not only professional emails, but also personal emails as well as spam.
Ben Shneiderman published parts of his personal email archive for research~\cite{shneiderman}.
Another popular corpus is the Twenty Newsgroups dataset of emails sampled from newsgroups in the early 90s, it only contains a few conversation threads~\cite{20news}.
In our work at hand, we use the Enron corpus as well as a newly gathered emails from public email archives of the Apache Software Foundation.

\paragraph{Benefits of Preprocessing}
A recent survey provides a detailed overview of the multitude email classification tasks alone~\cite{classification}.
Similarly interesting is the analysis of communication networks based on meta-data like sender, recipients, and time extracted from emails~\cite{sna1,sna2}

Models based on the written content of emails may get confused by automatically inserted text blocks or quoted messages.
Thus, working with real world data requires the need to normalise data prior to the problem at hand.
Rauscher et al. developed an approach to detect zones inside work-related emails where relevant business knowledge may be found~\cite{rauscher2015context}.

In their work towards detecting emails containing requests for action, Lampert et al. compare the performance of their model on full emails (which may contain quoted text) against the same emails after automatically removing quoted sections and managed to achieve a relative error reduction by 40\%~\cite{rfa}.
Similar observations were made more recently predicting reply behaviour within the Avocado dataset~\cite{replying}.

\paragraph{Email Zoning with Rules and Text Alignment}
We identify three approaches to email zoning: rule based, text alignment, and machine learning.

The most na\"ive approach is to write specific rules that match commonly used patterns in email text.
Talon\footnote{\url{https://github.com/mailgun/talon}} provides a sophisticated set of patterns to match most popular client header formats.
The obvious downside is the lack of flexibility and that it's error-prone to changes.

Assuming a complete email corpus, a message in one user's outbox may be found in the inbox of other user(s).
Likewise, quoted messages exist within the corpus as an original message from preceding communication.
By finding overlapping text passages across the corpus, Jamison et al. managed to resolve email threads of the Enron corpus almost perfectly~\cite{headerless}.
It has to be noted, that the claimed accuracy of almost 100\% was only tested on 20 email threads.

In order to reassemble email threads, Yeh et al. considered a similar approach with a more elaborate evaluation reaching an accuracy of 98\% separating email conversations into parts~\cite{similarity}.
To do so, they rely on additional meta information in emails sent through Microsoft Outlook (thread index) and rules that match specific client headers.
Thus, such an approach will not work on arbitrary emails, nor can it handle different localisation or edits by the user.

Contrary to approaches using text alignments, we don't assume a complete corpus.
Our goal is to extract all information from only a single email archive or even a single email.

\paragraph{Machine Learning for Email Zoning}
Another approach to email zoning uses machine learning with carefully designed features.

Carvalho and Cohn proposed Jangada, a system to remove quoted text and signature blocks from emails in the twenty newsgroup dataset~\cite{signature,20news}.
They first classify emails to find those that contain quoted text or signatures and then classify each line individually using Conditional Random Fields (CRF) and Collin's Perceptrons~\cite{crf,cperceptron}.
Reported accuracies range from 97\% to above 99\%.

Other researchers applied Jangada to Hotmail emails and measured accuracies around 64\%~\cite{profiling}.
With some adaptation, they managed to extract five different zones (author text, signature, advertisement, quoted text and reply lines) with an average accuracy of up to 88\%.

Lampert et al. developed the Zebra system~\cite{zones} as a pre-processor to their previously mentioned work on requests for action~\cite{rfa}.
Adversely to previous approaches, they use Support Vector Machines and therefore classify lines of an email into zones individually, rather than considering a sequence of lines.
For that, they describe graphic, orthographic, and lexical features to represent lines within their context reaching an average accuracy of 93\% on the two-zone task and 87\% on a nine-zone task.

Comparing the performance by zone type, most problems are caused by signature lines (F-score around 60\%), signoffs (70\%) and attachments (69\%).
It was found, that adding contextual features didn't improve the performance.
Contrary to our objectives, Zebra only tries to identify the zones within the very last message within an email thread and reject the rest as quoted text, whereas we aim to detect the zones across the entire email.

In our work, we aim to improve upon those results, while providing a system that is able to detect zones along the entire conversation thread contained in an email and not only the latest part.
Furthermore, our system uses neural networks so that feature engineering is not needed, which is potentially error-prone.
This way, even very small or incomplete datasets can be utilised for downstream tasks like social network analysis, speech acts and other research areas using email data.


%\cite{rfa} (detecting request for action) works better with zoning (see \cite{zones}), removes noise and focuses on text itself (excluding signature etc)

%\cite{zones} "Zebra" detects nine different zones, only last email in thread is considered, rest is quoted, classifies lines individually with features using SVM, three zones with 0.91 accuracy, 9 zones 0.87 accuracy; context features didn't add improve performance

%\cite{profiling} don't describe much, uses CRF inspired by \cite{signature}, five categories (author text, signature, advertisement, quoted text, reply lines), for eval using three zones with accuracy 0.88, F1 0.9, compared with \cite{signature} 0.64 and 0.75 on their data

%\cite{signature} "Jangada" detects signature and reply lines, only considers the last $n$ lines, using CRF with 0.97 accuracy, CPerceptron with 0.989 accuracy; extraction task only performed on mails known to contain signatures! used 20 newsgroup dataset \cite{20news}

%\cite{headerless} look for overlaps in enron dataset to split mails into parts of the conversation; closest to our task; we don't consider full dataset though (looking at each mail individually); their goal is to then reassemble the conversation threads, tested on 20 threads (leading to 465 mails), claim accuracy approaching 100\%

%\cite{similarity} similar to \cite{headerless}

%[citation needed] previous social network analysis wrong, because not considering full picture

%\cite{workhard} overview of recent technologies applied to enron and avocado (personal business classification + SNA)

%\cite{replying} also recent, on avocado, reply behaviour, only "main header"

%\cite{enron} enron corpus

%\cite{avocado} avocado corpus

%[citation needed] shneiderman corpus

%\cite{20news} 20 newsgroups

%https://github.com/mailgun/talon

\section{Segmentation of Emails}
\label{sec:model}
Systems for email segmentation that are discussed earlier are based on hand written rules to match common structures directly or use them as features for machine learning models.
Such an approach will fail when client headers are localised, formats are changed or quoted messages are edited by users or get corrupted.
In most cases it may seem obvious to the human eye how to segment an email into client headers and quoted text even though different or corrupted formats are used.

However, even a sophisticated text parsing program will fail since client headers follow no standardised format.
Usually lines start with attribute keywords such as "From:" or "Subject:", however their value may span multiple lines and use varying delimiters.
This even makes it hard to detect the boundaries between header and body blocks, since one can not rely on the presence of keywords or well formed, deterministic schemas.

\begin{figure}
	\centering{
		\resizebox{0.8\textwidth}{!}{\input{model.pdf_tex}}
		\caption{Schematic model overview; Left side shows line embedding stage using the CNN approach, right side outlines email zoning model.}
		\label{fig:model}
	}
\end{figure}

Therefore, our proposed model is based on multiple stages of neural network architectures as \Fref{fig:model} schematically shows.
In this section we describe how email text is represented and how classifiers can be used as a reliable and robust preprocessor for a simple program to extract its inherent structure.

\subsection{Representation of Email Data}
In the initial stage of our system the email text data is encoded into a low dimensional space.
This representation is used as input to later stages.
Here, the smallest fragment to be considered for email zoning are the lines in the email text.
The end of a line is delimited by a newline character (\textbackslash n).
Note however, that this may not necessarily be the same as floated lines displayed by an email program.
Analysis of the annotated data shows that this granularity is sufficient for all header, body, and signature zones as was assumed by other research on similar tasks.

Each line is encoded as a sequence of one-hot vectors representing respective characters.
We distinguish one hundred different case-sensitive alpha-numeric characters and basic ASCII symbols plus an out-of-scope placeholder.
This scope is sufficient for all email corpora we looked at, where only a negligible portion of characters exceeds this set.
For applications with Cyrillic, Arabic or other alphabets, this can easily be adapted.

Inspired by research on character-aware language models~\cite{char_nn}, we experiment with recurrent and convolutional neural networks.
The first model consists of a layer with varying number of gated recurrent units (GRU), where the last unit's output later serves as a fixed size embedding of the line.
The second model uses two convolutional layers, which scan the sequence of characters in a line and are intertwined by max-pooling and global-averaging layers finally leading into a densely connected layer, where the number of neurons corresponds to the embedding size as shown in the left in \Fref{fig:model}.

In both models, the line representations are learnt in a supervised fashion.
During training, a densely connected layer with softmax activation is appended so that a classifier can be trained to distinguish between lines of corresponding zone types.
Optimal parameters of the topology such as the number of layers and embedding size are determined experimentally.
A detailed analysis of embedding accuracy when limiting the length per line is found in \Fref{sec:modelpar}.


% parameters line num layers, embedding size etc experimentally determined
% limit line len
% learn char or line embedding

\subsection{Classification of Email Lines}
The first stage of our systems learns line representations by classifying them into zones.
That way however, the context in which a line appears is missing, resulting in less ideal performance on ambiguous or deceptive cases.
Thus, we add a second stage to our model for sequence to sequence classification, where the input is a sequence of line embeddings of an email corresponding to the right part in \Fref{fig:mode}.
Best performance was achieved with a bidirectional GRU layer, where the hidden states of each direction are concatenated, followed by a Conditional Random Field as model output.
Three of the five zone types only appear within message bodies, so we use two  concatenated embeddings as input, where one is pre-trained using two- and the other with five-zone classification.

In sequence to sequence classification, recurrent neural networks only consider the previous hidden state but neglect the actually predicted label sequence.
By using a bidirectional layer, we observed a small improvement over a unidirectional recurrent layer, since each line's context reflects the previous and following lines.
Like in language models~\cite{lstm_crf,lstm_cnn_crf}, the addition of a CRF to the output shows further performance gains.

We train the second stage of the system separate from the line encoding stage.
Directly connecting the embedding layer to the email line classifier described in this section and therefore training the entire model in one pass lead to unstable results even after pre-training the line encoder.

The sequence of zone types per line can be used to extract the conversational parts of an email and also separate the message from additional content such as signatures, greetings and signoffs.
Consecutive lines with the same predicted zone type are aggregated into a block.
Further processing inevitably requires making some assumptions about the general structure of emails.
Based on the analysis of emails in the training data, we assume that a body proceeds a header block.
Furthermore, we define that in-line replies with quoted parts belong to one message, which is not problematic, since this would usually only appear in Usenet-style emails, which use indicators like repeated $>$ at the beginning of a line and therefore don't require sophisticated processing as presented here.

Small errors in the prediction can be fixed heuristically, for example a block with a single line classified as header containing only the "Subject:" keyword likely is either a false positive or belongs to another block nearby.
Other similar rules could be constructed, which reduce the initial robustness of the model due to added assumptions.
In the scope of our work, we found that combining this model as a pre-processor for finding related blocks and rules to parse them significantly improves the accuracy of downstream tasks like constructing communication graphs by parsing header blocks compared to a purely rule-based parsed without pre-processing.

% continue training for classification into header/body
% how to split thread based on that

%\subsection{Extraction of Email Meta-Data}
%continue training for further classification of body and header parts individually
\subsection{Selection of Model Parameters}
\label{sec:modelpar}
In the description of the proposed model we highlighted adjustable parameters.
This includes the model for line representations in the first stage, limiting the length of each line, and finding the ideal dimension of line embeddings.
We base the model's topology configuration on the analysis of related models~\cite{char_nn,lstm_cnn_crf,lstm_crf}.
\begin{filecontents*}{dataevallines.csv}
	10,10,10,10,10,10,10,10,10,10,10,10,0.71,0.71,0.71,0.71,0.71,0.71,0.59,0.59,0.59,0.59,0.59,0.59,0.84,0.84,0.84,0.84,0.84,0.84,0.77,0.77,0.77,0.77,0.77,0.77,0.77,0.77,0.77,0.77,0.77,0.77,0.67,0.67,0.67,0.67,0.67,0.67,0.842061281337,0.842061281337,0.842061281337,0.842061281337,0.842061281337,0.842061281337,0.770334261838,0.770334261838,0.770334261838,0.770334261838,0.770334261838,0.770334261838
	20,20,20,20,20,20,20,20,20,20,20,20,0.93,0.94,0.93,0.93,0.93,0.93,0.85,0.85,0.86,0.85,0.85,0.84,0.93,0.94,0.93,0.94,0.93,0.93,0.71,0.79,0.83,0.76,0.74,0.62,0.93,0.94,0.93,0.94,0.93,0.93,0.76,0.81,0.84,0.79,0.77,0.69,0.933844011142,0.936908077994,0.933983286908,0.936490250696,0.927158774373,0.933844011142,0.711838440111,0.785515320334,0.833704735376,0.755849582173,0.735793871866,0.621727019499
	30,30,30,30,30,30,30,30,30,30,30,30,0.95,0.94,0.94,0.94,0.94,0.94,0.85,0.85,0.86,0.85,0.86,0.86,0.95,0.94,0.94,0.94,0.94,0.94,0.8,0.79,0.82,0.79,0.84,0.82,0.95,0.94,0.94,0.94,0.94,0.94,0.82,0.81,0.84,0.81,0.85,0.84,0.94721448468,0.940111420613,0.941364902507,0.942061281337,0.937883008357,0.936629526462,0.797075208914,0.790668523677,0.823259052925,0.791922005571,0.842757660167,0.824233983287
	40,40,40,40,40,40,40,40,40,40,40,40,0.95,0.95,0.93,0.89,0.94,0.95,0.82,0.86,0.87,0.86,0.87,0.87,0.95,0.95,0.93,0.85,0.94,0.95,0.52,0.83,0.82,0.83,0.8,0.82,0.95,0.95,0.93,0.86,0.94,0.95,0.59,0.85,0.84,0.84,0.83,0.84,0.947353760446,0.948328690808,0.927298050139,0.9059164345404,0.943871866295,0.953342618384,0.516155988858,0.833983286908,0.819777158774,0.833426183844,0.80069637883,0.820334261838
	50,50,50,50,50,50,50,50,50,50,50,50,0.94,0.95,0.95,0.95,0.93,0.93,0.86,0.87,0.87,0.87,0.88,0.87,0.94,0.95,0.95,0.95,0.93,0.93,0.81,0.83,0.8,0.83,0.81,0.82,0.94,0.95,0.95,0.95,0.93,0.93,0.83,0.84,0.82,0.85,0.84,0.84,0.938161559889,0.949860724234,0.95208913649,0.947075208914,0.92938718663,0.927158774373,0.806824512535,0.827576601671,0.79860724234,0.826601671309,0.81295264624,0.823955431755
	60,60,60,60,60,60,60,60,60,60,60,60,0.92,0.95,0.95,0.94,0.95,0.93,0.86,0.86,0.87,0.86,0.87,0.87,0.92,0.95,0.95,0.94,0.95,0.93,0.81,0.82,0.8,0.79,0.83,0.79,0.92,0.95,0.95,0.94,0.95,0.93,0.83,0.84,0.82,0.81,0.84,0.82,0.918245125348,0.950278551532,0.948746518106,0.942479108635,0.949442896936,0.933844011142,0.810167130919,0.824651810585,0.796796657382,0.785097493036,0.825626740947,0.785515320334
	70,70,70,70,70,70,70,70,70,70,70,70,0.95,0.94,0.95,0.95,0.95,0.94,0.87,0.83,0.84,0.87,0.83,0.86,0.95,0.94,0.95,0.95,0.95,0.94,0.79,0.49,0.67,0.83,0.6,0.81,0.95,0.94,0.95,0.95,0.95,0.94,0.82,0.57,0.73,0.85,0.66,0.83,0.948189415042,0.939554317549,0.952367688022,0.95348189415,0.953760445682,0.941922005571,0.793871866295,0.485097493036,0.751587743733,0.832451253482,0.596796657382,0.813509749304
	80,80,80,80,80,80,80,80,80,80,80,80,0.94,0.95,0.94,0.94,0.95,0.94,0.87,0.86,0.85,0.87,0.83,0.87,0.94,0.95,0.94,0.94,0.95,0.94,0.8,0.81,0.54,0.8,0.46,0.83,0.94,0.95,0.94,0.94,0.95,0.94,0.83,0.83,0.63,0.83,0.55,0.85,0.940668523677,0.94860724234,0.943871866295,0.943036211699,0.950417827298,0.944011142061,0.802367688022,0.808077994429,0.7235515320334,0.803760445682,0.461838440111,0.828412256267
	90,90,90,90,90,90,90,90,90,90,90,90,0.94,0.94,0.95,0.95,0.95,0.93,0.86,0.87,0.88,0.87,0.87,0.87,0.94,0.94,0.95,0.95,0.95,0.93,0.8,0.81,0.82,0.8,0.83,0.83,0.94,0.94,0.95,0.95,0.95,0.93,0.82,0.83,0.84,0.83,0.85,0.85,0.937883008357,0.942896935933,0.948050139276,0.94860724234,0.950835654596,0.93217270195,0.80278551532,0.805988857939,0.822144846797,0.798746518106,0.832033426184,0.832729805014
	100,100,100,100,100,100,100,100,100,100,100,100,0.94,0.95,0.95,0.95,0.93,0.95,0.87,0.87,0.87,0.87,0.87,0.87,0.94,0.95,0.95,0.95,0.92,0.95,0.81,0.82,0.83,0.79,0.8,0.83,0.94,0.95,0.95,0.95,0.93,0.95,0.83,0.84,0.85,0.82,0.83,0.84,0.943175487465,0.948885793872,0.947632311978,0.947632311978,0.923816155989,0.948885793872,0.808774373259,0.820891364903,0.833844011142,0.792479108635,0.796239554318,0.825487465181
	110,110,110,110,110,110,110,110,110,110,110,110,0.95,0.94,0.94,0.95,0.95,0.95,0.88,0.87,0.87,0.87,0.85,0.88,0.95,0.95,0.94,0.95,0.95,0.95,0.81,0.73,0.8,0.82,0.65,0.82,0.95,0.94,0.94,0.95,0.95,0.95,0.84,0.78,0.83,0.84,0.72,0.85,0.948746518106,0.945543175487,0.937883008357,0.948467966574,0.947353760446,0.948467966574,0.814902506964,0.726601671309,0.801671309192,0.823676880223,0.654735376045,0.824930362117
	120,120,120,120,120,120,120,120,120,120,120,120,0.94,0.94,0.95,0.94,0.94,0.95,0.87,0.87,0.87,0.86,0.87,0.87,0.94,0.94,0.95,0.94,0.94,0.95,0.79,0.8,0.79,0.79,0.84,0.81,0.94,0.94,0.95,0.94,0.94,0.95,0.82,0.83,0.82,0.82,0.85,0.84,0.942618384401,0.941364902507,0.949860724234,0.944846796657,0.943175487465,0.946657381616,0.793593314763,0.80069637883,0.791504178273,0.788579387187,0.83704735376,0.814902506964
	130,130,130,130,130,130,130,130,130,130,130,130,0.94,0.93,0.95,0.95,0.94,0.94,0.88,0.86,0.87,0.87,0.87,0.87,0.94,0.92,0.95,0.95,0.94,0.94,0.56,0.82,0.78,0.77,0.81,0.8,0.94,0.92,0.95,0.95,0.94,0.94,0.66,0.84,0.82,0.81,0.84,0.82,0.942757660167,0.920891364903,0.94791086351,0.948746518106,0.939554317549,0.935236768802,0.560863509749,0.816573816156,0.781754874652,0.774791086351,0.811559888579,0.795543175487
	140,140,140,140,140,140,140,140,140,140,140,140,0.93,0.95,0.94,0.95,0.95,0.95,0.89,0.87,0.87,0.86,0.87,0.87,0.93,0.95,0.94,0.95,0.95,0.95,0.55,0.83,0.79,0.81,0.83,0.77,0.93,0.95,0.94,0.95,0.95,0.95,0.65,0.85,0.82,0.83,0.84,0.8,0.930362116992,0.947632311978,0.943036211699,0.95069637883,0.951949860724,0.946657381616,0.547493036212,0.82938718663,0.787604456825,0.805431754875,0.828272980501,0.769080779944
	150,150,150,150,150,150,150,150,150,150,150,150,0.95,0.93,0.94,0.94,0.94,0.94,0.88,0.87,0.87,0.87,0.86,0.87,0.95,0.93,0.94,0.94,0.94,0.94,0.39,0.81,0.81,0.69,0.79,0.8,0.95,0.93,0.94,0.94,0.94,0.94,0.47,0.83,0.84,0.76,0.82,0.83,0.949025069638,0.93147632312,0.938718662953,0.941504178273,0.938022284123,0.935793871866,0.391225626741,0.806545961003,0.812813370474,0.694428969359,0.786629526462,0.796378830084
\end{filecontents*}
% [(0, 'linelen216cnn'), (1, 'linelen216rnn'), (2, 'linelen232cnn'), (3, 'linelen232rnn'), (4, 'linelen264cnn'), (5, 'linelen264rnn'), (6, 'linelen516cnn'), (7, 'linelen516rnn'), (8, 'linelen532cnn'), (9, 'linelen532rnn'), (10, 'linelen564cnn'), (11, 'linelen564rnn'), (12, 'prec216cnn'), (13, 'prec216rnn'), (14, 'prec232cnn'), (15, 'prec232rnn'), (16, 'prec264cnn'), (17, 'prec264rnn'), (18, 'prec516cnn'), (19, 'prec516rnn'), (20, 'prec532cnn'), (21, 'prec532rnn'), (22, 'prec564cnn'), (23, 'prec564rnn'), (24, 'rec216cnn'), (25, 'rec216rnn'), (26, 'rec232cnn'), (27, 'rec232rnn'), (28, 'rec264cnn'), (29, 'rec264rnn'), (30, 'rec516cnn'), (31, 'rec516rnn'), (32, 'rec532cnn'), (33, 'rec532rnn'), (34, 'rec564cnn'), (35, 'rec564rnn'), (36, 'f1216cnn'), (37, 'f1216rnn'), (38, 'f1232cnn'), (39, 'f1232rnn'), (40, 'f1264cnn'), (41, 'f1264rnn'), (42, 'f1516cnn'), (43, 'f1516rnn'), (44, 'f1532cnn'), (45, 'f1532rnn'), (46, 'f1564cnn'), (47, 'f1564rnn'), (48, 'acc216cnn'), (49, 'acc216rnn'), (50, 'acc232cnn'), (51, 'acc232rnn'), (52, 'acc264cnn'), (53, 'acc264rnn'), (54, 'acc516cnn'), (55, 'acc516rnn'), (56, 'acc532cnn'), (57, 'acc532rnn'), (58, 'acc564cnn'), (59, 'acc564rnn')]
\begin{figure}
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\begin{tikzpicture} % 16: 48,49 | 32: 50,51 | 64: 52,53 (cnn,rnn|2zone)
		\begin{axis}[width=0.98\textwidth, height=0.66\textwidth,
		             every tick label/.append style={font=\scriptsize},
		             every label/.append style={font=\scriptsize}, ylabel near ticks,
		             mark options={solid,scale=0.7},  xlabel near ticks, 
		             legend style={at={(0.99,0.01)},anchor=south east},
		             xlabel={maximum line length}, ylabel={Accuracy}]
		\addplot[solid, color=black, mark=x] table[x index={0},y index={50}, col sep=comma] {dataevallines.csv};
		\addlegendentry{\tiny CNN};
		\addplot[dashed, color=black, mark=o] table[x index={0},y index={51}, col sep=comma] {dataevallines.csv};
		\addlegendentry{\tiny RNN};
		\end{axis}
		\end{tikzpicture}
		%\caption{Two Zone Model}
	\end{subfigure}%
	~ 
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\begin{tikzpicture} % 16: 54,55 | 32: 56,57 | 64: 58,59 (cnn,rnn|5zone)
		\begin{axis}[width=0.98\textwidth, height=0.66\textwidth,
		mark options={solid,scale=0.7}, legend style={at={(0.99,0.01)},anchor=south east},
		every tick label/.append style={font=\scriptsize},
		every label/.append style={font=\scriptsize}, 
		yticklabel pos=right, ylabel near ticks, xlabel near ticks,
		xlabel={maximum line length}, ylabel={Accuracy}]
		\addplot[solid, color=black, mark=x] table[x index={0},y index={56}, col sep=comma] {dataevallines.csv};
		%\addlegendentry{\tiny CNN};
		\addplot[dashed, color=black, mark=o] table[x index={0},y index={57}, col sep=comma] {dataevallines.csv};
		%\addlegendentry{\tiny RNN};
		\end{axis}
		\end{tikzpicture}
		%\caption{Five Zone Model}
	\end{subfigure}
	\caption{Accuracy of $d$-dimensional line embeddings at increasing maximum line length; tested $d\in\{16, 32, 48, 64\}$, showing $d=32$; Trained using two-zone (left) and five-zone (right) classification}
	\label{fig:gridsearch}
\end{figure}

The ideal configuration for the embedding stage is determined through grid search across mentioned parameters.
We record the accuracy of the convolutional and recurrent approach for line encoding in \Fref{fig:gridsearch}.
Note, that the convolutional model assumes a fixed size input, so shorter lines are zero-padded at the end.
When evaluating the reported accuracy, one has to consider two things.
First, this metric may not project down to the later stage of our system, and second, the line type distribution bias reduces the range of values to 0.81 (or 0.65) upwards.

We did not observe significant differences between embedding dimensions above 32, so we choose this dimensionality in favour of a less complex model.
Most errors are caused by blank lines, which are usually classified as "Body".
Results when training using two-zone classification are mostly stable for both models.
The majority of lines in our training data are between forty and fifty characters long.
Interestingly, we observe a drop in performance of the convolutional model for fixed input lengths just over this average in the five-zone classification.

Both approaches for line representations seem to have their strengths and weaknesses.
Since there is no clear winner, we continue only using the convolutional model in this work.
We do so based on the argument, that one may want to process large corpora and prefers a faster model.

% conv cant handle much padding, rnn better at that
% avg line len 40-50, so choose cnn for simplicity
% robustness?

%\subsection{Flexibility of the Model}
%perturbation resistance
%size of training dataset (how much annotated data needed)
%cross corpus (?)


\section{Experimental Setup}
In this section we present an overview of the email datasets we used and discuss the sampling of emails to create an unbiased evaluation set.
Further, we describe competing approaches that are used as baselines for comparison of our results.
We also analyse model parameters and its robustness to changes in email text.

\subsection{Dataset}
We evaluate our proposed approach to email zoning on the Enron corpus~\cite{enron} and emails gathered from public mail archives of the Apache Software Foundation (ASF)\footnote{\url{http://mail-archives.apache.org/mod\_mbox/}}. 
Estival et al. and Lampert et al. discussed shortcomings in working with Usenet-style emails, leading us to refrain from using the twenty newsgroup dataset as was done for the Jangada system ~\cite{profiling,zones,20news}.
We found that more recent email threads from the ASF archives, especially those on mailinglists for users of different software projects, offer diverse formatting patterns.

Each dataset is divided into three subsets for training, validation, and testing.
Emails are sampled at random from their respective original dataset and put into one of those subsets.
To ensure representative results that are not biased by author or domain, sampling per subset is restricted to distinct mailboxes (Enron) or mailing lists (ASF).
The ASF dataset was compiled by randomly selecting emails from the \textit{flink-user}, \textit{spark-user}, and \textit{lucene-solr-user} mailing list archives.

Prior to sampling from the Enron corpus, duplicates are removed based on sender, recipient(s), and timestamp, which are provided by structured email meta data.
This reduces the corpus almost by 50\%, which confirms the assumption of a full dataset, where an email in one's sent folder is also found in someone else's archive.

\begin{table}
	\caption{Annotated Datasets in Numbers}
	\label{tab:dataset}
	\centering
	\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} ccccccc}
		\toprule
		& \multicolumn{3}{c}{Enron} &  \multicolumn{3}{c}{ASF} \\
		\cmidrule{2-4}
		\cmidrule{5-7}
		                          & Train    & Test   & Eval   & Train   & Test   & Eval  \\
		\midrule
		Emails                    & 500      & 200    & 100    &  350   &  100   &   50   \\
		Individual messages       & 1048     & 474    & 233    &  934   &  226   &   108  \\
		Average length of threads & 3.5      & 3.6    & 3.5    &  3.5   &  3.7   &   3.1  \\
		Number of signatures      & 103      & 58     & 26     &  76    &  13    &   5    \\
		\bottomrule
	\end{tabular*}
\end{table}

%##### > ../../DATA/ENRON/ANNOTATED < #####
%=========== Train:
%num mails 500
%num mails - no thread 279
%num mails - with thread 221
%num actual messages 1048
%avg threadlen 2.096
%avg threadlen - nonzero 3.47963800905
%num headers per mail 0: 0.56, 1: 0.16, 2: 0.14, 3: 0.05, 4: 0.04, 5: 0.02, 6: 0.01, 7: 0.01, 8: 0.01, 18: 0.00
%num sigs 103
%avg sig len 6.73786407767
%avg email len (lines) 46.934
%avg message len (lines) 23.4055299539
%=========== Test:
%num mails 200
%num mails - no thread 94
%num mails - with thread 106
%num actual messages 474
%avg threadlen 2.37
%avg threadlen - nonzero 3.58490566038
%num headers per mail 0: 0.47, 1: 0.14, 2: 0.23, 3: 0.07, 4: 0.04, 5: 0.03, 6: 0.02, 7: 0.01, 8: 0.01, 9: 0.01, 15: 0.01
%num sigs 58
%avg sig len 6.81034482759
%avg email len (lines) 42.875
%avg message len (lines) 20.5965909091
%=========== Eval:
%num mails 101
%num mails - no thread 48
%num mails - with thread 53
%num actual messages 233
%avg threadlen 2.30693069307
%avg threadlen - nonzero 3.49056603774
%num headers per mail 0: 0.48, 1: 0.18, 2: 0.22, 3: 0.05, 4: 0.04, 5: 0.01, 6: 0.01, 8: 0.01, 20: 0.01
%num sigs 26
%avg sig len 5.53846153846
%avg email len (lines) 42.0594059406
%avg message len (lines) 20.3579545455
%
%##### > ../../DATA/ASF/ANNOTATED < #####
%=========== Train:
%num mails 358
%num mails - no thread 132
%num mails - with thread 226
%num actual messages 934
%avg threadlen 2.60893854749
%avg threadlen - nonzero 3.54867256637
%num headers per mail 0: 0.37, 1: 0.26, 2: 0.15, 3: 0.08, 4: 0.06, 5: 0.02, 6: 0.03, 7: 0.01, 8: 0.01, 9: 0.00, 10: 0.00, 11: 0.00, 13: 0.00
%num sigs 76
%avg sig len 6.28947368421
%avg email len (lines) 68.7988826816
%avg message len (lines) 26.1347438753
%=========== Test:
%num mails 91
%num mails - no thread 41
%num mails - with thread 50
%num actual messages 226
%avg threadlen 2.48351648352
%avg threadlen - nonzero 3.7
%num headers per mail 0: 0.45, 1: 0.22, 2: 0.11, 3: 0.07, 4: 0.05, 5: 0.03, 6: 0.02, 7: 0.02, 8: 0.02
%num sigs 13
%avg sig len 6.15384615385
%avg email len (lines) 66.8351648352
%avg message len (lines) 24.1651785714
%=========== Eval:
%num mails 45
%num mails - no thread 16
%num mails - with thread 29
%num actual messages 108
%avg threadlen 2.4
%avg threadlen - nonzero 3.1724137931
%num headers per mail 0: 0.36, 1: 0.33, 2: 0.13, 3: 0.09, 4: 0.02, 6: 0.02, 7: 0.04
%num sigs 5
%avg sig len 5.8
%avg email len (lines) 64.8666666667
%avg message len (lines) 26.4166666667

\Fref{tab:dataset} shows an overview of the selected dataset and the expected number of messages to be extracted.
Prior heuristic analysis of the Enron corpus estimated 60\% of emails to contain conversation threads~\cite{enron}, which is close to our annotated data.
On average an email has two parts with 20 lines per message.
Only a few messages contain a signature, which on average are six lines long.

Our selected subsets of the Enron corpus are annotated with spans of characters nested by level of detail of their type.
Hereby the most coarse span types are headers and bodies.
Span types within a header are Subject, Person Name, Email Address, Date, and Time.
Within a body the following span types are annotated: Greeting and Signoff, Person Name within those, and Signatures. Signatures are further broken down into Name, Organisation, Addresses, Phone Numbers, Attachments, and Disclaimers.

Spans containing names (or email addresses) are linked as alias if they refer to the same person.
Other supplementary information such as organisations a person works for or contact details are also linked accordingly.
Although this level of detail exceeds the requirements for the objective of this paper, it provides valuable data for future work and research on social networks, speech acts, or other tasks surrounding email\footnote{Different versions of the dataset are published on our web page:\\ \url{https://url-remov.ed/for/review}}.

%We hope to inspire future work in the area [knowledge graph]

% describe how data is selected from enron corpus (800: 500 train, 200 test, 100 eval)
% annotated very detailed, even with some entity linking and signature parts, considered level of detail: header, body, signature, intro/outro; one annotator
% some numbers comparing raw data and contained information in \Fref{tab:dataset}
% based on subject and sender/recipient around 60% threads\cite{enron}
% we removed duplicates based users and time, about 50%
% https://hpi.de//naumann/projects/web-science/business-communication-analysis/email-structure.html
% https://github.com/TimRepke/email-splitting


\subsection{Competing Approaches}
We compare our proposed model for extracting zones from emails against several other approaches.
Most notably, Jangada~\cite{signature} and Zebra~\cite{zones} are reimplemented with slight modifications to fit the more refined problem statement.
Both systems originally are intended to distinguish lines within an email, which are not part of the latest message of that thread.
Clearly that deviates from our goal to extract \textit{all} individual parts and detect zones with additional detail within those.
Since the systems are supposed to detect zones within the first part of the email, their features and underlying models should in principle also work on our task.

The source code for Jangada is freely available on the author's web page\footnote{\url{http://www.cs.cmu.edu/~vitor/software/jangada/}}.
We used the source code as a basis for an implementation in Python, however continue to use the same implementation of the Collins Perceptron for sequence labelling~\cite{cperceptron}, which is part of the MinorThird Library~\cite{minorthird}.
For the extraction of signatures, Jangada originally only considers the last ten lines of an email.
In our implementation, the perceptron performs a multi-class classification along all lines of the email corresponding to zone types defined earlier.
The model is trained with window-size 5 for 40 epochs.

The Zebra project web page\footnote{\url{http://zebra.thoughtlets.org/zoning.php}} does only provide annotated data, but not the system's source code. 
Gossen et al. implemented\footnote{\url{https://github.com/gerhardgossen/soZebra}} it for their work on classification of action items in emails~\cite{sozebra}.
We used that as a guideline for our adapted Python implementation.
The SVM is trained for a maximum of 200 iterations in a one-versus-rest fashion for multi-class classification using RBF kernels.

Both models originally weren't designed to find the inherent structure of an entire email, but rather only the latest message in a conversation along with zones within it.
It seems fair to assume that the approaches for zoning the last message in an email should easily transfer to following messages as well.
We use a selection of features from both models as input for a recurrent neural network with two GRU layers~\cite{gru}, which we will refer to as \textit{FeatureRNN}.

We compare those to our proposed model as described in \Fref{sec:model} using convolutions in the first stage and refer to it as \textit{Quagga}.

% baseline (regex+rules)
% reimplement zebra\footnote{\url{http://zebra.thoughtlets.org/zoning.php}} https://github.com/gerhardgossen/soZebra
% jangada\footnote{\url{http://www.cs.cmu.edu/~vitor/software/jangada/}}
% describe differences, what needs to be changed to make it fair
% RNN with features
% show which task is more or less comparable between all


\section{Results}
We presented a system for extracting parts of an email by classifying its lines into one of two (or five) zones.
In this section we compare Quagga to similar systems found in related work.
To get a good understanding of the versatility, we not only look at the results shown in \Fref{tab:results-comp}, but also consider the robustness against noise or otherwise changing data as well as how many training samples are required to get good results.

We were not able to reproduce reported accuracies of Zebra~\cite{zebra}, which given the nature of the features and individual classification of lines disregarding their position and context in an email was expected.
Jangada uses more general features and looks at a sliding window of lines and we got close to reported accuracies, especially for the ASF dataset, which is closer to the twenty newsgroup data the authors used~\cite{zones}.
Overall, our system shows very good performances and seamlessly adapts to other datasets without problems.

\begin{table}
	\centering
	\caption{Classifying emails into zones (Precision/Recall/Accuracy)}
	\label{tab:results-comp}
	\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}ccccc}
		\toprule
		Approach                & Zones & Enron & ASF & Others\footnote{Accuracy as reported in the original paper; Jangada only distinguishes between signature lines and "other", or additionally reply lines}\\ \midrule
		Jangada\cite{signature} & 2 & 0.89 / 0.88 / 0.88 & 0.97 / 0.97 / 0.97 & 0.99 \\
		Zebra\cite{zones}       & 2 & 0.66 / 0.25 / 0.25 & 0.88 / 0.18 / 0.18 & 0.90 \\
		FeatureRNN              & 2 & 0.98 / 0.98 / 0.97 & 0.97 / 0.95 / 0.94 & --   \\
		Quagga                  & 2 & \textbf{0.98 / 0.98 / 0.98} & \textbf{0.98 / 0.98 / 0.98} & --   \\
		\midrule
		Jangada\cite{signature} & 5 & 0.82 / 0.85 / 0.85 & 0.90 / 0.92 / 0.91 & 0.98 \\
		Zebra\cite{zones}       & 5 & 0.60 / 0.25 / 0.24 & 0.81 / 0.20 / 0.20 & 0.87 \\
		FeatureRNN              & 5 & 0.92 / 0.75 / 0.75 & 0.90 / 0.60 / 0.60 & -- \\
		Quagga                  & 5 & \textbf{0.93 / 0.93 / 0.93} & \textbf{0.95 / 0.95 / 0.95} & -- \\
		\bottomrule	
	\end{tabular*}
\end{table}
% confusion matrix

\paragraph{Number of Training Samples}
Complex neural network based machine learning models require lots of training samples to reliably proficiently solve a given task.
We limited the number of Enron emails shown to the network during training down to 10\% of the Enron training set, corresponding to 50 emails and then measured the performance whilst continuing to add training samples up to all 500 emails.
The model trained with the least data in this scenario only lags behind around 1\% in accuracy compared to a model trained on all data in both the two- and five-zone task.

%  0.1    0.2    0.3    0.4    0.5    0.6    0.7    0.8    0.9    1.0  Enron
% 0.978, 0.982, 0.986, 0.988, 0.984, 0.986, 0.987, 0.986, 0.987, 0.989 (2zone)
% 0.916, 0.924, 0.935, 0.923, 0.923, 0.938, 0.929, 0.920, 0.927, 0.922 (5zone)


\paragraph{Cross Corpus Compatibility}
% acc, prec, rec, f1 | wacc, wprec, wrec, wf1
% Enron -> ASF
% 2zone, test: 0.94, 0.94, 0.94, 0.94 | 0.59, 0.72, 0.60, 0.53
% 2zone, eval: 0.97, 0.97, 0.97, 0.97 | 0.70, 0.81, 0.70, 0.67
% 5zone, test: 0.89, 0.89, 0.89, 0.87 | 0.36, 0.61, 0.36, 0.33
% 5zone, eval: 0.89, 0.89, 0.89, 0.86 | 0.40, 0.72, 0.40, 0.40
% ASF -> Enron
% 2zone, test: 0.88, 0.88, 0.88, 0.86 | 0.67, 0.79, 0.67, 0.63
% 2zone, eval: 0.88, 0.88, 0.88, 0.85 | 0.65, 0.79, 0.65, 0.60
% 5zone, test: 0.78, 0.83, 0.78, 0.78 | 0.55, 0.69, 0.55, 0.55
% 5zone, eval: 0.81, 0.88, 0.81, 0.80 | 0.65, 0.76, 0.65, 0.64
Ideally, a system like Quagga would be trained once and work well on arbitrary emails.
The Enron and ASF datasets show more differences than there are in samples in the training and testing data of one corpus.
Thus we trained Quagga on Enron and tested it on ASF emails and vice versa.
We observe, that by training on ASF emails, Quagga does not generalise as well to emails from he other corpus (Accuracy: 0.86 or 0.80, for two- or five-zones) as the other way around (Accuracy: 0.94 or 0.86).
Compared to the results when training and testing using emails from the same base dataset, this results in a decrease of performance of around 4-10\%.

\paragraph{Robustness to Noise}
\begin{filecontents*}{perturbation.csv}
	0.0,0.97,0.9,0.98,0.92,0.99,0.9,0.89,0.82,0.88,0.81,0.66,0.6,0.95,0.6,0.98,0.75,0.99,0.92,0.88,0.85,0.18,0.2,0.25,0.25,0.96,0.68,0.98,0.81,0.99,0.91,0.86,0.82,0.26,0.25,0.24,0.23,0.948207826373,0.600460374877,0.984644238659,0.75301840347,0.989148306478,0.919763235778,0.88231156957,0.852655022858,0.181025978297,0.203387043736,0.248153792053,0.246395498769,0.95,0.78,0.98,0.87,0.97,0.65,0.79,0.4,0.4,0.46,0.45,0.35,0.95,0.76,0.98,0.87,0.97,0.59,0.63,0.25,0.41,0.32,0.49,0.18,0.95,0.75,0.98,0.87,0.97,0.54,0.59,0.19,0.37,0.27,0.44,0.11,0.945952308907,0.763584662282,0.978435928709,0.871080781539,0.968519964209,0.587631113398,0.626271051795,0.254430666932,0.414265489904,0.322617495573,0.494967779772,0.179980990317
	0.1,0.97,0.9,0.98,0.92,0.98,0.88,0.88,0.78,0.89,0.8,0.66,0.6,0.95,0.61,0.98,0.77,0.98,0.9,0.87,0.83,0.13,0.15,0.22,0.22,0.96,0.7,0.98,0.82,0.98,0.89,0.83,0.79,0.18,0.17,0.17,0.17,0.952975994739,0.614107201578,0.976673309108,0.772125190482,0.980269648142,0.903814534693,0.868362442855,0.830852186145,0.132686616245,0.151266030911,0.216035634744,0.215215097878,0.92,0.74,0.95,0.83,0.88,0.67,0.78,0.2,0.41,0.43,0.45,0.37,0.92,0.72,0.95,0.82,0.86,0.5,0.58,0.22,0.44,0.3,0.52,0.18,0.92,0.71,0.95,0.82,0.85,0.47,0.52,0.15,0.36,0.24,0.42,0.1,0.917982773201,0.716760283506,0.951752602852,0.815337546861,0.858233016996,0.503305130045,0.57965394213,0.22021953812,0.440315721133,0.303483490565,0.515933124918,0.183790089806
	0.2,0.96,0.88,0.96,0.89,0.97,0.87,0.87,0.77,0.91,0.81,0.63,0.58,0.95,0.62,0.96,0.8,0.97,0.89,0.86,0.81,0.1,0.12,0.19,0.19,0.96,0.7,0.96,0.83,0.97,0.87,0.82,0.76,0.11,0.11,0.12,0.12,0.95281157514,0.621506083525,0.956394326574,0.796741296448,0.970240052614,0.888687931601,0.859336537334,0.814089790177,0.0976652416968,0.123807957909,0.193881139374,0.193763919822,0.86,0.71,0.91,0.78,0.83,0.65,0.77,0.18,0.48,0.46,0.44,0.33,0.84,0.66,0.89,0.72,0.77,0.44,0.55,0.2,0.47,0.32,0.52,0.18,0.84,0.67,0.89,0.73,0.75,0.41,0.47,0.13,0.35,0.25,0.41,0.08,0.844797163483,0.65898563555,0.886270070469,0.724460021309,0.766010352641,0.436435473238,0.550823379876,0.203594272926,0.47030747147,0.321936026923,0.524049514944,0.181381164414
	0.3,0.96,0.88,0.93,0.88,0.96,0.85,0.86,0.77,0.87,0.81,0.65,0.59,0.95,0.63,0.93,0.81,0.96,0.88,0.85,0.82,0.07,0.1,0.18,0.19,0.95,0.71,0.92,0.84,0.96,0.86,0.8,0.77,0.06,0.07,0.09,0.09,0.951825057547,0.634330812233,0.928730512249,0.809283788536,0.963498849063,0.876192042091,0.851365607783,0.816668620326,0.0703715882933,0.0986517592897,0.184503575196,0.186027429375,0.82,0.7,0.85,0.72,0.79,0.59,0.76,0.19,0.4,0.45,0.45,0.36,0.77,0.62,0.8,0.62,0.67,0.39,0.53,0.2,0.46,0.31,0.54,0.18,0.75,0.63,0.79,0.61,0.63,0.35,0.43,0.13,0.32,0.23,0.4,0.08,0.767758621644,0.621016941808,0.795996770968,0.619520589631,0.673900468187,0.389863478922,0.525714265687,0.204673641619,0.456962251111,0.312029262724,0.53753165947,0.183758727117
	0.4,0.95,0.87,0.92,0.87,0.95,0.85,0.86,0.74,0.89,0.76,0.65,0.58,0.95,0.64,0.92,0.81,0.96,0.87,0.85,0.8,0.06,0.08,0.18,0.18,0.95,0.72,0.91,0.83,0.95,0.85,0.79,0.75,0.03,0.04,0.07,0.07,0.949852022361,0.638605721802,0.919587387176,0.808932129879,0.959388359092,0.871588293325,0.845856288829,0.799437346149,0.057218020388,0.0805656034199,0.178408158481,0.178994256242,0.79,0.68,0.84,0.71,0.77,0.61,0.76,0.17,0.43,0.38,0.45,0.42,0.71,0.6,0.76,0.58,0.64,0.39,0.51,0.19,0.47,0.28,0.54,0.18,0.68,0.62,0.75,0.58,0.58,0.35,0.39,0.12,0.32,0.2,0.4,0.07,0.710574878993,0.603108114499,0.759231480962,0.584691601645,0.639336858999,0.386673935914,0.506664844203,0.192766009286,0.46841031374,0.284838237659,0.54209190826,0.181380645161
	0.5,0.94,0.86,0.9,0.85,0.94,0.84,0.85,0.73,0.91,0.76,0.65,0.59,0.95,0.64,0.9,0.8,0.96,0.87,0.84,0.79,0.05,0.08,0.17,0.18,0.95,0.71,0.89,0.81,0.94,0.85,0.78,0.74,0.02,0.03,0.06,0.06,0.948865504768,0.637290365012,0.902707771656,0.798616809284,0.955771127918,0.866491285761,0.841519165397,0.792169733912,0.0526142716212,0.0774416310424,0.173836595944,0.175008791466,0.76,0.65,0.81,0.69,0.74,0.61,0.75,0.18,0.49,0.41,0.45,0.33,0.66,0.55,0.71,0.52,0.58,0.37,0.49,0.19,0.47,0.29,0.55,0.18,0.61,0.56,0.69,0.52,0.48,0.32,0.37,0.11,0.31,0.21,0.39,0.07,0.659034584793,0.548223936618,0.708818739603,0.516093094246,0.58124678481,0.366206617854,0.493604436532,0.186311734201,0.472672118137,0.294001663759,0.546220490103,0.180911094637
	0.6,0.94,0.85,0.89,0.83,0.93,0.83,0.84,0.72,0.96,0.81,0.66,0.52,0.95,0.64,0.89,0.79,0.95,0.86,0.84,0.78,0.05,0.07,0.17,0.17,0.94,0.72,0.87,0.8,0.94,0.83,0.77,0.72,0.01,0.02,0.06,0.06,0.949358763565,0.641072015784,0.889461962255,0.791231977494,0.95412693193,0.855804011838,0.837885359278,0.782206071973,0.0470240052614,0.0710292666886,0.172312741765,0.172078302661,0.74,0.61,0.8,0.66,0.73,0.67,0.74,0.18,0.75,0.44,0.46,0.43,0.62,0.47,0.66,0.46,0.57,0.34,0.48,0.18,0.47,0.29,0.55,0.18,0.55,0.48,0.63,0.46,0.45,0.28,0.35,0.1,0.31,0.2,0.39,0.06,0.620209171209,0.473360955528,0.661953961756,0.46256370195,0.566741387864,0.341755098701,0.48124088768,0.177054668515,0.474690240434,0.291610406068,0.54759668405,0.179915000673
\end{filecontents*}
\begin{filecontents*}{quagga.csv}
	0.0,0.98,0.95,0.98,0.93,0.98,0.84,0.98,0.85,1.0,0.95,0.99,0.93,0.97,0.89,0.98,0.83,0.98,0.95,0.98,0.93,0.98,0.74,0.98,0.63,1.0,0.96,0.99,0.93,0.97,0.76,0.98,0.64,0.98,0.95,0.98,0.93,0.98,0.72,0.98,0.62,1.0,0.95,0.99,0.93,0.97,0.76,0.98,0.62,0.978132193357,0.947550147978,0.980658773883,0.933302074786,0.97952995927,0.736357207167,0.980676340494,0.630610194037,0.995889003083,0.956491949298,0.985875706215,0.931732580038,0.971725836235,0.760005975569,0.978492654106,0.637074423646
	0.1,0.98,0.94,0.98,0.92,0.95,0.78,0.98,0.84,0.99,0.94,0.99,0.92,0.96,0.88,0.98,0.82,0.98,0.94,0.98,0.92,0.95,0.67,0.98,0.55,0.99,0.94,0.99,0.93,0.95,0.69,0.98,0.57,0.98,0.94,0.98,0.91,0.95,0.64,0.98,0.54,0.99,0.93,0.99,0.92,0.95,0.72,0.98,0.55,0.97780335416,0.943604077606,0.979252139257,0.923103973743,0.952238090774,0.667366857883,0.975445387362,0.554641876607,0.994176087701,0.938677629325,0.985404896422,0.925612052731,0.952164036684,0.692774221792,0.975486934846,0.57068312721
	0.2,0.98,0.93,0.98,0.91,0.94,0.81,0.97,0.83,0.99,0.92,0.98,0.91,0.91,0.73,0.97,0.82,0.98,0.94,0.98,0.92,0.93,0.59,0.97,0.52,0.99,0.92,0.98,0.92,0.89,0.59,0.96,0.56,0.98,0.93,0.98,0.91,0.93,0.58,0.97,0.48,0.99,0.91,0.98,0.91,0.89,0.6,0.96,0.54,0.978132193357,0.935054258468,0.977962724182,0.921228460907,0.930713980645,0.594436913915,0.966175528663,0.51519625885,0.989037341555,0.923261390887,0.981403013183,0.922080979284,0.893478638031,0.594341281277,0.964379608005,0.564456091599
	0.3,0.98,0.91,0.96,0.91,0.88,0.82,0.94,0.84,0.98,0.9,0.97,0.92,0.85,0.69,0.94,0.82,0.98,0.92,0.96,0.91,0.85,0.5,0.93,0.49,0.98,0.9,0.97,0.93,0.78,0.5,0.94,0.57,0.98,0.91,0.96,0.9,0.85,0.5,0.93,0.45,0.98,0.89,0.97,0.92,0.77,0.5,0.94,0.54,0.975501479776,0.921243012167,0.963310280155,0.912436994491,0.853402970926,0.50374680638,0.931046872019,0.490908512492,0.978759849263,0.904076738609,0.973399246704,0.930320150659,0.783577502787,0.49820784711,0.937260366329,0.565774945243
	0.4,0.97,0.88,0.95,0.88,0.82,0.58,0.91,0.82,0.97,0.88,0.95,0.9,0.81,0.7,0.9,0.82,0.97,0.9,0.95,0.9,0.73,0.38,0.89,0.42,0.97,0.88,0.95,0.9,0.7,0.39,0.87,0.47,0.97,0.88,0.95,0.87,0.71,0.34,0.89,0.34,0.97,0.86,0.95,0.89,0.67,0.39,0.87,0.42,0.971226570207,0.898388687932,0.95252608135,0.895909037627,0.728224579935,0.378532300951,0.892495385999,0.418157870686,0.972593353888,0.884892086331,0.950329566855,0.904896421846,0.701950531309,0.392675324114,0.872813164251,0.468179714828
	0.5,0.96,0.87,0.93,0.88,0.79,0.6,0.88,0.79,0.97,0.88,0.94,0.9,0.79,0.72,0.88,0.78,0.97,0.89,0.94,0.89,0.66,0.32,0.85,0.41,0.97,0.88,0.94,0.9,0.65,0.38,0.86,0.44,0.96,0.86,0.93,0.86,0.61,0.27,0.85,0.35,0.96,0.85,0.94,0.88,0.6,0.37,0.86,0.37,0.965636303847,0.888523512003,0.9367014418,0.886648693002,0.658406642916,0.322141540016,0.853920829975,0.412917065603,0.967454607742,0.88215142172,0.943032015066,0.899246704331,0.646999963687,0.384490798723,0.85919424612,0.435160508038
	0.6,0.95,0.88,0.92,0.86,0.76,0.63,0.84,0.62,0.96,0.87,0.93,0.85,0.76,0.67,0.85,0.57,0.96,0.89,0.92,0.87,0.58,0.31,0.79,0.37,0.96,0.87,0.92,0.89,0.55,0.3,0.8,0.39,0.95,0.86,0.91,0.84,0.49,0.24,0.78,0.29,0.94,0.83,0.92,0.86,0.43,0.25,0.79,0.3,0.961361394278,0.890989805985,0.920994021803,0.870003516587,0.576620232924,0.308752411129,0.791307670144,0.369808410608,0.960260363138,0.869133264817,0.924670433145,0.886770244821,0.546166250415,0.298690776246,0.795921662903,0.392229417187
\end{filecontents*}
% 	[(0, 'perturbation'), (1, 'prec-featrnn-asf-2'), (2, 'prec-featrnn-asf-5'), (3, 'prec-featrnn-enron-2'), (4, 'prec-featrnn-enron-5'), (5, 'prec-jangada-asf-2'), (6, 'prec-jangada-asf-5'), (7, 'prec-jangada-enron-2'), (8, 'prec-jangada-enron-5'), (9, 'prec-zebra-asf-2'), (10, 'prec-zebra-asf-5'), (11, 'prec-zebra-enron-2'), (12, 'prec-zebra-enron-5'), (13, 'rec-featrnn-asf-2'), (14, 'rec-featrnn-asf-5'), (15, 'rec-featrnn-enron-2'), (16, 'rec-featrnn-enron-5'), (17, 'rec-jangada-asf-2'), (18, 'rec-jangada-asf-5'), (19, 'rec-jangada-enron-2'), (20, 'rec-jangada-enron-5'), (21, 'rec-zebra-asf-2'), (22, 'rec-zebra-asf-5'), (23, 'rec-zebra-enron-2'), (24, 'rec-zebra-enron-5'), (25, 'f1-featrnn-asf-2'), (26, 'f1-featrnn-asf-5'), (27, 'f1-featrnn-enron-2'), (28, 'f1-featrnn-enron-5'), (29, 'f1-jangada-asf-2'), (30, 'f1-jangada-asf-5'), (31, 'f1-jangada-enron-2'), (32, 'f1-jangada-enron-5'), (33, 'f1-zebra-asf-2'), (34, 'f1-zebra-asf-5'), (35, 'f1-zebra-enron-2'), (36, 'f1-zebra-enron-5'), (37, 'acc-featrnn-asf-2'), (38, 'acc-featrnn-asf-5'), (39, 'acc-featrnn-enron-2'), (40, 'acc-featrnn-enron-5'), (41, 'acc-jangada-asf-2'), (42, 'acc-jangada-asf-5'), (43, 'acc-jangada-enron-2'), (44, 'acc-jangada-enron-5'), (45, 'acc-zebra-asf-2'), (46, 'acc-zebra-asf-5'), (47, 'acc-zebra-enron-2'), (48, 'acc-zebra-enron-5'), (49, 'wprec-featrnn-asf-2'), (50, 'wprec-featrnn-asf-5'), (51, 'wprec-featrnn-enron-2'), (52, 'wprec-featrnn-enron-5'), (53, 'wprec-jangada-asf-2'), (54, 'wprec-jangada-asf-5'), (55, 'wprec-jangada-enron-2'), (56, 'wprec-jangada-enron-5'), (57, 'wprec-zebra-asf-2'), (58, 'wprec-zebra-asf-5'), (59, 'wprec-zebra-enron-2'), (60, 'wprec-zebra-enron-5'), (61, 'wrec-featrnn-asf-2'), (62, 'wrec-featrnn-asf-5'), (63, 'wrec-featrnn-enron-2'), (64, 'wrec-featrnn-enron-5'), (65, 'wrec-jangada-asf-2'), (66, 'wrec-jangada-asf-5'), (67, 'wrec-jangada-enron-2'), (68, 'wrec-jangada-enron-5'), (69, 'wrec-zebra-asf-2'), (70, 'wrec-zebra-asf-5'), (71, 'wrec-zebra-enron-2'), (72, 'wrec-zebra-enron-5'), (73, 'wf1-featrnn-asf-2'), (74, 'wf1-featrnn-asf-5'), (75, 'wf1-featrnn-enron-2'), (76, 'wf1-featrnn-enron-5'), (77, 'wf1-jangada-asf-2'), (78, 'wf1-jangada-asf-5'), (79, 'wf1-jangada-enron-2'), (80, 'wf1-jangada-enron-5'), (81, 'wf1-zebra-asf-2'), (82, 'wf1-zebra-asf-5'), (83, 'wf1-zebra-enron-2'), (84, 'wf1-zebra-enron-5'), (85, 'wacc-featrnn-asf-2'), (86, 'wacc-featrnn-asf-5'), (87, 'wacc-featrnn-enron-2'), (88, 'wacc-featrnn-enron-5'), (89, 'wacc-jangada-asf-2'), (90, 'wacc-jangada-asf-5'), (91, 'wacc-jangada-enron-2'), (92, 'wacc-jangada-enron-5'), (93, 'wacc-zebra-asf-2'), (94, 'wacc-zebra-asf-5'), (95, 'wacc-zebra-enron-2'), (96, 'wacc-zebra-enron-5')]
% [(0, 'perturbation'), (1, 'test--prec-asf-2'), (2, 'test--prec-asf-5'), (3, 'test--prec-enron-2'), (4, 'test--prec-enron-5'), (5, 'test-w-prec-asf-2'), (6, 'test-w-prec-asf-5'), (7, 'test-w-prec-enron-2'), (8, 'test-w-prec-enron-5'), (9, 'eval--prec-asf-2'), (10, 'eval--prec-asf-5'), (11, 'eval--prec-enron-2'), (12, 'eval--prec-enron-5'), (13, 'eval-w-prec-asf-2'), (14, 'eval-w-prec-asf-5'), (15, 'eval-w-prec-enron-2'), (16, 'eval-w-prec-enron-5'), (17, 'test--rec-asf-2'), (18, 'test--rec-asf-5'), (19, 'test--rec-enron-2'), (20, 'test--rec-enron-5'), (21, 'test-w-rec-asf-2'), (22, 'test-w-rec-asf-5'), (23, 'test-w-rec-enron-2'), (24, 'test-w-rec-enron-5'), (25, 'eval--rec-asf-2'), (26, 'eval--rec-asf-5'), (27, 'eval--rec-enron-2'), (28, 'eval--rec-enron-5'), (29, 'eval-w-rec-asf-2'), (30, 'eval-w-rec-asf-5'), (31, 'eval-w-rec-enron-2'), (32, 'eval-w-rec-enron-5'), (33, 'test--f1-asf-2'), (34, 'test--f1-asf-5'), (35, 'test--f1-enron-2'), (36, 'test--f1-enron-5'), (37, 'test-w-f1-asf-2'), (38, 'test-w-f1-asf-5'), (39, 'test-w-f1-enron-2'), (40, 'test-w-f1-enron-5'), (41, 'eval--f1-asf-2'), (42, 'eval--f1-asf-5'), (43, 'eval--f1-enron-2'), (44, 'eval--f1-enron-5'), (45, 'eval-w-f1-asf-2'), (46, 'eval-w-f1-asf-5'), (47, 'eval-w-f1-enron-2'), (48, 'eval-w-f1-enron-5'), (49, 'test--acc-asf-2'), (50, 'test--acc-asf-5'), (51, 'test--acc-enron-2'), (52, 'test--acc-enron-5'), (53, 'test-w-acc-asf-2'), (54, 'test-w-acc-asf-5'), (55, 'test-w-acc-enron-2'), (56, 'test-w-acc-enron-5'), (57, 'eval--acc-asf-2'), (58, 'eval--acc-asf-5'), (59, 'eval--acc-enron-2'), (60, 'eval--acc-enron-5'), (61, 'eval-w-acc-asf-2'), (62, 'eval-w-acc-asf-5'), (63, 'eval-w-acc-enron-2'), (64, 'eval-w-acc-enron-5')]
\begin{figure}
	% jan, zeb, fea
	% acc5: 44,48,40 | wacc5: 92,96,88 | acc2: 43,47,39, wacc2: 91,95,87 | f1_2: 31,35,27
	% quagga: acc5: 52  | wacc5: 55 | acc2: 51 | wacc2: 56
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\begin{tikzpicture}
		\begin{axis}[width=0.98\textwidth, height=0.8\textwidth,
		every tick label/.append style={font=\scriptsize},
		every label/.append style={font=\scriptsize}, ylabel near ticks, xlabel near ticks,
		mark options={solid,scale=0.7},legend style={at={(0.01,0.65)},anchor=north west},,
		xlabel={Perturbation}, ylabel={Accuracy}]
		\addplot[dotted, color=black, mark=o] table[x index={0},y index={43}, col sep=comma] {perturbation.csv};
		\addlegendentry{\tiny Jangada};
		\addplot[dashed, color=black, mark=o] table[x index={0},y index={47}, col sep=comma] {perturbation.csv};
		\addlegendentry{\tiny Zebra};
		\addplot[dotted, color=black, mark=x] table[x index={0},y index={87}, col sep=comma] {perturbation.csv};
		\addlegendentry{\tiny RNN};
		\addplot[solid, color=black, mark=*] table[x index={0},y index={51}, col sep=comma] {quagga.csv};
		\addlegendentry{\tiny Quagga};
		\end{axis}
		\end{tikzpicture}
		%\caption{Two Zone Model}
	\end{subfigure}%
	~ 
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\begin{tikzpicture}
		\begin{axis}[width=0.98\textwidth, height=0.8\textwidth,
		mark options={solid,scale=0.7}, 
		every tick label/.append style={font=\scriptsize},
		every label/.append style={font=\scriptsize}, 
		legend style={at={(0.01,0.65)},anchor=north west},
		yticklabel pos=right, ylabel near ticks, xlabel near ticks,
		xlabel={Perturbation}, ylabel={Accuracy}]
		\addplot[dotted, color=black, mark=o] table[x index={0},y index={44}, col sep=comma] {perturbation.csv};
		%\addlegendentry{\tiny Jangada};
		\addplot[dashed, color=black, mark=o] table[x index={0},y index={48}, col sep=comma] {perturbation.csv};
		%\addlegendentry{\tiny Zebra};
		\addplot[dotted, color=black, mark=x] table[x index={0},y index={88}, col sep=comma] {perturbation.csv};
		%\addlegendentry{\tiny RNN};
		\addplot[solid, color=black, mark=*] table[x index={0},y index={52}, col sep=comma] {quagga.csv};
		%\addlegendentry{\tiny Quagga};
		\end{axis}
		\end{tikzpicture}
		%\caption{Five Zone Model}
	\end{subfigure}
	\caption{Robustness against perturbation on two (left) and five zones (right) on the Enron test set}
	\label{fig:perturbation}
\end{figure}
Our hypothesis is, that a model which learns meaningful features itself is more robust towards changes to the email text as hard coded rules responding to specific keywords or patterns.
To show the flexibility of our model, we first introduce the notion of a perturbation threshold $\rho\in[0,1)$.
Before passing an email to a model, a function iterates over each character and with probability $\rho$ edits, removes, or duplicates it.
Training was performed on uncorrupted data only.

The robustness of each model against increasing perturbation is shown in \Fref{fig:perturbation}.
Relatively, the drop in performance is the same the for two- and five-zone task, although at different absolute accuracies.
Quagga doesn't seem to be affected up to $\rho=0.2$ and keeps producing reliable results even at higher perturbation thresholds.
Surprisingly, also Jangada is not influenced significantly by the introduction of perturbation.
As opposed to Zebra and FeatureRNN, it is using more features related to small patterns or proportions of types of symbols, whereas the others depend on more complex patterns which are more error prone to change.

\section{Conclusion and Future Work}
In this work we presented a reliable and flexible system for finding the inherent structure of an email containing multiple conversational parts.
In the first stage, the system uses a convolutional neural network to encode lines of an email which are used by a GRU-CRF to predict a sequence of zone types per line reaching accuracies of 98\%.
Compared to similar models, we show significant improvement and especially seamless adaptation to other datasets as well as robustness against corrupted data.

Research based on email data largely benefits from preprocessed text input, which helps to focus algorithms on relevant parts of an email like client headers containing meta-data such as sender, recipients or dates for social network analysis, or content based research, which can be focused on the actual message without irrelevant parts like signatures.

In addition to our system, which can be used automatically pre-process large corpora, we provide a detailed annotation of a subset of the Enron corpus that can directly be used for building communication networks without further parsing including linked person aliases and, if present, contact details from email signatures.

% novel clean dataset for email research on enron
% flexible model to clean up large amounts of email data


%\Fref{fig:mpd} shows influence of extracting information vs using raw data
%\begin{figure}
%	\centering
%	\begin{subfigure}[t]{0.5\textwidth}
%		\centering
%		\dummyfig{0.17}{0.9}{xaxis=timebuckets, yaxis=freq} 
%		\caption{raw data}
%	\end{subfigure}%
%	~ 
%	\begin{subfigure}[t]{0.5\textwidth}
%		\centering
%		\dummyfig{0.17}{0.9}{xaxis=timebuckets, yaxis=freq} 
%		\caption{extracted data}
%	\end{subfigure}
%	\caption{Mail Frequency}
%	\label{fig:mpd}
%\end{figure}

\bibliographystyle{splncs03}
\bibliography{biblio} 
\end{document}
