{
  "wrapper": "plaintext",
  "text": "Hi,\n\nI've faced a similar issue recently. Hope sharing my findings will help.\nThe problem can be split into 2 parts:\n\n*Source of container failures*\nThe logs you provided indicate that YARN kills its containers for exceeding\nmemory limits. Important point here is that memory limit = JVM heap memory\n+ off-heap memory. So if off-heap memory usage is high, YARN may kill\ncontainers despite JVM heap consumption is fine. To solve this issue, Flink\nreserves a share of container memory for off-heap memory. How much will be\nreserved is controlled by yarn.heap-cutoff-ratio and\nyarn.heap-cutoff-min configuration.\nBy default 25% of the requested container memory will be reserved for\noff-heap. This is seems to be a good start, but one should experiment and\ntune to meet their job specifics.\n\nIt's also worthwhile to figure out who consumes off-heap memory. Is it\nFlink managed memory moved off heap (taskmanager.memory.off-heap = true)?\nIs it some external library allocating something off heap? Is it your own\ncode?\n\n*How Flink handles task manager failures*\nWhenever a task manager fails, the Flink jobmanager decides whether it\nshould:\n- reallocate failed task manager container\n- fail application entirely\nThese decisions can be guided by certain configuration (\nhttps://ci.apache.org/projects/flink/flink-docs-\nrelease-1.1/setup/yarn_setup.html#recovery-behavior-of-flink-on-yarn). With\ndefault settings, job manager does reallocate task manager containers up to\nthe point when N failures have been observed, where N is the number of\nrequested task managers. After that the application is stopped.\n\nAccording to the logs, you have a finite number in\nyarn.maximum-failed-containers (11, as I can see from the logs - this may\nbe set by Flink if not provided explicitly). On 12th container restart,\njobmanager gives up and the application stops. I'm not sure why it keeps\nreporting not enough slots after that point. In my experience this may\nhappen when job eats up all the available slots, so that after container\nfailure its tasks cannot be restarted in other (live) containers. But I\nbelieve once the decision to stop the application is made, there should not\nbe any further attempts to restart the job, hence no logs like those.\nHopefully, someone else will explain this to us :)\n\nIn my case I made jobmanager restart containers infinitely by setting\nyarn.maximum-failed-containers\n= -1, so that taskmanager failure never results in application death. Note\nthis is unlikely a good choice for a batch job.\n\nRegards,\nYury\n\n2017-01-05 3:21 GMT+03:00 Shannon Carey <scarey@expedia.com>:\n\n> In Flink 1.1.3 on emr-5.2.0, I've experienced a particular problem twice\n> and I'm wondering if anyone has some insight about it.\n>\n> In both cases, we deployed a job that fails very frequently (within 15s-1m\n> of launch). Eventually, the Flink cluster dies.\n>\n> The sequence of events looks something like this:\n>\n>    - bad job is launched\n>    - bad job fails & is restarted many times (I didn't have the\n>    \"failure-rate\" restart strategy configuration right)\n>    - Task manager logs: org.apache.flink.yarn.YarnTaskManagerRunner\n>    (SIGTERM handler): RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.\n>    - At this point, the YARN resource manager also logs the container\n>    failure\n>    - More logs: Container ResourceID{resourceId='\n>    container_1481658997383_0003_01_000013'} failed. Exit status: Pmem\n>    limit exceeded (-104)\n>    - Diagnostics for container ResourceID{resourceId='\n>    container_1481658997383_0003_01_000013'} in state COMPLETE :\n>    exitStatus=Pmem limit exceeded (-104) diagnostics=Container\n>    [pid=21246,containerID=container_1481658997383_0003_01_000013] is\n>    running beyond physical memory limits. Current usage: 5.6 GB of 5.6 GB\n>    physical memory used; 9.6 GB of 28.1 GB virtual memory used. Killing\n>    container.\n>    Container killed on request. Exit code is 143\n>    Container exited with a non-zero exit code 143\n>    Total number of failed containers so far: 12\n>    Stopping YARN session because the number of failed containers (12)\n>    exceeded the maximum failed containers (11). This number is controlled by\n>    the 'yarn.maximum-failed-containers' configuration setting. By default\n>    its the number of requested containers.\n>    - From here onward, the logs repeatedly show that jobs fail to restart\n>    due to \"org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:\n>    Not enough free slots available to run the job. You can decrease the\n>    operator parallelism or increase the number of slots per TaskManager in the\n>    configuration. Task to schedule: < Attempt #68 (Source: \u00e2\u20ac\u00a6) @ (unassigned) -\n>    [SCHEDULED] > with groupID < 73191c171abfff61fb5102c161274145 > in\n>    sharing group < SlotSharingGroup [73191c171abfff61fb5102c161274145,\n>    19596f7834805c8409c419f0edab1f1b] >. Resources available to scheduler:\n>    Number of instances=0, total number of slots=0, available slots=0\"\n>    - Eventually, Flink stops for some reason (with another SIGTERM\n>    message), presumably because of YARN\n>\n> Does anyone have an idea why a bad job repeatedly failing would eventually\n> result in the Flink cluster dying?\n>\n> Any idea why I'd get \"Pmem limit exceeded\" or \"Not enough free slots\n> available to run the job\"? The JVM heap usage and the free memory on the\n> machines both look reasonable in my monitoring dashboards. Could it\n> possibly be a memory leak due to classloading or something?\n>\n> Thanks for any help or suggestions you can provide! I am hoping that the\n> \"failure-rate\" restart strategy will help avoid this issue in the future,\n> but I'd also like to understand what's making the cluster die so that I can\n> prevent it.\n>\n> -Shannon\n>\n\n",
  "denotations": [
    {
      "id": 1,
      "start": 0,
      "end": 4,
      "text": "Hi,\n",
      "type": "Body/Intro",
      "meta": null
    },
    {
      "id": 2,
      "start": 2509,
      "end": 2524,
      "text": "\nRegards,\nYury\n",
      "type": "Body/Outro",
      "meta": null
    },
    {
      "id": 3,
      "start": 2525,
      "end": 2587,
      "text": "2017-01-05 3:21 GMT+03:00 Shannon Carey <scarey@expedia.com>:\n",
      "type": "Header",
      "meta": null
    },
    {
      "id": 4,
      "start": 5741,
      "end": 5754,
      "text": ">\n> -Shannon\n",
      "type": "Body/Outro",
      "meta": null
    },
    {
      "id": 5,
      "start": 2587,
      "end": 5757,
      "text": "\n> In Flink 1.1.3 on emr-5.2.0, I've experienced a particular problem twice\n> and I'm wondering if anyone has some insight about it.\n>\n> In both cases, we deployed a job that fails very frequently (within 15s-1m\n> of launch). Eventually, the Flink cluster dies.\n>\n> The sequence of events looks something like this:\n>\n>    - bad job is launched\n>    - bad job fails & is restarted many times (I didn't have the\n>    \"failure-rate\" restart strategy configuration right)\n>    - Task manager logs: org.apache.flink.yarn.YarnTaskManagerRunner\n>    (SIGTERM handler): RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.\n>    - At this point, the YARN resource manager also logs the container\n>    failure\n>    - More logs: Container ResourceID{resourceId='\n>    container_1481658997383_0003_01_000013'} failed. Exit status: Pmem\n>    limit exceeded (-104)\n>    - Diagnostics for container ResourceID{resourceId='\n>    container_1481658997383_0003_01_000013'} in state COMPLETE :\n>    exitStatus=Pmem limit exceeded (-104) diagnostics=Container\n>    [pid=21246,containerID=container_1481658997383_0003_01_000013] is\n>    running beyond physical memory limits. Current usage: 5.6 GB of 5.6 GB\n>    physical memory used; 9.6 GB of 28.1 GB virtual memory used. Killing\n>    container.\n>    Container killed on request. Exit code is 143\n>    Container exited with a non-zero exit code 143\n>    Total number of failed containers so far: 12\n>    Stopping YARN session because the number of failed containers (12)\n>    exceeded the maximum failed containers (11). This number is controlled by\n>    the 'yarn.maximum-failed-containers' configuration setting. By default\n>    its the number of requested containers.\n>    - From here onward, the logs repeatedly show that jobs fail to restart\n>    due to \"org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:\n>    Not enough free slots available to run the job. You can decrease the\n>    operator parallelism or increase the number of slots per TaskManager in the\n>    configuration. Task to schedule: < Attempt #68 (Source: \u00e2\u20ac\u00a6) @ (unassigned) -\n>    [SCHEDULED] > with groupID < 73191c171abfff61fb5102c161274145 > in\n>    sharing group < SlotSharingGroup [73191c171abfff61fb5102c161274145,\n>    19596f7834805c8409c419f0edab1f1b] >. Resources available to scheduler:\n>    Number of instances=0, total number of slots=0, available slots=0\"\n>    - Eventually, Flink stops for some reason (with another SIGTERM\n>    message), presumably because of YARN\n>\n> Does anyone have an idea why a bad job repeatedly failing would eventually\n> result in the Flink cluster dying?\n>\n> Any idea why I'd get \"Pmem limit exceeded\" or \"Not enough free slots\n> available to run the job\"? The JVM heap usage and the free memory on the\n> machines both look reasonable in my monitoring dashboards. Could it\n> possibly be a memory leak due to classloading or something?\n>\n> Thanks for any help or suggestions you can provide! I am hoping that the\n> \"failure-rate\" restart strategy will help avoid this issue in the future,\n> but I'd also like to understand what's making the cluster die so that I can\n> prevent it.\n>\n> -Shannon\n>\n\n",
      "type": "Body",
      "meta": null
    },
    {
      "id": 6,
      "start": 0,
      "end": 2525,
      "text": "Hi,\n\nI've faced a similar issue recently. Hope sharing my findings will help.\nThe problem can be split into 2 parts:\n\n*Source of container failures*\nThe logs you provided indicate that YARN kills its containers for exceeding\nmemory limits. Important point here is that memory limit = JVM heap memory\n+ off-heap memory. So if off-heap memory usage is high, YARN may kill\ncontainers despite JVM heap consumption is fine. To solve this issue, Flink\nreserves a share of container memory for off-heap memory. How much will be\nreserved is controlled by yarn.heap-cutoff-ratio and\nyarn.heap-cutoff-min configuration.\nBy default 25% of the requested container memory will be reserved for\noff-heap. This is seems to be a good start, but one should experiment and\ntune to meet their job specifics.\n\nIt's also worthwhile to figure out who consumes off-heap memory. Is it\nFlink managed memory moved off heap (taskmanager.memory.off-heap = true)?\nIs it some external library allocating something off heap? Is it your own\ncode?\n\n*How Flink handles task manager failures*\nWhenever a task manager fails, the Flink jobmanager decides whether it\nshould:\n- reallocate failed task manager container\n- fail application entirely\nThese decisions can be guided by certain configuration (\nhttps://ci.apache.org/projects/flink/flink-docs-\nrelease-1.1/setup/yarn_setup.html#recovery-behavior-of-flink-on-yarn). With\ndefault settings, job manager does reallocate task manager containers up to\nthe point when N failures have been observed, where N is the number of\nrequested task managers. After that the application is stopped.\n\nAccording to the logs, you have a finite number in\nyarn.maximum-failed-containers (11, as I can see from the logs - this may\nbe set by Flink if not provided explicitly). On 12th container restart,\njobmanager gives up and the application stops. I'm not sure why it keeps\nreporting not enough slots after that point. In my experience this may\nhappen when job eats up all the available slots, so that after container\nfailure its tasks cannot be restarted in other (live) containers. But I\nbelieve once the decision to stop the application is made, there should not\nbe any further attempts to restart the job, hence no logs like those.\nHopefully, someone else will explain this to us :)\n\nIn my case I made jobmanager restart containers infinitely by setting\nyarn.maximum-failed-containers\n= -1, so that taskmanager failure never results in application death. Note\nthis is unlikely a good choice for a batch job.\n\nRegards,\nYury\n\n",
      "type": "Body",
      "meta": null
    }
  ],
  "meta": {},
  "id": "eval/train_74"
}