{
  "wrapper": "plaintext",
  "text": "I'll take a look into the ProcessFunction, thanks for the suggestion.\n\n-Jia\n\nOn Wed, May 17, 2017 at 12:33 AM, Tzu-Li (Gordon) Tai <tzulitai@apache.org>\nwrote:\n\n> Hi Jia,\n>\n> Actually just realized you can access the timestamp of records via the\n> more powerful `ProcessFunction` [1].\n> That\u00e2\u20ac\u2122ll be a bit easier to use than implementing your own custom operator,\n> which is quite low-level.\n>\n> Cheers,\n> Gordon\n>\n> [1] https://ci.apache.org/projects/flink/flink-docs-\n> release-1.2/dev/stream/process_function.html\n>\n> On 17 May 2017 at 1:45:38 PM, Jia Teoh (jiateoh@gmail.com) wrote:\n>\n> Hi Gordon,\n>\n> The timestamps are required for application logic. Thank you for\n> clarifying the custom operators - seems I mistakenly thought of the\n> functions that are passed to the operators rather than the operators\n> themselves. AbstractStreamOperator and the other classes you mentioned seem\n> like exactly what I'm looking for, so I will take a look into implementing\n> a custom one for my use case.\n>\n> Thank you,\n> Jia\n>\n> On Tue, May 16, 2017 at 9:52 PM, Tzu-Li (Gordon) Tai <tzulitai@apache.org>\n> wrote:\n>\n>> Hi Jia,\n>>\n>> How exactly do you want to use the Kafka timestamps? Do you want to\n>> access them and alter them with new values as the record timestamp? Or do\n>> you want to use them for some application logic in your functions?\n>>\n>> If its the former, you should be able to do that by using timestamp /\n>> watermark extractors. They come with an interface that exposes the current\n>> timestamp of the record. For Kafka 0.10, that timestamp would be the Kafka\n>> record\u00e2\u20ac\u2122s timestamp if it hasn\u00e2\u20ac\u2122t been explicitly assigned any other\n>> timestamp yet.\n>>\n>> If its the latter, then I think currently you have to use custom\n>> operators as Robert mentioned.\n>> Custom operators are classes that extend the `AbstractStreamOperator`\n>> base class as one of `OneInputStreamOperator` or `TwoInputStreamOperator`\n>> interfaces.\n>> You can take a look at the basic `StreamMap` or `StreamFlatMap` classes\n>> for an example, which are the underlying operators for the map and flatMap\n>> functions.\n>>\n>> At the operator level, you\u00e2\u20ac\u2122ll have access to a `StreamRecord` in the\n>> `processElement` function which wraps the record value (which you get when\n>> implementing functions) as well as the internal timestamp that comes with\n>> the record.\n>>\n>> Cheers,\n>> Gordon\n>>\n>>\n>> On 17 May 2017 at 4:27:36 AM, Jia Teoh (jiateoh@gmail.com) wrote:\n>>\n>> Hi Robert,\n>>\n>> Thanks for the reply. I ended up implementing an extension of the Kafka\n>> fetcher and consumer so that the deserialization API can include the\n>> timestamp field, which is sufficient for my specific use case. I can share\n>> the code if desired but it seems like it's an intentional design decision\n>> not to expose the timestamp in the deserialization API.\n>>\n>> I noticed you mentioned that I could use a custom operator to access the\n>> record event time. Could you elaborate on what you mean by \"operator\"? I\n>> initially thought that referred to DataStream.map/reduce/etc, but none of\n>> those functions provide arguments that can be used to extract the embedded\n>> timestamp.\n>>\n>> Thanks,\n>> Jia Teoh\n>>\n>> On Fri, May 12, 2017 at 9:25 AM, Robert Metzger <rmetzger@apache.org>\n>> wrote:\n>>\n>>> Hi Jia,\n>>>\n>>> The Kafka 0.10 connector internally relies on the Kafka09 fetcher, but\n>>> it is extensible / pluggable so that also the Kafka 0.9 Fetcher can read\n>>> the event timestamps from Kafka 10.\n>>> We don't expose the timestamp through the deserilaization API, because\n>>> we set it internally in Flink. (there is a \"hidden\" field with each record\n>>> containing the event time of the event)\n>>>\n>>> With a custom operator you can access the event time of a record.\n>>>\n>>> On Fri, May 12, 2017 at 3:26 AM, Jia Teoh <jiateoh@gmail.com> wrote:\n>>>\n>>>> Hi,\n>>>>\n>>>> Is there a way to retrieve the timestamps that Kafka associates with\n>>>> each key-value pair within Flink? I would like to be able to use these as\n>>>> values within my application flow, and defining them before or after Kafka\n>>>> is not acceptable for the use case due to the latency involved in sending\n>>>> or receiving from Kafka.\n>>>>\n>>>> It seems that Flink supports Kafka event time (link\n>>>> <https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/connectors/kafka.html#using-kafka-timestamps-and-flink-event-time-in-kafka-010>)\n>>>> but after a brief trace it seems that KafkaConsumer010 still relies on the\n>>>> Kafka09Fetcher\n>>>> <https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java#L137>\nfor\n>>>> iterating through each Kafka record and deserializing it. The\n>>>> KeyedDeserializationSchema api does not seem to have support for including\n>>>> timestamp as additional metadata (just offset, topic, and partition) so\n>>>> something such as JSONKeyValueDeserializationSchema will not return\n>>>> the Kafka-specified timestamp.\n>>>>\n>>>> For reference, I am using Kafka 0.10.2 and the Flink-Streaming API +\n>>>> Kafka Connector (1.2.1).\n>>>>\n>>>> Thanks,\n>>>> Jia Teoh\n>>>>\n>>>\n>>>\n>>\n>\n\n",
  "denotations": [
    {
      "id": 1,
      "start": 70,
      "end": 76,
      "text": "\n-Jia\n",
      "type": "Body/Outro",
      "meta": null
    },
    {
      "id": 2,
      "start": 0,
      "end": 77,
      "text": "I'll take a look into the ProcessFunction, thanks for the suggestion.\n\n-Jia\n\n",
      "type": "Body",
      "meta": null
    },
    {
      "id": 3,
      "start": 77,
      "end": 160,
      "text": "On Wed, May 17, 2017 at 12:33 AM, Tzu-Li (Gordon) Tai <tzulitai@apache.org>\nwrote:\n",
      "type": "Header",
      "meta": null
    },
    {
      "id": 4,
      "start": 160,
      "end": 171,
      "text": "\n> Hi Jia,\n",
      "type": "Body/Intro",
      "meta": null
    },
    {
      "id": 5,
      "start": 392,
      "end": 413,
      "text": ">\n> Cheers,\n> Gordon\n",
      "type": "Body/Outro",
      "meta": null
    },
    {
      "id": 6,
      "start": 160,
      "end": 519,
      "text": "\n> Hi Jia,\n>\n> Actually just realized you can access the timestamp of records via the\n> more powerful `ProcessFunction` [1].\n> That\u00e2\u20ac\u2122ll be a bit easier to use than implementing your own custom operator,\n> which is quite low-level.\n>\n> Cheers,\n> Gordon\n>\n> [1] https://ci.apache.org/projects/flink/flink-docs-\n> release-1.2/dev/stream/process_function.html\n>\n",
      "type": "Body",
      "meta": null
    },
    {
      "id": 7,
      "start": 519,
      "end": 587,
      "text": "> On 17 May 2017 at 1:45:38 PM, Jia Teoh (jiateoh@gmail.com) wrote:\n",
      "type": "Header",
      "meta": null
    },
    {
      "id": 8,
      "start": 587,
      "end": 602,
      "text": ">\n> Hi Gordon,\n",
      "type": "Body/Intro",
      "meta": null
    },
    {
      "id": 9,
      "start": 999,
      "end": 1020,
      "text": ">\n> Thank you,\n> Jia\n",
      "type": "Body/Outro",
      "meta": null
    },
    {
      "id": 10,
      "start": 1022,
      "end": 1108,
      "text": "> On Tue, May 16, 2017 at 9:52 PM, Tzu-Li (Gordon) Tai <tzulitai@apache.org>\n> wrote:\n",
      "type": "Header",
      "meta": null
    },
    {
      "id": 11,
      "start": 587,
      "end": 1022,
      "text": ">\n> Hi Gordon,\n>\n> The timestamps are required for application logic. Thank you for\n> clarifying the custom operators - seems I mistakenly thought of the\n> functions that are passed to the operators rather than the operators\n> themselves. AbstractStreamOperator and the other classes you mentioned seem\n> like exactly what I'm looking for, so I will take a look into implementing\n> a custom one for my use case.\n>\n> Thank you,\n> Jia\n>\n",
      "type": "Body",
      "meta": null
    },
    {
      "id": 12,
      "start": 1108,
      "end": 1121,
      "text": ">\n>> Hi Jia,\n",
      "type": "Body/Intro",
      "meta": null
    },
    {
      "id": 13,
      "start": 2350,
      "end": 2374,
      "text": ">>\n>> Cheers,\n>> Gordon\n",
      "type": "Body/Outro",
      "meta": null
    },
    {
      "id": 14,
      "start": 1108,
      "end": 2380,
      "text": ">\n>> Hi Jia,\n>>\n>> How exactly do you want to use the Kafka timestamps? Do you want to\n>> access them and alter them with new values as the record timestamp? Or do\n>> you want to use them for some application logic in your functions?\n>>\n>> If its the former, you should be able to do that by using timestamp /\n>> watermark extractors. They come with an interface that exposes the current\n>> timestamp of the record. For Kafka 0.10, that timestamp would be the Kafka\n>> record\u00e2\u20ac\u2122s timestamp if it hasn\u00e2\u20ac\u2122t been explicitly assigned any other\n>> timestamp yet.\n>>\n>> If its the latter, then I think currently you have to use custom\n>> operators as Robert mentioned.\n>> Custom operators are classes that extend the `AbstractStreamOperator`\n>> base class as one of `OneInputStreamOperator` or `TwoInputStreamOperator`\n>> interfaces.\n>> You can take a look at the basic `StreamMap` or `StreamFlatMap` classes\n>> for an example, which are the underlying operators for the map and flatMap\n>> functions.\n>>\n>> At the operator level, you\u00e2\u20ac\u2122ll have access to a `StreamRecord` in the\n>> `processElement` function which wraps the record value (which you get when\n>> implementing functions) as well as the internal timestamp that comes with\n>> the record.\n>>\n>> Cheers,\n>> Gordon\n>>\n>>\n",
      "type": "Body",
      "meta": null
    },
    {
      "id": 15,
      "start": 2380,
      "end": 2449,
      "text": ">> On 17 May 2017 at 4:27:36 AM, Jia Teoh (jiateoh@gmail.com) wrote:\n",
      "type": "Header",
      "meta": null
    },
    {
      "id": 16,
      "start": 2449,
      "end": 2466,
      "text": ">>\n>> Hi Robert,\n",
      "type": "Body/Intro",
      "meta": null
    },
    {
      "id": 17,
      "start": 3154,
      "end": 3180,
      "text": ">>\n>> Thanks,\n>> Jia Teoh\n",
      "type": "Body/Outro",
      "meta": null
    },
    {
      "id": 18,
      "start": 2449,
      "end": 3183,
      "text": ">>\n>> Hi Robert,\n>>\n>> Thanks for the reply. I ended up implementing an extension of the Kafka\n>> fetcher and consumer so that the deserialization API can include the\n>> timestamp field, which is sufficient for my specific use case. I can share\n>> the code if desired but it seems like it's an intentional design decision\n>> not to expose the timestamp in the deserialization API.\n>>\n>> I noticed you mentioned that I could use a custom operator to access the\n>> record event time. Could you elaborate on what you mean by \"operator\"? I\n>> initially thought that referred to DataStream.map/reduce/etc, but none of\n>> those functions provide arguments that can be used to extract the embedded\n>> timestamp.\n>>\n>> Thanks,\n>> Jia Teoh\n>>\n",
      "type": "Body",
      "meta": null
    },
    {
      "id": 19,
      "start": 3183,
      "end": 3266,
      "text": ">> On Fri, May 12, 2017 at 9:25 AM, Robert Metzger <rmetzger@apache.org>\n>> wrote:\n",
      "type": "Header",
      "meta": null
    },
    {
      "id": 20,
      "start": 3266,
      "end": 3281,
      "text": ">>\n>>> Hi Jia,\n",
      "type": "Body/Intro",
      "meta": null
    },
    {
      "id": 21,
      "start": 3266,
      "end": 3753,
      "text": ">>\n>>> Hi Jia,\n>>>\n>>> The Kafka 0.10 connector internally relies on the Kafka09 fetcher, but\n>>> it is extensible / pluggable so that also the Kafka 0.9 Fetcher can read\n>>> the event timestamps from Kafka 10.\n>>> We don't expose the timestamp through the deserilaization API, because\n>>> we set it internally in Flink. (there is a \"hidden\" field with each record\n>>> containing the event time of the event)\n>>>\n>>> With a custom operator you can access the event time of a record.\n>>>\n",
      "type": "Body",
      "meta": null
    },
    {
      "id": 22,
      "start": 3753,
      "end": 3826,
      "text": ">>> On Fri, May 12, 2017 at 3:26 AM, Jia Teoh <jiateoh@gmail.com> wrote:\n",
      "type": "Header",
      "meta": null
    },
    {
      "id": 23,
      "start": 3826,
      "end": 3839,
      "text": ">>>\n>>>> Hi,\n",
      "type": "Body/Intro",
      "meta": null
    },
    {
      "id": 24,
      "start": 5131,
      "end": 5163,
      "text": ">>>>\n>>>> Thanks,\n>>>> Jia Teoh\n",
      "type": "Body/Outro",
      "meta": null
    },
    {
      "id": 25,
      "start": 3826,
      "end": 5182,
      "text": ">>>\n>>>> Hi,\n>>>>\n>>>> Is there a way to retrieve the timestamps that Kafka associates with\n>>>> each key-value pair within Flink? I would like to be able to use these as\n>>>> values within my application flow, and defining them before or after Kafka\n>>>> is not acceptable for the use case due to the latency involved in sending\n>>>> or receiving from Kafka.\n>>>>\n>>>> It seems that Flink supports Kafka event time (link\n>>>> <https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/connectors/kafka.html#using-kafka-timestamps-and-flink-event-time-in-kafka-010>)\n>>>> but after a brief trace it seems that KafkaConsumer010 still relies on the\n>>>> Kafka09Fetcher\n>>>> <https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java#L137>\nfor\n>>>> iterating through each Kafka record and deserializing it. The\n>>>> KeyedDeserializationSchema api does not seem to have support for including\n>>>> timestamp as additional metadata (just offset, topic, and partition) so\n>>>> something such as JSONKeyValueDeserializationSchema will not return\n>>>> the Kafka-specified timestamp.\n>>>>\n>>>> For reference, I am using Kafka 0.10.2 and the Flink-Streaming API +\n>>>> Kafka Connector (1.2.1).\n>>>>\n>>>> Thanks,\n>>>> Jia Teoh\n>>>>\n>>>\n>>>\n>>\n>\n\n",
      "type": "Body",
      "meta": null
    }
  ],
  "meta": {},
  "id": "eval/train_2290"
}