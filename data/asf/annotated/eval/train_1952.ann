{
  "wrapper": "plaintext",
  "text": "Hi Ufuk,\n\nThe ProcessFunction receives elements and buffers them into a MapState, and\nperiodically (for example every x seconds) register processing time timers\n(according to some rules which it gets from a connected rule stream). When\na timer fires, I pop next element from state, request an external server,\nand collect the response.\nThe requests to the external server should happen periodically and not\ncontinuousely, that's why I control them using timers, and buffer elements\nin the RocksdbState.\n\n2017-04-24 13:48 GMT+02:00 Ufuk Celebi <uce@apache.org>:\n\n> @Yessine: no, there is no way to disable the back pressure mechanism. Do\n> you have more details about the two last operators? What do you mean with\n> the process function is slow on purpose?\n>\n> @Rune: with 1.3 Flink will configure the internal buffers in a way that\n> not too much data is buffered in the internal buffers (\n> https://issues.apache.org/jira/browse/FLINK-4545). You could try the\n> current master and check whether it improves the checkpointing behaviour\n> under back pressure. Out of curiosity, are you using the async I/O API for\n> the communication with the external REST service (https://ci.apache.org/\n> projects/flink/flink-docs-release-1.2/dev/stream/asyncio.html)?\n>\n> \u00e2\u20ac\u201c Ufuk\n>\n>\n> On Mon, Apr 24, 2017 at 11:08 AM, Rune Skou Larsen <rsl@trifork.com>\n> wrote:\n>\n>> Sorry I cant help you, but we're also experiencing slow checkpointing,\n>> when having backpressure from sink.\n>>\n>> I tried HDFS, S3, and RocksDB state backends, but to no avail -\n>> checkpointing always times out with backpressure.\n>>\n>> Can we somehow reduce Flink's internal buffer sizes, so checkpointing\n>> with backpressure becomes faster?\n>>\n>> - Rune\n>>\n>> ---\n>>\n>> Our current setup - (improvement suggestions welome!):\n>>\n>> Flink 1.2.0,  yarn@AWS EMR, 1 master + 3 slaves, m4.xlarge\n>>\n>> program_parallelism: 12taskmanagers: 6slotsPerTaskManager: 4taskmanager_heap_mb:\n4096jobmanager_heap_mb: 1024\n>>\n>> Basic program structure:\n>>\n>> 1) read batch from Kinesis\n>>\n>> 2) Split batch and shuffle using custom partitioner (consistent hashing).\n>>\n>> 3) enrich using external REST service\n>>\n>> 4) Write to database (This step is the bottleneck)\n>> On 24-04-2017 09:32, Yassine MARZOUGUI wrote:\n>>\n>> Im sorry guys if you received multiple instances of this mail, I kept\n>> trying to send it yesterday, but looks like the mailing list was stuck and\n>> didn't dispatch it until now. Sorry for the disturb.\n>> On Apr 23, 2017 20:53, \"Yassine MARZOUGUI\" <y.marzougui@mindlytix.com>\n>> wrote:\n>>>\n>>> Hi all,\n>>> I have a Streaming pipeline as follows:\n>>> 1 - read a folder continuousely from HDFS\n>>> 2 - filter duplicates (using keyby(x->x) and keeping a state per key\n>>> indicating whether its is seen)\n>>> 3 - schedule some future actions on the stream using ProcessFunction and\n>>> processing time timers (elements are kept in a MapState)\n>>> 4- write results back to HDFS using a BucketingSink.\n>>> I am using RocksdbStateBackend, and Flink 1.3-SNAPSHOT (Commit: 9fb074c).\n>>> Currenlty the source contain just one a file of 1GB, so that's the\n>>> maximum state that the job might hold. I noticed that the backpressure on\n>>> the operators #1 and #2 is High, and the split reader has only read 60 Mb\n>>> out of 1Gb source source file. I suspect this is because the\n>>> ProcessFunction is slow (on purpose). However looks like this affected the\n>>> checkpoints which are failing after the timeout (which is set to 2 hours),\n>>> see attached screenshot.\n>>> \u00e2\u20ac\u2039\n>>> In the job manager logs I keep getting warnings :\n>>>\n>>> 2017-04-23 19:32:38,827 WARN  org.apache.flink.runtime.checkpoint.CheckpointCoordinator\n    - Received late message for now expired checkpoint attempt 8 from 210769a077c67841d980776d8caece0a\nof job 6c7e44d205d738fc8a6cb4da181d2d86.\n>>>\n>>> Is the high backpressure the cause for the checkpoints being too slow?\n>>> If yes Is there a way to disbale the backpressure mechanism since the\n>>> records will be buffered in the rocksdb state after all which is backed by\n>>> the disk?\n>>> Thank you.\n>>> Best,\n>>> Yassine\n>>>\n>> --\n>>\n>> Venlig hilsen/Best regards *Rune Skou Larsen*\n>>\n>> [image: goto] Trifork Public A/S Dyssen 1 \u00c2\u00b7 DK-8200 Aarhus N \u00c2\u00b7 Denmark\n>> Phone +45 3160 2497 <+45%2031%2060%2024%2097> Skype: rsltrifork Twitter:\n>> RuneSkouLarsen\n>>\n>\n>\n\n",
  "denotations": [
    {
      "id": 1,
      "start": 0,
      "end": 9,
      "text": "Hi Ufuk,\n",
      "type": "Body/Intro",
      "meta": null
    },
    {
      "id": 2,
      "start": 0,
      "end": 504,
      "text": "Hi Ufuk,\n\nThe ProcessFunction receives elements and buffers them into a MapState, and\nperiodically (for example every x seconds) register processing time timers\n(according to some rules which it gets from a connected rule stream). When\na timer fires, I pop next element from state, request an external server,\nand collect the response.\nThe requests to the external server should happen periodically and not\ncontinuousely, that's why I control them using timers, and buffer elements\nin the RocksdbState.\n\n",
      "type": "Body",
      "meta": null
    },
    {
      "id": 3,
      "start": 504,
      "end": 561,
      "text": "2017-04-24 13:48 GMT+02:00 Ufuk Celebi <uce@apache.org>:\n",
      "type": "Header",
      "meta": null
    },
    {
      "id": 4,
      "start": 1254,
      "end": 1267,
      "text": ">\n> \u00e2\u20ac\u201c Ufuk\n",
      "type": "Body/Outro",
      "meta": null
    },
    {
      "id": 5,
      "start": 561,
      "end": 1271,
      "text": "\n> @Yessine: no, there is no way to disable the back pressure mechanism. Do\n> you have more details about the two last operators? What do you mean with\n> the process function is slow on purpose?\n>\n> @Rune: with 1.3 Flink will configure the internal buffers in a way that\n> not too much data is buffered in the internal buffers (\n> https://issues.apache.org/jira/browse/FLINK-4545). You could try the\n> current master and check whether it improves the checkpointing behaviour\n> under back pressure. Out of curiosity, are you using the async I/O API for\n> the communication with the external REST service (https://ci.apache.org/\n> projects/flink/flink-docs-release-1.2/dev/stream/asyncio.html)?\n>\n> \u00e2\u20ac\u201c Ufuk\n>\n>\n",
      "type": "Body",
      "meta": null
    },
    {
      "id": 6,
      "start": 1271,
      "end": 1351,
      "text": "> On Mon, Apr 24, 2017 at 11:08 AM, Rune Skou Larsen <rsl@trifork.com>\n> wrote:\n",
      "type": "Header",
      "meta": null
    },
    {
      "id": 7,
      "start": 1351,
      "end": 2471,
      "text": ">\n>> Sorry I cant help you, but we're also experiencing slow checkpointing,\n>> when having backpressure from sink.\n>>\n>> I tried HDFS, S3, and RocksDB state backends, but to no avail -\n>> checkpointing always times out with backpressure.\n>>\n>> Can we somehow reduce Flink's internal buffer sizes, so checkpointing\n>> with backpressure becomes faster?\n>>\n>> - Rune\n>>\n>> ---\n>>\n>> Our current setup - (improvement suggestions welome!):\n>>\n>> Flink 1.2.0,  yarn@AWS EMR, 1 master + 3 slaves, m4.xlarge\n>>\n>> program_parallelism: 12taskmanagers: 6slotsPerTaskManager: 4taskmanager_heap_mb:\n4096jobmanager_heap_mb: 1024\n>>\n>> Basic program structure:\n>>\n>> 1) read batch from Kinesis\n>>\n>> 2) Split batch and shuffle using custom partitioner (consistent hashing).\n>>\n>> 3) enrich using external REST service\n>>\n>> 4) Write to database (This step is the bottleneck)\n>> On 24-04-2017 09:32, Yassine MARZOUGUI wrote:\n>>\n>> Im sorry guys if you received multiple instances of this mail, I kept\n>> trying to send it yesterday, but looks like the mailing list was stuck and\n>> didn't dispatch it until now. Sorry for the disturb.\n",
      "type": "Body",
      "meta": null
    },
    {
      "id": 8,
      "start": 2471,
      "end": 2555,
      "text": ">> On Apr 23, 2017 20:53, \"Yassine MARZOUGUI\" <y.marzougui@mindlytix.com>\n>> wrote:\n",
      "type": "Header",
      "meta": null
    },
    {
      "id": 9,
      "start": 2555,
      "end": 2571,
      "text": ">>>\n>>> Hi all,\n",
      "type": "Body/Intro",
      "meta": null
    },
    {
      "id": 10,
      "start": 4085,
      "end": 4107,
      "text": ">>> Best,\n>>> Yassine\n",
      "type": "Body/Outro",
      "meta": null
    },
    {
      "id": 11,
      "start": 4111,
      "end": 4169,
      "text": ">> --\n>>\n>> Venlig hilsen/Best regards *Rune Skou Larsen*\n",
      "type": "Body/Outro",
      "meta": null
    },
    {
      "id": 12,
      "start": 4172,
      "end": 4342,
      "text": ">> [image: goto] Trifork Public A/S Dyssen 1 \u00c2\u00b7 DK-8200 Aarhus N \u00c2\u00b7 Denmark\n>> Phone +45 3160 2497 <+45%2031%2060%2024%2097> Skype: rsltrifork Twitter:\n>> RuneSkouLarsen\n",
      "type": "Body/Signature",
      "meta": null
    },
    {
      "id": 13,
      "start": 2555,
      "end": 4350,
      "text": ">>>\n>>> Hi all,\n>>> I have a Streaming pipeline as follows:\n>>> 1 - read a folder continuousely from HDFS\n>>> 2 - filter duplicates (using keyby(x->x) and keeping a state per key\n>>> indicating whether its is seen)\n>>> 3 - schedule some future actions on the stream using ProcessFunction and\n>>> processing time timers (elements are kept in a MapState)\n>>> 4- write results back to HDFS using a BucketingSink.\n>>> I am using RocksdbStateBackend, and Flink 1.3-SNAPSHOT (Commit: 9fb074c).\n>>> Currenlty the source contain just one a file of 1GB, so that's the\n>>> maximum state that the job might hold. I noticed that the backpressure on\n>>> the operators #1 and #2 is High, and the split reader has only read 60 Mb\n>>> out of 1Gb source source file. I suspect this is because the\n>>> ProcessFunction is slow (on purpose). However looks like this affected the\n>>> checkpoints which are failing after the timeout (which is set to 2 hours),\n>>> see attached screenshot.\n>>> \u00e2\u20ac\u2039\n>>> In the job manager logs I keep getting warnings :\n>>>\n>>> 2017-04-23 19:32:38,827 WARN  org.apache.flink.runtime.checkpoint.CheckpointCoordinator\n    - Received late message for now expired checkpoint attempt 8 from 210769a077c67841d980776d8caece0a\nof job 6c7e44d205d738fc8a6cb4da181d2d86.\n>>>\n>>> Is the high backpressure the cause for the checkpoints being too slow?\n>>> If yes Is there a way to disbale the backpressure mechanism since the\n>>> records will be buffered in the rocksdb state after all which is backed by\n>>> the disk?\n>>> Thank you.\n>>> Best,\n>>> Yassine\n>>>\n>> --\n>>\n>> Venlig hilsen/Best regards *Rune Skou Larsen*\n>>\n>> [image: goto] Trifork Public A/S Dyssen 1 \u00c2\u00b7 DK-8200 Aarhus N \u00c2\u00b7 Denmark\n>> Phone +45 3160 2497 <+45%2031%2060%2024%2097> Skype: rsltrifork Twitter:\n>> RuneSkouLarsen\n>>\n>\n>\n\n",
      "type": "Body",
      "meta": null
    }
  ],
  "meta": {},
  "id": "eval/train_1952"
}