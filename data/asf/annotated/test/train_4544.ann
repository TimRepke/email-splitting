{
  "wrapper": "plaintext",
  "text": "Sengupta\n further to this, if you try the following notebook in databricks cloud, it\nwill read a .csv file , write to a parquet file and read it again (just to\ncount the number of rows stored)\nPlease note that the path to the csv file might differ for you.....\nSo, what you will need todo is\n1 - create an account to community.cloud.databricks.com\n2 - upload the .csv file onto the Data of your databricks private cluster\n3  - run the script. that will store the data on the distrubuted filesystem\nof the databricks cloudn (dbfs)\n\nIt's worth investing in this free databricks cloud as it can create a\ncluster for you with minimal effort, and it's  a very easy way to test your\nspark scripts on a real cluster\n\nhope this helps\nkr\n\n##################################\nfrom pyspark.sql import SQLContext\n\nfrom random import randint\nfrom time import sleep\nfrom pyspark.sql.session import SparkSession\nimport logging\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nch = logging.StreamHandler()\nlogger.addHandler(ch)\n\n\nimport sys\n\ndef read_parquet_file(parquetFileName):\n  logger.info('Reading now the parquet files we just created...:%s',\nparquetFileName)\n  parquet_data = sqlContext.read.parquet(parquetFileName)\n  logger.info('Parquet file has %s', parquet_data.count())\n\ndef dataprocessing(filePath, count, sqlContext):\n    logger.info( 'Iter count is:%s' , count)\n    if count == 0:\n        print 'exiting'\n    else:\n        df_traffic_tmp =\nsqlContext.read.format(\"csv\").option(\"header\",'true').load(filePath)\n        logger.info( '#############################DataSet has:%s' ,\ndf_traffic_tmp.count())\n        logger.info('WRting to a parquet file')\n        parquetFileName = \"dbfs:/myParquetDf2.parquet\"\n        df_traffic_tmp.write.parquet(parquetFileName)\n        sleepInterval = randint(10,100)\n        logger.info( '#############################Sleeping for %s' ,\nsleepInterval)\n        sleep(sleepInterval)\n        read_parquet_file(parquetFileName)\n        dataprocessing(filePath, count-1, sqlContext)\n\nfilename =\n'/FileStore/tables/wb4y1wrv1502027870004/tree_addhealth.csv'#This path\nmight differ for you\niterations = 1\nlogger.info('----------------------')\nlogger.info('Filename:%s', filename)\nlogger.info('Iterations:%s', iterations )\nlogger.info('----------------------')\n\nlogger.info ('Initializing sqlContext')\nlogger.info( '........Starting spark..........Loading from%s for %s\niterations' , filename, iterations)\nlogger.info(  'Starting up....')\nsc = SparkSession.builder.appName(\"Data Processsing\").getOrCreate()\nlogger.info ('Initializing sqlContext')\nsqlContext = SQLContext(sc)\ndataprocessing(filename, iterations, sqlContext)\nlogger.info('Out of here..')\n######################################\n\n\nOn Sat, Aug 5, 2017 at 9:09 PM, Marco Mistroni <mmistroni@gmail.com> wrote:\n\n> Uh believe me there are lots of ppl on this list who will send u code\n> snippets if u ask... \u00f0\u0178\u02dc\u20ac\n>\n> Yes that is what Steve pointed out, suggesting also that for that simple\n> exercise you should perform all operations on a spark standalone instead\n> (or alt. Use an nfs on the cluster)\n> I'd agree with his suggestion....\n> I suggest u another alternative:\n> https://community.cloud.databricks.com/\n>\n> That's a ready made cluster and you can run your spark app as well store\n> data on the cluster (well I haven't tried myself but I assume it's\n> possible).   Try that out... I will try ur script there as I have an\n> account there (though I guess I'll get there before me.....)\n>\n> Try that out and let me know if u get stuck....\n> Kr\n>\n> On Aug 5, 2017 8:40 PM, \"Gourav Sengupta\" <gourav.sengupta@gmail.com>\n> wrote:\n>\n>> Hi Marco,\n>>\n>> For the first time in several years FOR THE VERY FIRST TIME. I am seeing\n>> someone actually executing code and providing response. It feel wonderful\n>> that at least someone considered to respond back by executing code and just\n>> did not filter out each and every technical details to brood only on my\n>> superb social skills, while claiming the reason for ignoring technical\n>> details is that it elementary. I think that Steve also is the first person\n>> who could answer the WHY of an elementary question instead of saying that\n>> is how it is and pointed out to the correct documentation.\n>>\n>> That code works fantastically. But the problem which I have tried to find\n>> out is while writing out the data and not reading it.\n>>\n>>\n>> So if you see try to read the data from the same folder which has the\n>> same file across all the nodes then it will work fine. In fact that is what\n>> should work.\n>>\n>> What does not work is that if you try to write back the file and then\n>> read it once again from the location you have written that is when the\n>> issue starts happening.\n>>\n>> Therefore if in my code you were to save the pandas dataframe as a CSV\n>> file and then read it then you will find the following observations:\n>>\n>> FOLLOWING WILL FAIL SINCE THE FILE IS NOT IN ALL THE NODES\n>> ------------------------------------------------------------\n>> ------------------------------------------------------------\n>> ------------------------------------------------------------\n>> ---------------------------\n>> pandasdf = pandas.DataFrame(numpy.random.randn(10000, 4),\n>> columns=list('ABCD'))\n>> pandasdf.to_csv(\"/Users/gouravsengupta/Development/spark/sparkdata/testdir/test.csv\",\n>> header=True, sep=\",\", index=0)\n>> testdf = spark.read.load(\"/Users/gouravsengupta/Development/spark/spa\n>> rkdata/testdir/\")\n>> testdf.cache()\n>> testdf.count()\n>> ------------------------------------------------------------\n>> ------------------------------------------------------------\n>> ------------------------------------------------------------\n>> ---------------------------\n>>\n>>\n>> FOLLOWING WILL WORK BUT THE PROCESS WILL NOT AT ALL USE THE NODE IN WHICH\n>> THE DATA DOES NOT EXISTS\n>> ------------------------------------------------------------\n>> ------------------------------------------------------------\n>> ------------------------------------------------------------\n>> ---------------------------\n>> pandasdf = pandas.DataFrame(numpy.random.randn(10000, 4),\n>> columns=list('ABCD'))\n>> pandasdf.to_csv(\"/Users/gouravsengupta/Development/spark/sparkdata/testdir/test.csv\",\n>> header=True, sep=\",\", index=0)\n>> testdf = spark.read.load(\"file:///Users/gouravsengupta/Development/\n>> spark/sparkdata/testdir/\")\n>> testdf.cache()\n>> testdf.count()\n>> ------------------------------------------------------------\n>> ------------------------------------------------------------\n>> ------------------------------------------------------------\n>> ---------------------------\n>>\n>>\n>> if you execute my code then also you will surprisingly see that the\n>> writes in the nodes which is not the master node does not complete moving\n>> the files from the _temporary folder to the main one.\n>>\n>>\n>> Regards,\n>> Gourav Sengupta\n>>\n>>\n>>\n>> On Fri, Aug 4, 2017 at 9:45 PM, Marco Mistroni <mmistroni@gmail.com>\n>> wrote:\n>>\n>>> Hello\n>>>  please have a look at this. it'sa simple script that just read a\n>>> dataframe for n time, sleeping at random interval. i used it to test memory\n>>> issues that another user was experiencing on a spark cluster\n>>>\n>>> you should run it like this e.g\n>>> spark-submit dataprocessing_Sample.-2py <path to tree_addhealth.csv>\n>>> <num of iterations>\n>>>\n>>> i ran it on the cluster like this\n>>>\n>>> ./spark-submit --master spark://ec2-54-218-113-119.us-\n>>> west-2.compute.amazonaws.com:7077   /root/pyscripts/dataprocessing_Sample-2.py\n>>> file:///root/pyscripts/tree_addhealth.csv\n>>>\n>>> hth, ping me back if you have issues\n>>> i do agree with Steve's comments.... if you want to test your  spark\n>>> script s just for playing, do it on  a standaone server on your localhost.\n>>> Moving to a c luster is just a matter of deploying your script and mke sure\n>>> you have a common place where to read and store the data..... SysAdmin\n>>> should give you this when they setup the cluster...\n>>>\n>>> kr\n>>>\n>>>\n>>>\n>>>\n>>> On Fri, Aug 4, 2017 at 4:50 PM, Gourav Sengupta <\n>>> gourav.sengupta@gmail.com> wrote:\n>>>\n>>>> Hi Marco,\n>>>>\n>>>> I am sincerely obliged for your kind time and response. Can you please\n>>>> try the solution that you have so kindly suggested?\n>>>>\n>>>> It will be a lot of help if you could kindly execute the code that I\n>>>> have given. I dont think that anyone has yet.\n>>>>\n>>>> There are lots of fine responses to my question here, but if you read\n>>>> the last response from Simon, it comes the closest to being satisfactory.\nI\n>>>> am sure even he did not execute the code, but at least he came quite close\n>>>> to understanding what the problem is.\n>>>>\n>>>>\n>>>> Regards,\n>>>> Gourav Sengupta\n>>>>\n>>>>\n>>>> On Thu, Aug 3, 2017 at 7:59 PM, Marco Mistroni <mmistroni@gmail.com>\n>>>> wrote:\n>>>>\n>>>>> Hello\n>>>>>  my 2 cents here, hope it helps\n>>>>> If you want to just to play around with Spark, i'd leave Hadoop out,\n>>>>> it's an unnecessary dependency that you dont need for just running a\npython\n>>>>> script\n>>>>> Instead do the following:\n>>>>> - got to the root of our master / slave node. create a directory\n>>>>> /root/pyscripts\n>>>>> - place your csv file there as well as the python script\n>>>>> - run the script to replicate the whole directory  across the cluster\n>>>>> (i believe it's called copy-script.sh)\n>>>>> - then run your spark-submit , it will be something lke\n>>>>>     ./spark-submit /root/pyscripts/mysparkscripts.py\n>>>>> file:///root/pyscripts/tree_addhealth.csv 10 --master\n>>>>> spark://ip-172-31-44-155.us-west-2.compute.internal:7077\n>>>>> - in your python script, as part of your processing, write the parquet\n>>>>> file in directory /root/pyscripts\n>>>>>\n>>>>> If you have an AWS account and you are versatile with that - you need\n>>>>> to setup bucket permissions etc - , you can just\n>>>>> - store your file in one of your S3 bucket\n>>>>> - create an EMR cluster\n>>>>> - connect to master or slave\n>>>>> - run your  scritp that reads from the s3 bucket and write to the same\n>>>>> s3 bucket\n>>>>>\n>>>>>\n>>>>> Feel free to mail me privately, i have a working script i have used to\n>>>>> test some code on spark standalone cluster\n>>>>> hth\n>>>>>\n>>>>>\n>>>>>\n>>>>>\n>>>>>\n>>>>>\n>>>>>\n>>>>>\n>>>>>\n>>>>>\n>>>>> On Thu, Aug 3, 2017 at 10:30 AM, Gourav Sengupta <\n>>>>> gourav.sengupta@gmail.com> wrote:\n>>>>>\n>>>>>> Hi Steve,\n>>>>>>\n>>>>>> I love you mate, thanks a ton once again for ACTUALLY RESPONDING.\n>>>>>>\n>>>>>> I am now going through the documentation (\n>>>>>> https://github.com/steveloughran/hadoop/blob/s3guard/HADOOP\n>>>>>> -13786-committer/hadoop-tools/hadoop-aws/src/site/markdown/t\n>>>>>> ools/hadoop-aws/s3a_committer_architecture.md) and it makes much\n>>>>>> much more sense now.\n>>>>>>\n>>>>>> Regards,\n>>>>>> Gourav Sengupta\n>>>>>>\n>>>>>> On Thu, Aug 3, 2017 at 10:09 AM, Steve Loughran <\n>>>>>> stevel@hortonworks.com> wrote:\n>>>>>>\n>>>>>>>\n>>>>>>> On 2 Aug 2017, at 20:05, Gourav Sengupta <gourav.sengupta@gmail.com>\n>>>>>>> wrote:\n>>>>>>>\n>>>>>>> Hi Steve,\n>>>>>>>\n>>>>>>> I have written a sincere note of apology to everyone in a separate\n>>>>>>> email. I sincerely request your kind forgiveness before hand\nif anything\n>>>>>>> does sound impolite in my emails, in advance.\n>>>>>>>\n>>>>>>> Let me first start by thanking you.\n>>>>>>>\n>>>>>>> I know it looks like I formed all my opinion based on that document,\n>>>>>>> but that is not the case at all. If you or anyone tries to execute\nthe code\n>>>>>>> that I have given then they will see what I mean. Code speaks\nlouder and\n>>>>>>> better than words for me.\n>>>>>>>\n>>>>>>> So I am not saying you are wrong. I am asking verify and expecting\n>>>>>>> someone will be able to correct  a set of understanding that\na moron like\n>>>>>>> me has gained after long hours of not having anything better\nto do.\n>>>>>>>\n>>>>>>>\n>>>>>>> SCENARIO: there are two files file1.csv and file2.csv stored\nin HDFS\n>>>>>>> with replication 2 and there is a HADOOP cluster of three nodes.\nAll these\n>>>>>>> nodes have SPARK workers (executors) running in them.  Both are\nstored in\n>>>>>>> the following way:\n>>>>>>> -----------------------------------------------------\n>>>>>>> | SYSTEM 1 |  SYSTEM 2 | SYSTEM 3 |\n>>>>>>> | (worker1)   |  (worker2)    |  (worker3)   |\n>>>>>>> | (master)     |                     |                    |\n>>>>>>> -----------------------------------------------------\n>>>>>>> | file1.csv      |                     | file1.csv     |\n>>>>>>> -----------------------------------------------------\n>>>>>>> |                    |  file2.csv      | file2.csv     |\n>>>>>>> -----------------------------------------------------\n>>>>>>> | file3.csv      |  file3.csv      |                   |\n>>>>>>> -----------------------------------------------------\n>>>>>>>\n>>>>>>>\n>>>>>>>\n>>>>>>>\n>>>>>>>\n>>>>>>> CONSIDERATION BASED ON WHICH ABOVE SCENARIO HAS BEEN DRAWN:\n>>>>>>> HDFS replication does not store the same file in all the nodes\nin\n>>>>>>> the cluster. So if I have three nodes and the replication is\ntwo then the\n>>>>>>> same file will be stored physically in two nodes in the cluster.\nDoes that\n>>>>>>> sound right?\n>>>>>>>\n>>>>>>>\n>>>>>>> HDFS breaks files up into blocks (default = 128MB). If a .csv\nfile\n>>>>>>> is > 128 then it will be broken up into blocks\n>>>>>>>\n>>>>>>> file1.cvs -> [block0001, block002, block0003]\n>>>>>>>\n>>>>>>> and each block will be replicated. With replication = 2 there\nwill\n>>>>>>> be two copies of each block, but the file itself can span >\n2 hosts.\n>>>>>>>\n>>>>>>>\n>>>>>>> ASSUMPTION  (STEVE PLEASE CLARIFY THIS):\n>>>>>>> If SPARK is trying to process to the records then I am expecting\n>>>>>>> that WORKER2 should not be processing file1.csv, and similary\nWORKER 1\n>>>>>>> should not be processing file2.csv and WORKER3 should not be\nprocessing\n>>>>>>> file3.csv. Because in case WORKER2 was trying to process file1.csv\nthen it\n>>>>>>> will actually causing network transmission of the file unnecessarily.\n>>>>>>>\n>>>>>>>\n>>>>>>> Spark prefers to schedule work locally, so as to save on network\n>>>>>>> traffic, but it schedules for execution time over waiting for\nworkers free\n>>>>>>> on the node with the data. IF a block is on nodes 2 and 3 but\nthere is only\n>>>>>>> a free thread on node 1, then node 1 gets the work\n>>>>>>>\n>>>>>>> There's details on whether/how work across blocks takes place\nwhich\n>>>>>>> I'm avoiding. For now know those formats which are \"splittable\"\nwill have\n>>>>>>> work scheduled by block. If you use Parquet/ORC/avro for your\ndata and\n>>>>>>> compress with snappy, it will be split. This gives you maximum\nperformance\n>>>>>>> as >1 thread can work on different blocks. That is, if file1\nis split into\n>>>>>>> three blocks, three worker threads can process it.\n>>>>>>>\n>>>>>>>\n>>>>>>> ASSUMPTION BASED ON ABOVE ASSUMPTION (STEVE ONCE AGAIN, PLEASE\n>>>>>>> CLARIFY THIS):\n>>>>>>> if WORKER 2 is not processing file1.csv then how does it matter\n>>>>>>> whether the file is there or not at all in the system? Should\nnot SPARK\n>>>>>>> just ask the workers to process the files which are avialable\nin the worker\n>>>>>>> nodes? In case both WORKER2 and WORKER3 fails and are not available\nthen\n>>>>>>> file2.csv will not be processed at all.\n>>>>>>>\n>>>>>>>\n>>>>>>> locality is best-effort, not guaranteed.\n>>>>>>>\n>>>>>>>\n>>>>>>> ALSO I DID POST THE CODE AND I GENUINELY THINK THAT THE CODE\nSHOULD\n>>>>>>> BE EXECUTED (Its been pointed out that I am learning SPARK, and\neven I did\n>>>>>>> not take more than 13 mins to set up the cluster and run the\ncode).\n>>>>>>>\n>>>>>>> Once you execute the code then you will find that:\n>>>>>>> 1.  if the path starts with file:/// while reading back then\nthere\n>>>>>>> is no error reported, but the number of records reported back\nare only\n>>>>>>> those records in the worker which also has the server.\n>>>>>>> 2. also you will notice that once you cache the file before writing\n>>>>>>> the partitions are ditributed nicely across the workers, and\nwhile writing\n>>>>>>> back, the dataframe partitions does write properly to the worker\nnode in\n>>>>>>> the Master, but the workers in the other system have the files\nwritten in\n>>>>>>> _temporary folder which does not get copied back to the main\nfolder.\n>>>>>>> Inspite of this the job is not reported as failed in SPARK.\n>>>>>>>\n>>>>>>>\n>>>>>>> This gets into the \"commit protocol\". You don't want to know\nall the\n>>>>>>> dirty details (*) but essentially its this\n>>>>>>>\n>>>>>>> 1. Every worker writes its output to a directory under the\n>>>>>>> destination directory, something like '$dest/_temporary/$appAtt\n>>>>>>> emptId/_temporary/$taskAttemptID'\n>>>>>>> 2. it is the spark driver which \"commits\" the job by moving the\n>>>>>>> output from the individual workers from the temporary directories\ninto\n>>>>>>> $dest, then deleting $dest/_temporary\n>>>>>>> 3. For which it needs to be able to list all the output in\n>>>>>>> $dest/_temporary\n>>>>>>>\n>>>>>>> In your case, only the output on the same node of the driver\nis\n>>>>>>> being committed, because only those files can be listed and moved.\nThe\n>>>>>>> output on the other nodes isn't seen, so isn't committed, nor\ncleaned up.\n>>>>>>>\n>>>>>>>\n>>>>>>>\n>>>>>>> Now in my own world, if I see, the following things are happening,\n>>>>>>> something is going wrong (with me):\n>>>>>>> 1. SPARK transfers files from different systems to process, instead\n>>>>>>> of processing them locally (I do not have code to prove this,\nand therefore\n>>>>>>> its just an assumption)\n>>>>>>> 2. SPARK cannot determine when the writes are failing in standalone\n>>>>>>> clusters workers and reports success (code is there for this)\n>>>>>>> 3. SPARK reports back number of records in the worker running\nin the\n>>>>>>> master node when count() is given without reporting an error\nwhile using\n>>>>>>> file:/// and reports an error when I mention the path without\n>>>>>>> file:/// (for SPARK 2.1.x onwards, code is there for this)\n>>>>>>>\n>>>>>>>\n>>>>>>>\n>>>>>>> s everyone's been saying, file:// requires a shared filestore,\nwith\n>>>>>>> uniform paths everywhere. That's needed to list the files to\nprocess, read\n>>>>>>> the files in the workers and commit the final output. NFS cross-mounting\nis\n>>>>>>> the simplest way to do this, especially as for three nodes HDFS\nis\n>>>>>>> overkill: more services to keep running, no real fault tolerance.\nExport a\n>>>>>>> directory tree from one of the servers, give the rest access\nto it, don't\n>>>>>>> worry about bandwidth use as the shared disk itself will become\nthe\n>>>>>>> bottleneck\n>>>>>>>\n>>>>>>>\n>>>>>>>\n>>>>>>> I very sincerely hope with your genuine help the bar of language\nand\n>>>>>>> social skills will be lowered for me. And everyone will find\na way to\n>>>>>>> excuse me and not qualify this email as a means to measure my\nextremely\n>>>>>>> versatile and amazingly vivid social skills. It will be a lot\nof help to\n>>>>>>> just focus on the facts related to machines, data, error and\n(the language\n>>>>>>> that I somehow understand better) code.\n>>>>>>>\n>>>>>>>\n>>>>>>> My sincere apologies once again, as I am 100% sure that I did\nnot\n>>>>>>> meet the required social and language skills.\n>>>>>>>\n>>>>>>> Thanks a ton once again for your kindness, patience and\n>>>>>>> understanding.\n>>>>>>>\n>>>>>>>\n>>>>>>> Regards,\n>>>>>>> Gourav Sengupta\n>>>>>>>\n>>>>>>>\n>>>>>>> * for the curious, the details of the v1 and v2 commit protocols\nare\n>>>>>>> https://github.com/steveloughran/hadoop/blob/s3guard/HADOOP-\n>>>>>>> 13786-committer/hadoop-tools/hadoop-aws/src/site/markdown/to\n>>>>>>> ols/hadoop-aws/s3a_committer_architecture.md\n>>>>>>>\n>>>>>>> Like I said: you don't want to know the details, and you really\n>>>>>>> don't want to step through Hadoop's FileOutputCommitter to see\nwhat's going\n>>>>>>> on. The Spark side is much easier to follow.\n>>>>>>>\n>>>>>>>\n>>>>>>\n>>>>>\n>>>>\n>>>\n>>\n\n",
  "denotations": [
    {
      "id": 1,
      "start": 0,
      "end": 9,
      "text": "Sengupta\n",
      "type": "Body/Intro",
      "meta": null
    },
    {
      "id": 2,
      "start": 709,
      "end": 729,
      "text": "\nhope this helps\nkr\n",
      "type": "Body/Outro",
      "meta": null
    },
    {
      "id": 3,
      "start": 0,
      "end": 828,
      "text": "Sengupta\n further to this, if you try the following notebook in databricks cloud, it\nwill read a .csv file , write to a parquet file and read it again (just to\ncount the number of rows stored)\nPlease note that the path to the csv file might differ for you.....\nSo, what you will need todo is\n1 - create an account to community.cloud.databricks.com\n2 - upload the .csv file onto the Data of your databricks private cluster\n3  - run the script. that will store the data on the distrubuted filesystem\nof the databricks cloudn (dbfs)\n\nIt's worth investing in this free databricks cloud as it can create a\ncluster for you with minimal effort, and it's  a very easy way to test your\nspark scripts on a real cluster\n\nhope this helps\nkr\n\n##################################\nfrom pyspark.sql import SQLContext\n\nfrom random import randint\n",
      "type": "Body",
      "meta": null
    }
  ],
  "meta": {},
  "id": "test/train_4544"
}