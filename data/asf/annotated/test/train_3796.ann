{
  "wrapper": "plaintext",
  "text": "Hi, all!\nI have a code, serializing RDD as Kryo, and saving it as sequence file. It\nworks fine in 1.5.1, but, while switching to 2.1.1 it does not work.\n\nI am trying to serialize RDD of Tuple2<> (got from PairRDD).\n\n   1. RDD consists of different heterogeneous objects (aggregates, like\n   HLL, QTree, Min, Max, etc.)\n   2. Save is performed within streaming\n   3. Read is performed out of streaming (another app)\n   4. Supposed, that such error can be due to custom serializers - turned\n   them off, but errors still exists\n   5. Tried disabling references in Kryo (since I saw an error while\n   resolving references) - got StackOverflow, and significant performance\n   degradation\n   6. Implementing Serializable/Externalizable is not a solution,\n   unfortunately.\n\nExpected behavior:\n\nsaveAsObjectFile/loadObjectFile are symmetric, and it's possible to load\npreviously saved RDD.\n\nCode of save/load:\n\nobject KryoFile {\n\n  val THREAD_LOCAL_CACHE = new ThreadLocal[Kryo]\n\n  /*\n   * Used to write as Object file using kryo serialization\n   */\n  def saveAsObjectFile[T: ClassTag](rdd: RDD[T], path: String) {\n    val kryoSerializer = new KryoSerializer(rdd.context.getConf)\n\n    rdd.context.setJobDescription(\"Saving to path \" + path)\n    rdd.mapPartitions(iter => iter.grouped(10)\n      .map(_.toArray))\n      .map(splitArray => {\n        //initializes kyro and calls your registrator class\n        var kryo = THREAD_LOCAL_CACHE.get()\n        if (null == kryo) {\n          kryo = kryoSerializer.newKryo()\n          THREAD_LOCAL_CACHE.set(kryo)\n        }\n\n        //convert data to bytes\n        val bao = new ByteArrayOutputStream()\n        val output = kryoSerializer.newKryoOutput()\n        output.setOutputStream(bao)\n        kryo.writeClassAndObject(output, splitArray)\n        output.close()\n        kryo.reset()\n\n        // We are ignoring key field of sequence file\n        val byteWritable = new BytesWritable(bao.toByteArray)\n        (NullWritable.get(), byteWritable)\n      }).saveAsSequenceFile(path)\n  }\n\n  /*\n   * Method to read from object file which is saved kryo format.\n   */\n  def loadObjectFile[T](sc: SparkContext, path: String, minPartitions:\nInt = 1)(implicit ct: ClassTag[T]) = {\n    val kryoSerializer = new KryoSerializer(sc.getConf)\n\n    sc.sequenceFile(path, classOf[NullWritable],\nclassOf[BytesWritable], minPartitions)\n      .flatMap(x => {\n\n        var kryo = THREAD_LOCAL_CACHE.get()\n        if (null == kryo) {\n          kryo = kryoSerializer.newKryo()\n          THREAD_LOCAL_CACHE.set(kryo)\n        }\n\n        val input = new Input()\n        input.setBuffer(x._2.getBytes)\n        val data = kryo.readClassAndObject(input)\n        kryo.reset()\n        val dataObject = data.asInstanceOf[Array[T]]\n        dataObject\n      })\n\n  }\n}\n\n\nWhen trying to deserialize, I got such errors:\n17/06/21 08:19:18 ERROR Executor: Exception in task 14.0 in stage 0.0 (TID\n14)\njava.lang.ArrayIndexOutOfBoundsException: -2\n    at java.util.ArrayList.elementData(ArrayList.java:418)\n    at java.util.ArrayList.get(ArrayList.java:431)\n    at\ncom.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:60)\n    at com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:834)\n    at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:706)\n    at\ncom.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:396)\n    at\ncom.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307)\n    at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)\n    at\ncom.badoo.uds.commons.helper.KryoFile$$anonfun$objectFile$1.apply(KryoFile.scala:75)\n    at\ncom.badoo.uds.commons.helper.KryoFile$$anonfun$objectFile$1.apply(KryoFile.scala:62)\n    at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n    at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n    at\norg.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n    at\norg.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n    at org.apache.spark.scheduler.Task.run(Task.scala:99)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n    at\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\n17/06/21 08:19:18 ERROR Executor: Exception in task 12.0 in stage 0.0 (TID\n12)\njava.lang.ArrayStoreException: java.util.Collections$EmptyMap\n    at\ncom.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:396)\n    at\ncom.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307)\n    at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)\n    at\ncom.badoo.uds.commons.helper.KryoFile$$anonfun$objectFile$1.apply(KryoFile.scala:75)\n    at\ncom.badoo.uds.commons.helper.KryoFile$$anonfun$objectFile$1.apply(KryoFile.scala:62)\n    at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n    at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n    at\norg.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n    at\norg.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n    at org.apache.spark.scheduler.Task.run(Task.scala:99)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n    at\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n--\n*Alexander Krasheninnikov*\nHead of Data Team\n\n",
  "denotations": [
    {
      "id": 1,
      "start": 0,
      "end": 9,
      "text": "Hi, all!\n",
      "type": "Body/Intro",
      "meta": null
    },
    {
      "id": 2,
      "start": 6369,
      "end": 6417,
      "text": "--\n*Alexander Krasheninnikov*\nHead of Data Team\n",
      "type": "Body/Signature",
      "meta": null
    },
    {
      "id": 3,
      "start": 0,
      "end": 6418,
      "text": "Hi, all!\nI have a code, serializing RDD as Kryo, and saving it as sequence file. It\nworks fine in 1.5.1, but, while switching to 2.1.1 it does not work.\n\nI am trying to serialize RDD of Tuple2<> (got from PairRDD).\n\n   1. RDD consists of different heterogeneous objects (aggregates, like\n   HLL, QTree, Min, Max, etc.)\n   2. Save is performed within streaming\n   3. Read is performed out of streaming (another app)\n   4. Supposed, that such error can be due to custom serializers - turned\n   them off, but errors still exists\n   5. Tried disabling references in Kryo (since I saw an error while\n   resolving references) - got StackOverflow, and significant performance\n   degradation\n   6. Implementing Serializable/Externalizable is not a solution,\n   unfortunately.\n\nExpected behavior:\n\nsaveAsObjectFile/loadObjectFile are symmetric, and it's possible to load\npreviously saved RDD.\n\nCode of save/load:\n\nobject KryoFile {\n\n  val THREAD_LOCAL_CACHE = new ThreadLocal[Kryo]\n\n  /*\n   * Used to write as Object file using kryo serialization\n   */\n  def saveAsObjectFile[T: ClassTag](rdd: RDD[T], path: String) {\n    val kryoSerializer = new KryoSerializer(rdd.context.getConf)\n\n    rdd.context.setJobDescription(\"Saving to path \" + path)\n    rdd.mapPartitions(iter => iter.grouped(10)\n      .map(_.toArray))\n      .map(splitArray => {\n        //initializes kyro and calls your registrator class\n        var kryo = THREAD_LOCAL_CACHE.get()\n        if (null == kryo) {\n          kryo = kryoSerializer.newKryo()\n          THREAD_LOCAL_CACHE.set(kryo)\n        }\n\n        //convert data to bytes\n        val bao = new ByteArrayOutputStream()\n        val output = kryoSerializer.newKryoOutput()\n        output.setOutputStream(bao)\n        kryo.writeClassAndObject(output, splitArray)\n        output.close()\n        kryo.reset()\n\n        // We are ignoring key field of sequence file\n        val byteWritable = new BytesWritable(bao.toByteArray)\n        (NullWritable.get(), byteWritable)\n      }).saveAsSequenceFile(path)\n  }\n\n  /*\n   * Method to read from object file which is saved kryo format.\n   */\n  def loadObjectFile[T](sc: SparkContext, path: String, minPartitions:\nInt = 1)(implicit ct: ClassTag[T]) = {\n    val kryoSerializer = new KryoSerializer(sc.getConf)\n\n    sc.sequenceFile(path, classOf[NullWritable],\nclassOf[BytesWritable], minPartitions)\n      .flatMap(x => {\n\n        var kryo = THREAD_LOCAL_CACHE.get()\n        if (null == kryo) {\n          kryo = kryoSerializer.newKryo()\n          THREAD_LOCAL_CACHE.set(kryo)\n        }\n\n        val input = new Input()\n        input.setBuffer(x._2.getBytes)\n        val data = kryo.readClassAndObject(input)\n        kryo.reset()\n        val dataObject = data.asInstanceOf[Array[T]]\n        dataObject\n      })\n\n  }\n}\n\n\nWhen trying to deserialize, I got such errors:\n17/06/21 08:19:18 ERROR Executor: Exception in task 14.0 in stage 0.0 (TID\n14)\njava.lang.ArrayIndexOutOfBoundsException: -2\n    at java.util.ArrayList.elementData(ArrayList.java:418)\n    at java.util.ArrayList.get(ArrayList.java:431)\n    at\ncom.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:60)\n    at com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:834)\n    at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:706)\n    at\ncom.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:396)\n    at\ncom.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307)\n    at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)\n    at\ncom.badoo.uds.commons.helper.KryoFile$$anonfun$objectFile$1.apply(KryoFile.scala:75)\n    at\ncom.badoo.uds.commons.helper.KryoFile$$anonfun$objectFile$1.apply(KryoFile.scala:62)\n    at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n    at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n    at\norg.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n    at\norg.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n    at org.apache.spark.scheduler.Task.run(Task.scala:99)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n    at\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n\n17/06/21 08:19:18 ERROR Executor: Exception in task 12.0 in stage 0.0 (TID\n12)\njava.lang.ArrayStoreException: java.util.Collections$EmptyMap\n    at\ncom.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:396)\n    at\ncom.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307)\n    at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)\n    at\ncom.badoo.uds.commons.helper.KryoFile$$anonfun$objectFile$1.apply(KryoFile.scala:75)\n    at\ncom.badoo.uds.commons.helper.KryoFile$$anonfun$objectFile$1.apply(KryoFile.scala:62)\n    at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n    at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n    at\norg.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n    at\norg.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n    at org.apache.spark.scheduler.Task.run(Task.scala:99)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n    at\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n--\n*Alexander Krasheninnikov*\nHead of Data Team\n\n",
      "type": "Body",
      "meta": null
    }
  ],
  "meta": {},
  "id": "test/train_3796"
}