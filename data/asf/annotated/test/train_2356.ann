{
  "wrapper": "plaintext",
  "text": "I am sharing this code snippet since I spent quite some time figuring it\nout and I couldn't find any examples online. Between the Kinesis\ndocumentation, tutorial on AWS site and other code snippets on the\nInternet, I was confused about structure/format of the messages that Spark\nfetches from Kinesis - base64 encoded, json, gzipped - which one first and\nwhat order.\n\nI tested this on EMR-5.4.0, Amazon Hadoop 2.7.3 and Spark 2.1.0. Hope it\nhelps others googling for similar info. I tried using Structured Streaming\nbut (1) it's in Alpha and (2) despite including what I thought were all the\ndependencies, it complained of not finding DataSource.Kinesis. You probably\ndo not need all the libs but I am just too lazy to redact ones you don't\nrequire for the snippet below :)\n\nimport org.apache.spark.streaming.Duration\nimport org.apache.spark.streaming.kinesis._\nimport\ncom.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream\nimport org.apache.spark._\nimport org.apache.spark.streaming._\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.rdd.RDD\nimport java.util.Base64\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.functions.explode\nimport org.apache.commons.math3.stat.descriptive._\nimport java.io.File\nimport java.net.InetAddress\nimport scala.util.control.NonFatal\nimport org.apache.spark.SparkFiles\nimport org.apache.spark.sql.SaveMode\nimport java.util.Properties;\nimport org.json4s._\nimport org.json4s.jackson.JsonMethods._\nimport java.io.{ByteArrayOutputStream, ByteArrayInputStream}\nimport java.util.zip.{GZIPOutputStream, GZIPInputStream}\nimport scala.util.Try\n\n\n//sc.setLogLevel(\"INFO\")\n\nval ssc = new StreamingContext(sc, Seconds(30))\n\nval kinesisStreams = (0 until 2).map { i => KinesisUtils.createStream(ssc,\n\"myApp\", \"cloudwatchlogs\",\n\"https://kinesis.us-east-1.amazonaws.com\",\"us-east-1\",\nInitialPositionInStream.LATEST , Seconds(30),\nStorageLevel.MEMORY_AND_DISK_2,\"myId\",\"mySecret\") }\n\nval unionStreams = ssc.union(kinesisStreams)\n\nunionStreams.foreachRDD((rdd: RDD[Array[Byte]], time: Time) => {\n      if(rdd.count() > 0) {\n      val json = rdd.map(input => {\n      val inputStream = new GZIPInputStream(new ByteArrayInputStream(input))\n      val record = scala.io.Source.fromInputStream(inputStream).mkString\n      compact(render(parse(record)))\n      })\n\n      val df = spark.sqlContext.read.json(json)\n      val preDF =\ndf.select($\"logGroup\",explode($\"logEvents\").as(\"events_flat\"))\n      val penDF = preDF.select($\"logGroup\",$\"events_flat.extractedFields\")\n      val finalDF =\npenDF.select($\"logGroup\".as(\"cluster\"),$\"extractedFields.*\")\n      finalDF.printSchema()\n      finalDF.show()\n     }\n})\n\nssc.start\n\n\n\n--\nThanks,\n\nTim\n\n",
  "denotations": [
    {
      "id": 1,
      "start": 2766,
      "end": 2782,
      "text": "--\nThanks,\n\nTim\n",
      "type": "Body/Outro",
      "meta": null
    },
    {
      "id": 2,
      "start": 0,
      "end": 2783,
      "text": "I am sharing this code snippet since I spent quite some time figuring it\nout and I couldn't find any examples online. Between the Kinesis\ndocumentation, tutorial on AWS site and other code snippets on the\nInternet, I was confused about structure/format of the messages that Spark\nfetches from Kinesis - base64 encoded, json, gzipped - which one first and\nwhat order.\n\nI tested this on EMR-5.4.0, Amazon Hadoop 2.7.3 and Spark 2.1.0. Hope it\nhelps others googling for similar info. I tried using Structured Streaming\nbut (1) it's in Alpha and (2) despite including what I thought were all the\ndependencies, it complained of not finding DataSource.Kinesis. You probably\ndo not need all the libs but I am just too lazy to redact ones you don't\nrequire for the snippet below :)\n\nimport org.apache.spark.streaming.Duration\nimport org.apache.spark.streaming.kinesis._\nimport\ncom.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream\nimport org.apache.spark._\nimport org.apache.spark.streaming._\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.rdd.RDD\nimport java.util.Base64\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.functions.explode\nimport org.apache.commons.math3.stat.descriptive._\nimport java.io.File\nimport java.net.InetAddress\nimport scala.util.control.NonFatal\nimport org.apache.spark.SparkFiles\nimport org.apache.spark.sql.SaveMode\nimport java.util.Properties;\nimport org.json4s._\nimport org.json4s.jackson.JsonMethods._\nimport java.io.{ByteArrayOutputStream, ByteArrayInputStream}\nimport java.util.zip.{GZIPOutputStream, GZIPInputStream}\nimport scala.util.Try\n\n\n//sc.setLogLevel(\"INFO\")\n\nval ssc = new StreamingContext(sc, Seconds(30))\n\nval kinesisStreams = (0 until 2).map { i => KinesisUtils.createStream(ssc,\n\"myApp\", \"cloudwatchlogs\",\n\"https://kinesis.us-east-1.amazonaws.com\",\"us-east-1\",\nInitialPositionInStream.LATEST , Seconds(30),\nStorageLevel.MEMORY_AND_DISK_2,\"myId\",\"mySecret\") }\n\nval unionStreams = ssc.union(kinesisStreams)\n\nunionStreams.foreachRDD((rdd: RDD[Array[Byte]], time: Time) => {\n      if(rdd.count() > 0) {\n      val json = rdd.map(input => {\n      val inputStream = new GZIPInputStream(new ByteArrayInputStream(input))\n      val record = scala.io.Source.fromInputStream(inputStream).mkString\n      compact(render(parse(record)))\n      })\n\n      val df = spark.sqlContext.read.json(json)\n      val preDF =\ndf.select($\"logGroup\",explode($\"logEvents\").as(\"events_flat\"))\n      val penDF = preDF.select($\"logGroup\",$\"events_flat.extractedFields\")\n      val finalDF =\npenDF.select($\"logGroup\".as(\"cluster\"),$\"extractedFields.*\")\n      finalDF.printSchema()\n      finalDF.show()\n     }\n})\n\nssc.start\n\n\n\n--\nThanks,\n\nTim\n\n",
      "type": "Body",
      "meta": null
    }
  ],
  "meta": {},
  "id": "test/train_2356"
}