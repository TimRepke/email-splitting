{
  "wrapper": "plaintext",
  "text": "\nI only want to simulate very huge \"network\" with even millions parallel time syncronized actors\n(state machines). There are also communication between actors via some (key-value pairs) database.\nI also want the simulation should work in the real time.\n\n\nI don't know what would be the best framework or tool for that kind of simulation. I think\nAkka would be the best and easiest to deploy ?\n\nOr do you know better frameworks or tools ?\n\n\nNowdays Spark is using Netty instead of Akka ?\n\n________________________________\nL\u00c3\u00a4hett\u00c3\u00a4j\u00c3\u00a4: J\u00c3\u00b6rn Franke <jornfranke@gmail.com>\nL\u00c3\u00a4hetetty: 7. hein\u00c3\u00a4kuuta 2017 10:04\nVastaanottaja: Esa Heikkinen\nKopio: Mahesh Sawaiker; user@spark.apache.org\nAihe: Re: VS: Using Spark as a simulator\n\nSpark dropped Akka some time ago...\nI think the main issue he will face is a library for simulating the state machines (randomly),\nstoring a huge amount of files (HDFS is probably the way to go if you want it fast) and distributing\nthe work (here you can select different options).\nAre you trying to have some mathematical guarantees on the state machines, such as Markov\nchains/steady state?\n\nOn 7. Jul 2017, at 08:46, Esa Heikkinen <esa.heikkinen@student.tut.fi<mailto:esa.heikkinen@student.tut.fi>>\nwrote:\n\n\n\nWould it be better to use Akka as simulator rather than Spark ?\n\nhttp://akka.io/\n\nAkka<http://akka.io/>\nakka.io<http://akka.io>\nBuild powerful reactive, concurrent & distributed applications more easily. Akka is a\ntoolkit and runtime for building highly concurrent, distributed, and resilient ...\n\n\nThe spark was originally built on it (Akka).\n\n\nEsa\n\n________________________________\nL\u00c3\u00a4hett\u00c3\u00a4j\u00c3\u00a4: Mahesh Sawaiker <mahesh_sawaiker@persistent.com<mailto:mahesh_sawaiker@persistent.com>>\nL\u00c3\u00a4hetetty: 21. kes\u00c3\u00a4kuuta 2017 14:45\nVastaanottaja: Esa Heikkinen; J\u00c3\u00b6rn Franke\nKopio: user@spark.apache.org<mailto:user@spark.apache.org>\nAihe: RE: Using Spark as a simulator\n\n\nSpark can help you to create one large file if needed, but hdfs itself will provide abstraction\nover such things, so it\u00e2\u20ac\u2122s a trivial problem if anything.\n\nIf you have spark installed, then you can use spark-shell to try a few samples, and build\nfrom there.If you can collect all the files in a folder then spark can read all files from\nthere. The programming guide below has enough information to get started.\n\n\n\nhttps://spark.apache.org/docs/latest/programming-guide.html\n\nSpark Programming Guide - Spark 2.1.1 Documentation<https://spark.apache.org/docs/latest/programming-guide.html>\nspark.apache.org<http://spark.apache.org>\nSpark Programming Guide. Overview; Linking with Spark; Initializing Spark. Using the Shell;\nResilient Distributed Datasets (RDDs) Parallelized Collections\n\n\nAll of Spark\u00e2\u20ac\u2122s file-based input methods, including textFile, support running on directories,\ncompressed files, and wildcards as well. For example, you can use textFile(\"/my/directory\"),\ntextFile(\"/my/directory/*.txt\"), and textFile(\"/my/directory/*.gz\").\n\n\n\nAfter reading the file you can map it using map function, which will split the individual\nline and possibly create a scala object. This way you will get a RDD of scala objects, which\nyou can then process functional/set operators.\n\n\n\nYou would want to read about PairRDDs.\n\n\n\nFrom: Esa Heikkinen [mailto:esa.heikkinen@student.tut.fi]\nSent: Wednesday, June 21, 2017 1:12 PM\nTo: J\u00c3\u00b6rn Franke\nCc: user@spark.apache.org<mailto:user@spark.apache.org>\nSubject: VS: Using Spark as a simulator\n\n\n\n\n\nHi\n\n\n\nThanks for the answer.\n\n\n\nI think my simulator includes a lot of parallel state machines and each of them generates\nlog file (with timestamps). Finally all events (rows) of all log files should combine as time\norder to (one) very huge log file. Practically the combined huge log file can also be split\ninto smaller ones.\n\n\n\nWhat transformation or action functions can i use in Spark for that purpose ?\n\nOr are there exist some code sample (Python or Scala) about that ?\n\nRegards\n\nEsa Heikkinen\n\n\n\n________________________________\n\nL\u00c3\u00a4hett\u00c3\u00a4j\u00c3\u00a4: J\u00c3\u00b6rn Franke <jornfranke@gmail.com<mailto:jornfranke@gmail.com>>\nL\u00c3\u00a4hetetty: 20. kes\u00c3\u00a4kuuta 2017 17:12\nVastaanottaja: Esa Heikkinen\nKopio: user@spark.apache.org<mailto:user@spark.apache.org>\nAihe: Re: Using Spark as a simulator\n\n\n\nIt is fine, but you have to design it that generated rows are written in large blocks for\noptimal performance.\n\nThe most tricky part with data generation is the conceptual part, such as probabilistic distribution\netc\n\nYou have to check as well that you use a good random generator, for some cases the Java internal\nmight be not that well.\n\nOn 20. Jun 2017, at 16:04, Esa Heikkinen <esa.heikkinen@student.tut.fi<mailto:esa.heikkinen@student.tut.fi>>\nwrote:\n\nHi\n\n\n\nSpark is a data analyzer, but would it be possible to use Spark as a data generator or simulator\n?\n\n\n\nMy simulation can be very huge and i think a parallelized simulation using by Spark (cloud)\ncould work.\n\nIs that good or bad idea ?\n\n\n\nRegards\n\nEsa Heikkinen\n\n\n\nDISCLAIMER\n==========\nThis e-mail may contain privileged and confidential information which is the property of Persistent\nSystems Ltd. It is intended only for the use of the individual or entity to which it is addressed.\nIf you are not the intended recipient, you are not authorized to read, retain, copy, print,\ndistribute or use this message. If you have received this communication in error, please notify\nthe sender and delete all copies of this message. Persistent Systems Ltd. does not accept\nany liability for virus infected mails.\n\n",
  "denotations": [
    {
      "id": 1,
      "start": 488,
      "end": 725,
      "text": "________________________________\nL\u00c3\u00a4hett\u00c3\u00a4j\u00c3\u00a4: J\u00c3\u00b6rn Franke <jornfranke@gmail.com>\nL\u00c3\u00a4hetetty: 7. hein\u00c3\u00a4kuuta 2017 10:04\nVastaanottaja: Esa Heikkinen\nKopio: Mahesh Sawaiker; user@spark.apache.org\nAihe: Re: VS: Using Spark as a simulator\n",
      "type": "Header",
      "meta": null
    },
    {
      "id": 2,
      "start": 0,
      "end": 488,
      "text": "\nI only want to simulate very huge \"network\" with even millions parallel time syncronized actors\n(state machines). There are also communication between actors via some (key-value pairs) database.\nI also want the simulation should work in the real time.\n\n\nI don't know what would be the best framework or tool for that kind of simulation. I think\nAkka would be the best and easiest to deploy ?\n\nOr do you know better frameworks or tools ?\n\n\nNowdays Spark is using Netty instead of Akka ?\n\n",
      "type": "Body",
      "meta": null
    },
    {
      "id": 3,
      "start": 725,
      "end": 1120,
      "text": "\nSpark dropped Akka some time ago...\nI think the main issue he will face is a library for simulating the state machines (randomly),\nstoring a huge amount of files (HDFS is probably the way to go if you want it fast) and distributing\nthe work (here you can select different options).\nAre you trying to have some mathematical guarantees on the state machines, such as Markov\nchains/steady state?\n\n",
      "type": "Body",
      "meta": null
    },
    {
      "id": 4,
      "start": 1120,
      "end": 1235,
      "text": "On 7. Jul 2017, at 08:46, Esa Heikkinen <esa.heikkinen@student.tut.fi<mailto:esa.heikkinen@student.tut.fi>>\nwrote:\n",
      "type": "Header",
      "meta": null
    },
    {
      "id": 5,
      "start": 1583,
      "end": 1588,
      "text": "\nEsa\n",
      "type": "Body/Outro",
      "meta": null
    },
    {
      "id": 6,
      "start": 1589,
      "end": 1901,
      "text": "________________________________\nL\u00c3\u00a4hett\u00c3\u00a4j\u00c3\u00a4: Mahesh Sawaiker <mahesh_sawaiker@persistent.com<mailto:mahesh_sawaiker@persistent.com>>\nL\u00c3\u00a4hetetty: 21. kes\u00c3\u00a4kuuta 2017 14:45\nVastaanottaja: Esa Heikkinen; J\u00c3\u00b6rn Franke\nKopio: user@spark.apache.org<mailto:user@spark.apache.org>\nAihe: RE: Using Spark as a simulator\n",
      "type": "Body/Signature",
      "meta": null
    },
    {
      "id": 7,
      "start": 1901,
      "end": 3225,
      "text": "\n\nSpark can help you to create one large file if needed, but hdfs itself will provide abstraction\nover such things, so it\u00e2\u20ac\u2122s a trivial problem if anything.\n\nIf you have spark installed, then you can use spark-shell to try a few samples, and build\nfrom there.If you can collect all the files in a folder then spark can read all files from\nthere. The programming guide below has enough information to get started.\n\n\n\nhttps://spark.apache.org/docs/latest/programming-guide.html\n\nSpark Programming Guide - Spark 2.1.1 Documentation<https://spark.apache.org/docs/latest/programming-guide.html>\nspark.apache.org<http://spark.apache.org>\nSpark Programming Guide. Overview; Linking with Spark; Initializing Spark. Using the Shell;\nResilient Distributed Datasets (RDDs) Parallelized Collections\n\n\nAll of Spark\u00e2\u20ac\u2122s file-based input methods, including textFile, support running on directories,\ncompressed files, and wildcards as well. For example, you can use textFile(\"/my/directory\"),\ntextFile(\"/my/directory/*.txt\"), and textFile(\"/my/directory/*.gz\").\n\n\n\nAfter reading the file you can map it using map function, which will split the individual\nline and possibly create a scala object. This way you will get a RDD of scala objects, which\nyou can then process functional/set operators.\n\n\n\nYou would want to read about PairRDDs.\n\n\n\n",
      "type": "Body",
      "meta": null
    },
    {
      "id": 8,
      "start": 3225,
      "end": 3435,
      "text": "From: Esa Heikkinen [mailto:esa.heikkinen@student.tut.fi]\nSent: Wednesday, June 21, 2017 1:12 PM\nTo: J\u00c3\u00b6rn Franke\nCc: user@spark.apache.org<mailto:user@spark.apache.org>\nSubject: VS: Using Spark as a simulator\n",
      "type": "Header",
      "meta": null
    },
    {
      "id": 9,
      "start": 3439,
      "end": 3443,
      "text": "\nHi\n",
      "type": "Body/Intro",
      "meta": null
    },
    {
      "id": 10,
      "start": 3916,
      "end": 3940,
      "text": "\nRegards\n\nEsa Heikkinen\n",
      "type": "Body/Outro",
      "meta": null
    },
    {
      "id": 11,
      "start": 3943,
      "end": 4219,
      "text": "________________________________\n\nL\u00c3\u00a4hett\u00c3\u00a4j\u00c3\u00a4: J\u00c3\u00b6rn Franke <jornfranke@gmail.com<mailto:jornfranke@gmail.com>>\nL\u00c3\u00a4hetetty: 20. kes\u00c3\u00a4kuuta 2017 17:12\nVastaanottaja: Esa Heikkinen\nKopio: user@spark.apache.org<mailto:user@spark.apache.org>\nAihe: Re: Using Spark as a simulator\n",
      "type": "Body/Signature",
      "meta": null
    },
    {
      "id": 12,
      "start": 3435,
      "end": 4562,
      "text": "\n\n\n\n\nHi\n\n\n\nThanks for the answer.\n\n\n\nI think my simulator includes a lot of parallel state machines and each of them generates\nlog file (with timestamps). Finally all events (rows) of all log files should combine as time\norder to (one) very huge log file. Practically the combined huge log file can also be split\ninto smaller ones.\n\n\n\nWhat transformation or action functions can i use in Spark for that purpose ?\n\nOr are there exist some code sample (Python or Scala) about that ?\n\nRegards\n\nEsa Heikkinen\n\n\n\n________________________________\n\nL\u00c3\u00a4hett\u00c3\u00a4j\u00c3\u00a4: J\u00c3\u00b6rn Franke <jornfranke@gmail.com<mailto:jornfranke@gmail.com>>\nL\u00c3\u00a4hetetty: 20. kes\u00c3\u00a4kuuta 2017 17:12\nVastaanottaja: Esa Heikkinen\nKopio: user@spark.apache.org<mailto:user@spark.apache.org>\nAihe: Re: Using Spark as a simulator\n\n\n\nIt is fine, but you have to design it that generated rows are written in large blocks for\noptimal performance.\n\nThe most tricky part with data generation is the conceptual part, such as probabilistic distribution\netc\n\nYou have to check as well that you use a good random generator, for some cases the Java internal\nmight be not that well.\n\n",
      "type": "Body",
      "meta": null
    },
    {
      "id": 13,
      "start": 4562,
      "end": 4678,
      "text": "On 20. Jun 2017, at 16:04, Esa Heikkinen <esa.heikkinen@student.tut.fi<mailto:esa.heikkinen@student.tut.fi>>\nwrote:\n",
      "type": "Header",
      "meta": null
    },
    {
      "id": 14,
      "start": 4678,
      "end": 4682,
      "text": "\nHi\n",
      "type": "Body/Intro",
      "meta": null
    },
    {
      "id": 15,
      "start": 4921,
      "end": 4945,
      "text": "\nRegards\n\nEsa Heikkinen\n",
      "type": "Body/Outro",
      "meta": null
    },
    {
      "id": 16,
      "start": 4678,
      "end": 5488,
      "text": "\nHi\n\n\n\nSpark is a data analyzer, but would it be possible to use Spark as a data generator or simulator\n?\n\n\n\nMy simulation can be very huge and i think a parallelized simulation using by Spark (cloud)\ncould work.\n\nIs that good or bad idea ?\n\n\n\nRegards\n\nEsa Heikkinen\n\n\n\nDISCLAIMER\n==========\nThis e-mail may contain privileged and confidential information which is the property of Persistent\nSystems Ltd. It is intended only for the use of the individual or entity to which it is addressed.\nIf you are not the intended recipient, you are not authorized to read, retain, copy, print,\ndistribute or use this message. If you have received this communication in error, please notify\nthe sender and delete all copies of this message. Persistent Systems Ltd. does not accept\nany liability for virus infected mails.\n\n",
      "type": "Body",
      "meta": null
    }
  ],
  "meta": {},
  "id": "test/train_4089"
}