{
  "wrapper": "plaintext",
  "text": "(this was also posted to stackoverflow on 03/10)\n\nI am setting up a very simple logistic regression problem in scikit-learn\nand in spark.ml, and the results diverge: the models they learn are\ndifferent, but I can't figure out why (data is the same, model type is the\nsame, regularization is the same...).\n\nNo doubt I am missing some setting on one side or the other. Which setting?\nHow should I set up either scikit or spark.ml to find the same model as its\ncounterpart?\n\nI give the sklearn code and spark.ml code below. Both should be ready to\ncut-and-paste and run.\n\nscikit-learn code:\n----------------------\n\n    import numpy as np\n    from sklearn.linear_model import LogisticRegression, Ridge\n\n    X = np.array([\n        [-0.7306653538519616, 0.0],\n        [0.6750417712898752, -0.4232874171873786],\n        [0.1863463229359709, -0.8163423997075965],\n        [-0.6719842051493347, 0.0],\n        [0.9699938346531928, 0.0],\n        [0.22759406190283604, 0.0],\n        [0.9688721028330911, 0.0],\n        [0.5993795346650845, 0.0],\n        [0.9219423508390701, -0.8972778242305388],\n        [0.7006904841584055, -0.5607635619919824]\n    ])\n\n    y = np.array([\n        0.0,\n        1.0,\n        1.0,\n        0.0,\n        1.0,\n        1.0,\n        1.0,\n        0.0,\n        0.0,\n        0.0\n    ])\n\n    m, n = X.shape\n\n    # Add intercept term to simulate inputs to GameEstimator\n    X_with_intercept = np.hstack((X, np.ones(m)[:,np.newaxis]))\n\n    l = 0.3\n    e = LogisticRegression(\n        fit_intercept=False,\n        penalty='l2',\n        C=1/l,\n        max_iter=100,\n        tol=1e-11)\n\n    e.fit(X_with_intercept, y)\n\n    print e.coef_\n    # => [[ 0.98662189  0.45571052 -0.23467255]]\n\n    # Linear regression is called Ridge in sklearn\n    e = Ridge(\n        fit_intercept=False,\n        alpha=l,\n        max_iter=100,\n        tol=1e-11)\n\n    e.fit(X_with_intercept, y)\n\n    print e.coef_\n    # =>[ 0.32155545  0.17904355  0.41222418]\n\nspark.ml code:\n-------------------\n\n    import org.apache.spark.{SparkConf, SparkContext}\n    import org.apache.spark.ml.classification.LogisticRegression\n    import org.apache.spark.ml.regression.LinearRegression\n    import org.apache.spark.mllib.linalg.Vectors\n    import org.apache.spark.mllib.regression.LabeledPoint\n    import org.apache.spark.sql.SQLContext\n\n    object TestSparkRegression {\n      def main(args: Array[String]): Unit = {\n        import org.apache.log4j.{Level, Logger}\n\n        Logger.getLogger(\"org\").setLevel(Level.OFF)\n        Logger.getLogger(\"akka\").setLevel(Level.OFF)\n\n        val conf = new SparkConf().setAppName(\"test\").setMaster(\"local\")\n        val sc = new SparkContext(conf)\n\n        val sparkTrainingData = new SQLContext(sc)\n          .createDataFrame(Seq(\n            LabeledPoint(0.0, Vectors.dense(-0.7306653538519616, 0.0)),\n            LabeledPoint(1.0, Vectors.dense(0.6750417712898752,\n-0.4232874171873786)),\n            LabeledPoint(1.0, Vectors.dense(0.1863463229359709,\n-0.8163423997075965)),\n            LabeledPoint(0.0, Vectors.dense(-0.6719842051493347, 0.0)),\n            LabeledPoint(1.0, Vectors.dense(0.9699938346531928, 0.0)),\n            LabeledPoint(1.0, Vectors.dense(0.22759406190283604, 0.0)),\n            LabeledPoint(1.0, Vectors.dense(0.9688721028330911, 0.0)),\n            LabeledPoint(0.0, Vectors.dense(0.5993795346650845, 0.0)),\n            LabeledPoint(0.0, Vectors.dense(0.9219423508390701,\n-0.8972778242305388)),\n            LabeledPoint(0.0, Vectors.dense(0.7006904841584055,\n-0.5607635619919824))))\n          .toDF(\"label\", \"features\")\n\n        val logisticModel = new LogisticRegression()\n          .setRegParam(0.3)\n          .setLabelCol(\"label\")\n          .setFeaturesCol(\"features\")\n          .fit(sparkTrainingData)\n\n        println(s\"Spark logistic model coefficients:\n${logisticModel.coefficients} Intercept: ${logisticModel.intercept}\")\n        // Spark logistic model coefficients:\n[0.5451588538376263,0.26740606573584713]\nIntercept: -0.13897955358689987\n\n        val linearModel = new LinearRegression()\n          .setRegParam(0.3)\n          .setLabelCol(\"label\")\n          .setFeaturesCol(\"features\")\n          .setSolver(\"l-bfgs\")\n          .fit(sparkTrainingData)\n\n        println(s\"Spark linear model coefficients:\n${linearModel.coefficients} Intercept: ${linearModel.intercept}\")\n        // Spark linear model coefficients:\n[0.19852664861346023,0.11501200541407802]\nIntercept: 0.45464906876832323\n\n        sc.stop()\n      }\n    }\n\nThanks,\n\nFrank\n\n",
  "denotations": [
    {
      "id": 1,
      "start": 4464,
      "end": 4480,
      "text": "\nThanks,\n\nFrank\n",
      "type": "Body/Outro",
      "meta": null
    },
    {
      "id": 2,
      "start": 0,
      "end": 4481,
      "text": "(this was also posted to stackoverflow on 03/10)\n\nI am setting up a very simple logistic regression problem in scikit-learn\nand in spark.ml, and the results diverge: the models they learn are\ndifferent, but I can't figure out why (data is the same, model type is the\nsame, regularization is the same...).\n\nNo doubt I am missing some setting on one side or the other. Which setting?\nHow should I set up either scikit or spark.ml to find the same model as its\ncounterpart?\n\nI give the sklearn code and spark.ml code below. Both should be ready to\ncut-and-paste and run.\n\nscikit-learn code:\n----------------------\n\n    import numpy as np\n    from sklearn.linear_model import LogisticRegression, Ridge\n\n    X = np.array([\n        [-0.7306653538519616, 0.0],\n        [0.6750417712898752, -0.4232874171873786],\n        [0.1863463229359709, -0.8163423997075965],\n        [-0.6719842051493347, 0.0],\n        [0.9699938346531928, 0.0],\n        [0.22759406190283604, 0.0],\n        [0.9688721028330911, 0.0],\n        [0.5993795346650845, 0.0],\n        [0.9219423508390701, -0.8972778242305388],\n        [0.7006904841584055, -0.5607635619919824]\n    ])\n\n    y = np.array([\n        0.0,\n        1.0,\n        1.0,\n        0.0,\n        1.0,\n        1.0,\n        1.0,\n        0.0,\n        0.0,\n        0.0\n    ])\n\n    m, n = X.shape\n\n    # Add intercept term to simulate inputs to GameEstimator\n    X_with_intercept = np.hstack((X, np.ones(m)[:,np.newaxis]))\n\n    l = 0.3\n    e = LogisticRegression(\n        fit_intercept=False,\n        penalty='l2',\n        C=1/l,\n        max_iter=100,\n        tol=1e-11)\n\n    e.fit(X_with_intercept, y)\n\n    print e.coef_\n    # => [[ 0.98662189  0.45571052 -0.23467255]]\n\n    # Linear regression is called Ridge in sklearn\n    e = Ridge(\n        fit_intercept=False,\n        alpha=l,\n        max_iter=100,\n        tol=1e-11)\n\n    e.fit(X_with_intercept, y)\n\n    print e.coef_\n    # =>[ 0.32155545  0.17904355  0.41222418]\n\nspark.ml code:\n-------------------\n\n    import org.apache.spark.{SparkConf, SparkContext}\n    import org.apache.spark.ml.classification.LogisticRegression\n    import org.apache.spark.ml.regression.LinearRegression\n    import org.apache.spark.mllib.linalg.Vectors\n    import org.apache.spark.mllib.regression.LabeledPoint\n    import org.apache.spark.sql.SQLContext\n\n    object TestSparkRegression {\n      def main(args: Array[String]): Unit = {\n        import org.apache.log4j.{Level, Logger}\n\n        Logger.getLogger(\"org\").setLevel(Level.OFF)\n        Logger.getLogger(\"akka\").setLevel(Level.OFF)\n\n        val conf = new SparkConf().setAppName(\"test\").setMaster(\"local\")\n        val sc = new SparkContext(conf)\n\n        val sparkTrainingData = new SQLContext(sc)\n          .createDataFrame(Seq(\n            LabeledPoint(0.0, Vectors.dense(-0.7306653538519616, 0.0)),\n            LabeledPoint(1.0, Vectors.dense(0.6750417712898752,\n-0.4232874171873786)),\n            LabeledPoint(1.0, Vectors.dense(0.1863463229359709,\n-0.8163423997075965)),\n            LabeledPoint(0.0, Vectors.dense(-0.6719842051493347, 0.0)),\n            LabeledPoint(1.0, Vectors.dense(0.9699938346531928, 0.0)),\n            LabeledPoint(1.0, Vectors.dense(0.22759406190283604, 0.0)),\n            LabeledPoint(1.0, Vectors.dense(0.9688721028330911, 0.0)),\n            LabeledPoint(0.0, Vectors.dense(0.5993795346650845, 0.0)),\n            LabeledPoint(0.0, Vectors.dense(0.9219423508390701,\n-0.8972778242305388)),\n            LabeledPoint(0.0, Vectors.dense(0.7006904841584055,\n-0.5607635619919824))))\n          .toDF(\"label\", \"features\")\n\n        val logisticModel = new LogisticRegression()\n          .setRegParam(0.3)\n          .setLabelCol(\"label\")\n          .setFeaturesCol(\"features\")\n          .fit(sparkTrainingData)\n\n        println(s\"Spark logistic model coefficients:\n${logisticModel.coefficients} Intercept: ${logisticModel.intercept}\")\n        // Spark logistic model coefficients:\n[0.5451588538376263,0.26740606573584713]\nIntercept: -0.13897955358689987\n\n        val linearModel = new LinearRegression()\n          .setRegParam(0.3)\n          .setLabelCol(\"label\")\n          .setFeaturesCol(\"features\")\n          .setSolver(\"l-bfgs\")\n          .fit(sparkTrainingData)\n\n        println(s\"Spark linear model coefficients:\n${linearModel.coefficients} Intercept: ${linearModel.intercept}\")\n        // Spark linear model coefficients:\n[0.19852664861346023,0.11501200541407802]\nIntercept: 0.45464906876832323\n\n        sc.stop()\n      }\n    }\n\nThanks,\n\nFrank\n\n",
      "type": "Body",
      "meta": null
    }
  ],
  "meta": {},
  "id": "test/train_1875"
}