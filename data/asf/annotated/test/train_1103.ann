{
  "wrapper": "plaintext",
  "text": "Strange that it's working for some directories but not others.  Looks like\nwholeTextFiles maybe doesn't work with S3?\nhttps://issues.apache.org/jira/browse/SPARK-4414 .\n\nIf it's possible to load the data into EMR and run Spark from there that\nmay be a workaround.  This blogspot shows a python workaround that might\nwork as well:\nhttp://michaelryanbell.com/processing-whole-files-spark-s3.html\n\nJon\n\n\nOn Mon, Feb 6, 2017 at 6:38 PM, Paul Tremblay <paulhtremblay@gmail.com>\nwrote:\n\n> I've actually been able to trace the problem to the files being read in.\n> If I change to a different directory, then I don't get the error. Is one of\n> the executors running out of memory?\n>\n>\n>\n>\n>\n> On 02/06/2017 02:35 PM, Paul Tremblay wrote:\n>\n>> When I try to create an rdd using wholeTextFiles, I get an\n>> incomprehensible error. But when I use the same path with sc.textFile, I\n>> get no error.\n>>\n>> I am using pyspark with spark 2.1.\n>>\n>> in_path = 's3://commoncrawl/crawl-data/CC-MAIN-2016-50/segments/148069\n>> 8542939.6/warc/\n>>\n>> rdd = sc.wholeTextFiles(in_path)\n>>\n>> rdd.take(1)\n>>\n>>\n>> /usr/lib/spark/python/pyspark/rdd.py in take(self, num)\n>>    1341\n>>    1342             p = range(partsScanned, min(partsScanned +\n>> numPartsToTry, totalParts))\n>> -> 1343             res = self.context.runJob(self, takeUpToNumLeft, p)\n>>    1344\n>>    1345             items += res\n>>\n>> /usr/lib/spark/python/pyspark/context.py in runJob(self, rdd,\n>> partitionFunc, partitions, allowLocal)\n>>     963         # SparkContext#runJob.\n>>     964         mappedRDD = rdd.mapPartitions(partitionFunc)\n>> --> 965         port = self._jvm.PythonRDD.runJob(self._jsc.sc(),\n>> mappedRDD._jrdd, partitions)\n>>     966         return list(_load_from_socket(port,\n>> mappedRDD._jrdd_deserializer))\n>>     967\n>>\n>> /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in\n>> __call__(self, *args)\n>>    1131         answer = self.gateway_client.send_command(command)\n>>    1132         return_value = get_return_value(\n>> -> 1133             answer, self.gateway_client, self.target_id,\n>> self.name)\n>>    1134\n>>    1135         for temp_arg in temp_args:\n>>\n>> /usr/lib/spark/python/pyspark/sql/utils.py in deco(*a, **kw)\n>>      61     def deco(*a, **kw):\n>>      62         try:\n>> ---> 63             return f(*a, **kw)\n>>      64         except py4j.protocol.Py4JJavaError as e:\n>>      65             s = e.java_exception.toString()\n>>\n>> /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in\n>> get_return_value(answer, gateway_client, target_id, name)\n>>     317                 raise Py4JJavaError(\n>>     318                     \"An error occurred while calling\n>> {0}{1}{2}.\\n\".\n>> --> 319                     format(target_id, \".\", name), value)\n>>     320             else:\n>>     321                 raise Py4JError(\n>>\n>> Py4JJavaError: An error occurred while calling\n>> z:org.apache.spark.api.python.PythonRDD.runJob.\n>> : org.apache.spark.SparkException: Job aborted due to stage failure:\n>> Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in\n>> stage 1.0 (TID 7, ip-172-31-45-114.us-west-2.compute.internal, executor\n>> 8): ExecutorLostFailure (executor 8 exited caused by one of the running\n>> tasks) Reason: Container marked as failed: container_1486415078210_0005_01_000016\n>> on host: ip-172-31-45-114.us-west-2.compute.internal. Exit status: 52.\n>> Diagnostics: Exception from container-launch.\n>> Container id: container_1486415078210_0005_01_000016\n>> Exit code: 52\n>> Stack trace: ExitCodeException exitCode=52:\n>>     at org.apache.hadoop.util.Shell.runCommand(Shell.java:582)\n>>     at org.apache.hadoop.util.Shell.run(Shell.java:479)\n>>     at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh\n>> ell.java:773)\n>>     at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerEx\n>> ecutor.launchContainer(DefaultContainerExecutor.java:212)\n>>     at org.apache.hadoop.yarn.server.nodemanager.containermanager.l\n>> auncher.ContainerLaunch.call(ContainerLaunch.java:302)\n>>     at org.apache.hadoop.yarn.server.nodemanager.containermanager.l\n>> auncher.ContainerLaunch.call(ContainerLaunch.java:82)\n>>     at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n>>     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPool\n>> Executor.java:1142)\n>>     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoo\n>> lExecutor.java:617)\n>>     at java.lang.Thread.run(Thread.java:745)\n>>\n>> rdd = sc.textFile(in_path)\n>>\n>> In [8]: rdd.take(1)\n>> Out[8]: [u'WARC/1.0']\n>>\n>>\n>\n> ---------------------------------------------------------------------\n> To unsubscribe e-mail: user-unsubscribe@spark.apache.org\n>\n>\n\n",
  "denotations": [
    {
      "id": 1,
      "start": 394,
      "end": 399,
      "text": "\nJon\n",
      "type": "Body/Outro",
      "meta": null
    },
    {
      "id": 2,
      "start": 0,
      "end": 401,
      "text": "Strange that it's working for some directories but not others.  Looks like\nwholeTextFiles maybe doesn't work with S3?\nhttps://issues.apache.org/jira/browse/SPARK-4414 .\n\nIf it's possible to load the data into EMR and run Spark from there that\nmay be a workaround.  This blogspot shows a python workaround that might\nwork as well:\nhttp://michaelryanbell.com/processing-whole-files-spark-s3.html\n\nJon\n\n\n",
      "type": "Body",
      "meta": null
    },
    {
      "id": 3,
      "start": 401,
      "end": 480,
      "text": "On Mon, Feb 6, 2017 at 6:38 PM, Paul Tremblay <paulhtremblay@gmail.com>\nwrote:\n",
      "type": "Header",
      "meta": null
    },
    {
      "id": 4,
      "start": 683,
      "end": 730,
      "text": "> On 02/06/2017 02:35 PM, Paul Tremblay wrote:\n",
      "type": "Header",
      "meta": null
    },
    {
      "id": 5,
      "start": 480,
      "end": 683,
      "text": "\n> I've actually been able to trace the problem to the files being read in.\n> If I change to a different directory, then I don't get the error. Is one of\n> the executors running out of memory?\n>\n>\n>\n>\n>\n",
      "type": "Body",
      "meta": null
    },
    {
      "id": 6,
      "start": 730,
      "end": 4706,
      "text": ">\n>> When I try to create an rdd using wholeTextFiles, I get an\n>> incomprehensible error. But when I use the same path with sc.textFile, I\n>> get no error.\n>>\n>> I am using pyspark with spark 2.1.\n>>\n>> in_path = 's3://commoncrawl/crawl-data/CC-MAIN-2016-50/segments/148069\n>> 8542939.6/warc/\n>>\n>> rdd = sc.wholeTextFiles(in_path)\n>>\n>> rdd.take(1)\n>>\n>>\n>> /usr/lib/spark/python/pyspark/rdd.py in take(self, num)\n>>    1341\n>>    1342             p = range(partsScanned, min(partsScanned +\n>> numPartsToTry, totalParts))\n>> -> 1343             res = self.context.runJob(self, takeUpToNumLeft, p)\n>>    1344\n>>    1345             items += res\n>>\n>> /usr/lib/spark/python/pyspark/context.py in runJob(self, rdd,\n>> partitionFunc, partitions, allowLocal)\n>>     963         # SparkContext#runJob.\n>>     964         mappedRDD = rdd.mapPartitions(partitionFunc)\n>> --> 965         port = self._jvm.PythonRDD.runJob(self._jsc.sc(),\n>> mappedRDD._jrdd, partitions)\n>>     966         return list(_load_from_socket(port,\n>> mappedRDD._jrdd_deserializer))\n>>     967\n>>\n>> /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in\n>> __call__(self, *args)\n>>    1131         answer = self.gateway_client.send_command(command)\n>>    1132         return_value = get_return_value(\n>> -> 1133             answer, self.gateway_client, self.target_id,\n>> self.name)\n>>    1134\n>>    1135         for temp_arg in temp_args:\n>>\n>> /usr/lib/spark/python/pyspark/sql/utils.py in deco(*a, **kw)\n>>      61     def deco(*a, **kw):\n>>      62         try:\n>> ---> 63             return f(*a, **kw)\n>>      64         except py4j.protocol.Py4JJavaError as e:\n>>      65             s = e.java_exception.toString()\n>>\n>> /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in\n>> get_return_value(answer, gateway_client, target_id, name)\n>>     317                 raise Py4JJavaError(\n>>     318                     \"An error occurred while calling\n>> {0}{1}{2}.\\n\".\n>> --> 319                     format(target_id, \".\", name), value)\n>>     320             else:\n>>     321                 raise Py4JError(\n>>\n>> Py4JJavaError: An error occurred while calling\n>> z:org.apache.spark.api.python.PythonRDD.runJob.\n>> : org.apache.spark.SparkException: Job aborted due to stage failure:\n>> Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in\n>> stage 1.0 (TID 7, ip-172-31-45-114.us-west-2.compute.internal, executor\n>> 8): ExecutorLostFailure (executor 8 exited caused by one of the running\n>> tasks) Reason: Container marked as failed: container_1486415078210_0005_01_000016\n>> on host: ip-172-31-45-114.us-west-2.compute.internal. Exit status: 52.\n>> Diagnostics: Exception from container-launch.\n>> Container id: container_1486415078210_0005_01_000016\n>> Exit code: 52\n>> Stack trace: ExitCodeException exitCode=52:\n>>     at org.apache.hadoop.util.Shell.runCommand(Shell.java:582)\n>>     at org.apache.hadoop.util.Shell.run(Shell.java:479)\n>>     at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh\n>> ell.java:773)\n>>     at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerEx\n>> ecutor.launchContainer(DefaultContainerExecutor.java:212)\n>>     at org.apache.hadoop.yarn.server.nodemanager.containermanager.l\n>> auncher.ContainerLaunch.call(ContainerLaunch.java:302)\n>>     at org.apache.hadoop.yarn.server.nodemanager.containermanager.l\n>> auncher.ContainerLaunch.call(ContainerLaunch.java:82)\n>>     at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n>>     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPool\n>> Executor.java:1142)\n>>     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoo\n>> lExecutor.java:617)\n>>     at java.lang.Thread.run(Thread.java:745)\n>>\n>> rdd = sc.textFile(in_path)\n>>\n>> In [8]: rdd.take(1)\n>> Out[8]: [u'WARC/1.0']\n>>\n>>\n>\n> ---------------------------------------------------------------------\n> To unsubscribe e-mail: user-unsubscribe@spark.apache.org\n>\n>\n\n",
      "type": "Body",
      "meta": null
    }
  ],
  "meta": {},
  "id": "test/train_1103"
}